<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 39.96?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">5147820</article-id><article-id pub-id-type="pmid">27935955</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0166204</article-id><article-id pub-id-type="publisher-id">PONE-D-15-52622</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Ears</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Ears</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Equipment</subject><subj-group><subject>Optical Equipment</subject><subj-group><subject>Lasers</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Optimization</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Computational Techniques</subject><subj-group><subject>Biometrics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and Places</subject><subj-group><subject>Population Groupings</subject><subj-group><subject>Religious Faiths</subject><subj-group><subject>Islam</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Data Acquisition</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Face Recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and Memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Face Recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face Recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face Recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face Recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Computer Software</subject></subj-group></subj-group></article-categories><title-group><article-title>Online 3D Ear Recognition by Combining Global and Local Features</article-title><alt-title alt-title-type="running-head">Online 3D Ear Recognition by Combining Global and Local Features</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Yahui</given-names></name><xref ref-type="aff" rid="aff001"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Bob</given-names></name><xref ref-type="aff" rid="aff002"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Lu</surname><given-names>Guangming</given-names></name><xref ref-type="aff" rid="aff001"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>David</given-names></name><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="corresp" rid="cor001">*</xref></contrib></contrib-group><aff id="aff001"><label>1</label>
<addr-line>Department of Computer Science and Technology, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China</addr-line></aff><aff id="aff002"><label>2</label>
<addr-line>Department of Computer and Information Science, University of Macau, Macau</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Hu</surname><given-names>Dewen</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1"><addr-line>National University of Defense Technology College of Mechatronic Engineering and Automation, CHINA</addr-line></aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><fn fn-type="con"><p><list list-type="simple"><list-item><p><bold>Conceptualization:</bold> YL BZ GL DZ.</p></list-item><list-item><p><bold>Data curation:</bold> YL GL DZ.</p></list-item><list-item><p><bold>Formal analysis:</bold> GL DZ.</p></list-item><list-item><p><bold>Funding acquisition:</bold> GL DZ.</p></list-item><list-item><p><bold>Investigation:</bold> YL BZ.</p></list-item><list-item><p><bold>Methodology:</bold> YL BZ GL DZ.</p></list-item><list-item><p><bold>Project administration:</bold> GL DZ.</p></list-item><list-item><p><bold>Resources:</bold> YL BZ GL DZ.</p></list-item><list-item><p><bold>Software:</bold> YL.</p></list-item><list-item><p><bold>Supervision:</bold> GL DZ.</p></list-item><list-item><p><bold>Validation:</bold> DZ.</p></list-item><list-item><p><bold>Visualization:</bold> YL.</p></list-item><list-item><p><bold>Writing &#8211; original draft:</bold> YL BZ.</p></list-item><list-item><p><bold>Writing &#8211; review &amp; editing:</bold> YL BZ GL DZ.</p></list-item></list>
</p></fn><corresp id="cor001">* E-mail: <email>cvpro1024@gmail.com</email></corresp></author-notes><pub-date pub-type="epub"><day>9</day><month>12</month><year>2016</year></pub-date><pub-date pub-type="collection"><year>2016</year></pub-date><volume>11</volume><issue>12</issue><elocation-id>e0166204</elocation-id><history><date date-type="received"><day>17</day><month>2</month><year>2016</year></date><date date-type="accepted"><day>25</day><month>10</month><year>2016</year></date></history><permissions><copyright-statement>&#169; 2016 Liu et al</copyright-statement><copyright-year>2016</copyright-year><copyright-holder>Liu et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0166204.pdf"/><abstract><p>The three-dimensional shape of the ear has been proven to be a stable candidate for biometric authentication because of its desirable properties such as universality, uniqueness, and permanence. In this paper, a special laser scanner designed for online three-dimensional ear acquisition was described. Based on the dataset collected by our scanner, two novel feature classes were defined from a three-dimensional ear image: the global feature class (empty centers and angles) and local feature class (points, lines, and areas). These features are extracted and combined in an optimal way for three-dimensional ear recognition. Using a large dataset consisting of 2,000 samples, the experimental results illustrate the effectiveness of fusing global and local features, obtaining an equal error rate of 2.2%.</p></abstract><funding-group><award-group id="award001"><funding-source><institution>NSFC fund</institution></funding-source><award-id>61332011,61020106004, 61272292, 61271344</award-id><principal-award-recipient><name><surname>Lu</surname><given-names>Guangming</given-names></name></principal-award-recipient></award-group><award-group id="award002"><funding-source><institution>Shenzhen Fundamental Research fund</institution></funding-source><award-id>JCYJ20130401152508661</award-id><principal-award-recipient><name><surname>Lu</surname><given-names>Guangming</given-names></name></principal-award-recipient></award-group><award-group id="award003"><funding-source><institution>GRF fund from the HKSAR Government</institution></funding-source><principal-award-recipient><name><surname>Lu</surname><given-names>Guangming</given-names></name></principal-award-recipient></award-group><award-group id="award004"><funding-source><institution>Key Laboratory of Network Oriented Intelligent Computation, Shenzhen, China</institution></funding-source><principal-award-recipient><name><surname>Lu</surname><given-names>Guangming</given-names></name></principal-award-recipient></award-group><funding-statement>The work is partially supported by the GRF fund from the HKSAR Government, the central fund from Hong Kong Polytechnic University, the NSFC fund (61332011, 61272292, 61271344), Shenzhen Fundamental Research fund (JCYJ20150403161923528), and Key Laboratory of Network Oriented Intelligent Computation, Shenzhen, China. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="18"/><table-count count="6"/><page-count count="19"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All our data are fully available without restriction. Data are available from the <ext-link ext-link-type="uri" xlink:href="http://figshare.com">figshare.com</ext-link> and Biometrics Research Center of The Hong Kong Polytechnic University. Interested researchers can download the 3D ear database from following URLs: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6084/m9.figshare.1378463">http://dx.doi.org/10.6084/m9.figshare.1378463</ext-link> or <ext-link ext-link-type="uri" xlink:href="http://www4.comp.polyu.edu.hk/~biometrics/">http://www4.comp.polyu.edu.hk/~biometrics/</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All our data are fully available without restriction. Data are available from the <ext-link ext-link-type="uri" xlink:href="http://figshare.com">figshare.com</ext-link> and Biometrics Research Center of The Hong Kong Polytechnic University. Interested researchers can download the 3D ear database from following URLs: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6084/m9.figshare.1378463">http://dx.doi.org/10.6084/m9.figshare.1378463</ext-link> or <ext-link ext-link-type="uri" xlink:href="http://www4.comp.polyu.edu.hk/~biometrics/">http://www4.comp.polyu.edu.hk/~biometrics/</ext-link>.</p></notes></front><body><sec id="sec001"><title>1. Introduction</title><p>Biometric authentication is of great importance for applications in public security [<xref rid="pone.0166204.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0166204.ref002" ref-type="bibr">2</xref>, <xref rid="pone.0166204.ref003" ref-type="bibr">3</xref>]. Nowadays, several novel biometrics, including palmprints [<xref rid="pone.0166204.ref004" ref-type="bibr">4</xref>], veins [<xref rid="pone.0166204.ref005" ref-type="bibr">5</xref>], and ears [<xref rid="pone.0166204.ref006" ref-type="bibr">6</xref>&#8211;<xref rid="pone.0166204.ref012" ref-type="bibr">12</xref>], have been developed to meet the needs of different security requirements.</p><p>With advances in three-dimensional (3D) imaging technology, 3D biometric authentication has drawn increasing attention from researchers. Examples include 3D face [<xref rid="pone.0166204.ref013" ref-type="bibr">13</xref>, <xref rid="pone.0166204.ref014" ref-type="bibr">14</xref>], palmprint [<xref rid="pone.0166204.ref015" ref-type="bibr">15</xref>&#8211;<xref rid="pone.0166204.ref017" ref-type="bibr">17</xref>], and ear recognition [<xref rid="pone.0166204.ref018" ref-type="bibr">18</xref>&#8211;<xref rid="pone.0166204.ref023" ref-type="bibr">23</xref>]. A 3D ear image is robust to imaging conditions, and contains surface shape information that is related to anatomical structure. In addition, it is insensitive to environmental illuminations. Yan and Bowyer [<xref rid="pone.0166204.ref018" ref-type="bibr">18</xref>] utilized both color and depth images to determine the ear pit for automated 3D ear segmentation. Furthermore, they proposed an improved Iterative Closest Point (ICP) algorithm for 3D ear point cloud matching. Chen and Bhanu [<xref rid="pone.0166204.ref019" ref-type="bibr">19</xref>] gave a 3D ear recognition method founded on a Local Surface Patch (LSP) and ICP algorithm. Moreover, they proposed an indexing approach [<xref rid="pone.0166204.ref020" ref-type="bibr">20</xref>] that combines feature embedding and a support vector machine-based learning technique for ranking their hypotheses. Islam et al. presented a local 3D features extraction method based on the key point detection [<xref rid="pone.0166204.ref021" ref-type="bibr">21</xref>, <xref rid="pone.0166204.ref022" ref-type="bibr">22</xref>]. Zhou et al. presented a 3D ear recognition system combining local and holistic features [<xref rid="pone.0166204.ref023" ref-type="bibr">23</xref>]. Zhang et al. introduced a sparse representation framework into the field of 3D ear identification [<xref rid="pone.0166204.ref024" ref-type="bibr">24</xref>]. Chen and Mu proposed a hybrid multi-keypoint descriptor sparse representation-based classification (MKD-SRC) method to solve one sample per person problem in ear recognition [<xref rid="pone.0166204.ref025" ref-type="bibr">25</xref>].</p><p>Even though good results were achieved in these studies, there is no overall system for online 3D ear recognition. First, most of the current methods use commercial laser scanners to acquire the 3D range image, for example, the widely used Minolta VIVID Series [<xref rid="pone.0166204.ref018" ref-type="bibr">18</xref>&#8211;<xref rid="pone.0166204.ref025" ref-type="bibr">25</xref>]. Although these scanners are general-purpose and high-performance, they are expensive and cumbersome. Second, previous 3D ear recognition methods focused on a single aspect, that is, mostly local features, while global features such as the ear-parotic area angle, and the ear hole shape have not been discussed or used. Given these considerations, a laser scanner specifically designed for 3D ear acquisition and recognition was first developed using the laser-triangulation principle. The scanner provides 2D intensity images and 3D point-cloud data for subsequent recognition, and the total scanning and transmission time is less than 2 s. Based on the 3D ear images collected by our laser scanning device, two feature classes consisting of five features were defined. The empty center shape and the angle feature represent the depth and orientation of a 3D ear, and are treated as global features. The point, line, and area features describe key points, shapes, and the local area of the 3D ears. They are treated as local features. By combining these global features with local features, a hierarchical structure was introduced for 3D ear recognition. The 3D ears are pre-classified using global features and then recognized using local features. Thus, much time can be saved and accuracy can be improved in 3D ear recognition. Therefore, the 3D ear recognition system achieves both a high efficiency and accuracy.</p><p>The purpose of this study was to create a 3D ear recognition system using equipment that is practical for real applications. The contributions of this paper can be summarized as follows. Firstly, the global and local features categories in 3D ear are proposed. Secondly, multi-forms of features in 3D ears have been defined and extracted. Thirdly, multi-features fusion and hierarchical recognition of 3D ears have been discussed. Finally, a complete solution for 3D ear authentication has been achieved. The results on the collected 3D ear data show that the system is efficient and accurate.</p></sec><sec id="sec002"><title>2. Special Scanner Design for Online 3D Ear Acquisition</title><p>The 3D ear scanner we developed is based on the laser trangulation princple [<xref rid="pone.0166204.ref026" ref-type="bibr">26</xref>]. <xref ref-type="fig" rid="pone.0166204.g001">Fig 1</xref> illustrates the imaging principle of laser triangulation. In the reference <italic>X-Y-Z</italic> coordinates, the 3D coordinates (<italic>x</italic>, <italic>y</italic>, <italic>z</italic>) can be calculated according to Eq (<xref ref-type="disp-formula" rid="pone.0166204.e001">1</xref>).</p><disp-formula id="pone.0166204.e001"><alternatives><graphic xlink:href="pone.0166204.e001.jpg" id="pone.0166204.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>&#8594;</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>z</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>d</mml:mi><mml:mi mathvariant="normal">tan</mml:mi><mml:mi>&#952;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>'</mml:mo><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mi mathvariant="normal">tan</mml:mi><mml:mi>&#952;</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>y</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>d</mml:mi><mml:mi mathvariant="normal">tan</mml:mi><mml:mi>&#952;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>'</mml:mo><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mi mathvariant="normal">tan</mml:mi><mml:mi>&#952;</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo>&#8722;</mml:mo><mml:mi>d</mml:mi><mml:mi mathvariant="normal">tan</mml:mi><mml:mi>&#952;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>'</mml:mo><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mi mathvariant="normal">tan</mml:mi><mml:mi>&#952;</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(1)</label></disp-formula><fig id="pone.0166204.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g001</object-id><label>Fig 1</label><caption><title>Imaging principle of laser-triangulation imaging.</title></caption><graphic xlink:href="pone.0166204.g001"/></fig><p><xref ref-type="fig" rid="pone.0166204.g002">Fig 2</xref> illustrates the framework of the 3D ear recognition system. The system consists of two main parts: hardware and software. To meet the requirements of online recognition, the hardware and software should be optimized for speed and accuracy. At the same time, its portability and cost for real applications should be considered. The laser scanner developed for 3D ear acquisition is shown in <xref ref-type="fig" rid="pone.0166204.g003">Fig 3A</xref>. <xref ref-type="fig" rid="pone.0166204.g003">Fig 3B</xref> shows two group of typical 3D ear samples captured by our device, where each row is the 3D point cloud from one ear viewed at different angles.</p><fig id="pone.0166204.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g002</object-id><label>Fig 2</label><caption><title>Framework of the 3D ear recognition system.</title></caption><graphic xlink:href="pone.0166204.g002"/></fig><fig id="pone.0166204.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g003</object-id><label>Fig 3</label><caption><title>Proposed 3D ear acquisition system:</title><p>(A) 3D ear acquisition device and (B) 3D ear samples viewed at different angles (each row is collected from a single ear).</p></caption><graphic xlink:href="pone.0166204.g003"/></fig><p><xref rid="pone.0166204.t001" ref-type="table">Table 1</xref> provide a performance comparison of our proposed device with the Minolta Vivid 910 range scanner that is a widely used commercial scanner and has been used to acquire 3D ear data for UND data set. The acquisition time refers to the total scanning and transmission time, accuracy refers to the depth precision of the measurement, dimensions refer to the width, height and length of the scanner, in addition, the weight and price are also listed. Although the measurement accuracy of our acquisition system is inferior to that of Vivid 910, it has a higher speed, smaller size, and much lower cost. Moreover, the device could provide original frames of laser lines that describe the fundamental structure of 3D features. All these traits make the specially designed device suitable for 3D ear acquisition in practical biometrics applications.</p><table-wrap id="pone.0166204.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.t001</object-id><label>Table 1</label><caption><title>Comparison of the scanning device.</title></caption><alternatives><graphic id="pone.0166204.t001g" xlink:href="pone.0166204.t001"/></alternatives></table-wrap><p>A 3D ear database was established using the developed 3D ear acquisition device by collecting 3D ears on two separate occasions separated by an interval of around one month. On each occasion, the subject was asked to provide two samples. The database contains 2,000 samples from 500 volunteers consisting of 341 males and 159 females. The volunteers were students and staff of the Shenzhen Graduate School of Harbin Institute of Technology. The written consents were obtained from the participants prior to the study. The study was approved by the Academic Committee of the Department of Computing of Harbin Institute of Technology, Shenzhen Graduate School, which ensures that research programs are consistent with academic ethics. The 3D ear acquisition study was discussed in a meeting of the committee, and written approval was subsequently granted by the Department Head. Because our research work does not involve patients or privacy, and all the participants have given written consent to the use of their ear images for academic purposes, all the data and figures published in this paper are fully available from the <ext-link ext-link-type="uri" xlink:href="http://figshare.com">figshare.com</ext-link> and Biometrics Research Center of The Hong Kong Polytechnic University. Interested researchers can download the 3D ear database from following URLs: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6084/m9.figshare.1378463">http://dx.doi.org/10.6084/m9.figshare.1378463</ext-link> or <ext-link ext-link-type="uri" xlink:href="http://www4.comp.polyu.edu.hk/~biometrics/">http://www4.comp.polyu.edu.hk/~biometrics/</ext-link>. The database is used for feature extraction and recognition in the following sections.</p></sec><sec id="sec003"><title>3. 3D Ear Global and Local Feature Classes</title><p>Prior to feature extraction, the 3D ears were normalized using a projection density method [<xref rid="pone.0166204.ref027" ref-type="bibr">27</xref>]. After that, a 3D image of the ear is formed as a normalized posture in unified <italic>X-Y-Z</italic> coordinate<strike>s,</strike> where all features are extracted from the 3D point cloud of the ear.</p><sec id="sec004"><title>3.1 Global Feature Class</title><p>Two global features, empty center and angle, are defined in the proposed system.</p><sec id="sec005"><title>3.1.1 Empty Center Feature</title><p>In the normalized <italic>X-Y</italic> coordinates, the boundary points of the ear were first detected (<xref ref-type="fig" rid="pone.0166204.g004">Fig 4A</xref>), then the connected areas were labeled (<xref ref-type="fig" rid="pone.0166204.g004">Fig 4B</xref>). The connected areas that are less than a threshold were removed then (<xref ref-type="fig" rid="pone.0166204.g004">Fig 4C</xref>). Lastly, the connected pixels inside the ear were selected as the empty center feature (<xref ref-type="fig" rid="pone.0166204.g004">Fig 4D</xref>).</p><fig id="pone.0166204.g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g004</object-id><label>Fig 4</label><caption><title>Empty center feature extraction.</title></caption><graphic xlink:href="pone.0166204.g004"/></fig><p>The template matching technique is used to calculate the distance between two empty center features. The distance is defined as: <italic>D</italic> = <italic>E</italic><sub>1</sub>&#10753;<italic>E</italic><sub>2</sub>/<italic>E</italic><sub>1</sub> &#8746; <italic>E</italic><sub>2</sub>, where <italic>E</italic><sub>1</sub> and <italic>E</italic><sub>2</sub> are the empty center features of different samples. To avoid displacement interference, the test image was shifted by &#177;40 pixels left-right and up-down, where the minimum distance is taken to be the difference of the two empty center areas (<xref ref-type="fig" rid="pone.0166204.g005">Fig 5</xref>).</p><fig id="pone.0166204.g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g005</object-id><label>Fig 5</label><caption><title>Matching empty center features.</title></caption><graphic xlink:href="pone.0166204.g005"/></fig><p><xref ref-type="fig" rid="pone.0166204.g006">Fig 6</xref> shows the empty center feature vectors extracted from Sample A, Sample B, and Sample C. Sample A and Sample B are from the same ear, and Sample C is from a different ear. The distance between Sample A and Sample B is 0.23, and the distance between Sample B and Sample C is 0.56, which indicates that the empty center feature vectors from the same ear are alike and those from different ears are dissimilar.</p><fig id="pone.0166204.g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g006</object-id><label>Fig 6</label><caption><title>Discriminating the same ear and different ears using the empty center feature vector.</title></caption><graphic xlink:href="pone.0166204.g006"/></fig></sec><sec id="sec006"><title>3.1.2 Angle Feature</title><p>In <xref ref-type="fig" rid="pone.0166204.g007">Fig 7</xref>, there is an angle between the ear and parotic area of a person [<xref rid="pone.0166204.ref028" ref-type="bibr">28</xref>]. It can be assumed that there is a plane, <italic>A</italic><sub><italic>f</italic></sub><italic>x</italic> + <italic>B</italic><sub><italic>f</italic></sub><italic>y</italic> + <italic>C</italic><sub><italic>f</italic></sub><italic>z</italic> + <italic>D</italic><sub><italic>f</italic></sub> = 0, which represents the 3D points on the parotic region (green circle shown in <xref ref-type="fig" rid="pone.0166204.g008">Fig 8</xref>). And there is another plane, <italic>A</italic><sub><italic>e</italic></sub><italic>x</italic> + <italic>B</italic><sub><italic>e</italic></sub><italic>y</italic> + <italic>C</italic><sub><italic>e</italic></sub><italic>z</italic> + <italic>D</italic><sub><italic>e</italic></sub> = 0, represents the 3D points on the ear edge. Thus, the normal vector of the parotic plane can be obtained as <italic>n</italic><sub><italic>f</italic></sub> = (<italic>A</italic><sub><italic>f</italic></sub>,<italic>B</italic><sub><italic>f</italic></sub>,<italic>C</italic><sub><italic>f</italic></sub>), and the normal vector of the ear plane is <italic>n</italic><sub><italic>e</italic></sub> = (<italic>A</italic><sub><italic>e</italic></sub>,<italic>B</italic><sub><italic>e</italic></sub>,<italic>C</italic><sub><italic>e</italic></sub>). The angle <italic>&#952;</italic> between the parotic and ear planes can be defined as follows:
<disp-formula id="pone.0166204.e002"><alternatives><graphic xlink:href="pone.0166204.e002.jpg" id="pone.0166204.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#952;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>&#8214;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&#8214;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:math></alternatives><label>(2)</label></disp-formula>
Where &lt;<italic>n</italic><sub><italic>f</italic></sub>,<italic>n</italic><sub><italic>e</italic></sub>&gt; is the inner product of normal vectors <italic>n</italic><sub><italic>f</italic></sub> and <italic>n</italic><sub><italic>e</italic></sub>. The &#8214;<italic>n</italic><sub><italic>f</italic></sub>&#8214;<sub>2</sub> and &#8214;<italic>n</italic><sub><italic>e</italic></sub>&#8214;<sub>2</sub> are L<sub>2</sub>-norms of <italic>n</italic><sub><italic>f</italic></sub> and <italic>n</italic><sub><italic>e</italic></sub> respectively.</p><p>Hence,
<disp-formula id="pone.0166204.e003"><alternatives><graphic xlink:href="pone.0166204.e003.jpg" id="pone.0166204.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:mi>&#952;</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#952;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="2em"/><mml:mi mathvariant="normal">if</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#952;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>90</mml:mn><mml:mi>&#176;</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>180</mml:mn><mml:mi>&#176;</mml:mi><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#952;</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="1em"/><mml:mi mathvariant="normal">otherwise</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives><label>(3)</label></disp-formula></p><fig id="pone.0166204.g007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g007</object-id><label>Fig 7</label><caption><title>Angle feature extraction.</title></caption><graphic xlink:href="pone.0166204.g007"/></fig><fig id="pone.0166204.g008" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g008</object-id><label>Fig 8</label><caption><title>Detected key-points:</title><p>The left two samples are from one ear and the other two samples are from a different ear. The detected key-points are marked by red and blue respectively.</p></caption><graphic xlink:href="pone.0166204.g008"/></fig></sec></sec><sec id="sec007"><title>3.2 Local Feature Class</title><p>Three categories of local features in the 3D ear image were defined: point, line, and area features.</p><sec id="sec008"><title>3.2.1 Point Feature</title><p>The 3D ear model consists of a number of points in 3D coordinates. Therefore, if the key points that are stable for the same ear and distinguishable for different ears could be found, then the 3D ear models would be recognized using these key points.</p><p>The aim of key-point detection is to select points on the 3D ear surface that can be identified with high repeatability in different models of the same surface. Islam and Mian proposed a key-point detection and feature extraction method that is effective on 3D ears [<xref rid="pone.0166204.ref021" ref-type="bibr">21</xref>] and faces [<xref rid="pone.0166204.ref029" ref-type="bibr">29</xref>]. Although the core of our key-point detection technique is similar to theirs, the technique is modified to make it suitable for the 3D ears data captured by our proposed device. In addition, the point feature is defined differently.</p><p>The input to the algorithm is a point cloud of the ear <italic>E</italic> = {<italic>P</italic><sub>1</sub>,&#8230;,<italic>P</italic><sub><italic>n</italic></sub>}. For each point <italic>P</italic>(<italic>x</italic><sub><italic>i</italic></sub>
<italic>y</italic><sub><italic>i</italic></sub>
<italic>z</italic><sub><italic>i</italic></sub>)<sup><italic>T</italic></sup>, where <italic>i</italic> = 1,&#8230;,<italic>n</italic>, a local surface is cropped from the point cloud using a sphere of radius <italic>r</italic> centered at <italic>P</italic> and recorded as <inline-formula id="pone.0166204.e004"><alternatives><graphic xlink:href="pone.0166204.e004.jpg" id="pone.0166204.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt><mml:mo>&lt;</mml:mo><mml:mi mathvariant="normal">r</mml:mi><mml:mo>}</mml:mo></mml:math></alternatives></inline-formula>. The principle component analysis is then applied on the data points <italic>SetL</italic>. The difference between the eigenvalues along the first two principal axes of the local surface is computed as <italic>d</italic>. The value of <italic>d</italic> indicates the extent of asymmetry around the center point <italic>P</italic>, which is zero if <italic>SetL</italic> is planar or spherical. It is then compared to a threshold <italic>t</italic>, and if <italic>d</italic> &gt; <italic>t</italic>, the point <italic>P</italic>(<italic>x</italic><sub><italic>i</italic></sub>
<italic>y</italic><sub><italic>i</italic></sub>
<italic>z</italic><sub><italic>i</italic></sub>)<sup><italic>T</italic></sup> is selected as a key-point. At the same time, the angular separation <italic>&#966;</italic> between the third principal axes and the original unified <italic>Z</italic> coordinate was calculated. Let <italic>K</italic><sub><italic>m</italic></sub> = [<italic>x</italic><sub><italic>m</italic></sub>
<italic>y</italic><sub><italic>m</italic></sub>
<italic>z</italic><sub><italic>m</italic></sub>
<italic>d</italic><sub><italic>m</italic></sub>
<italic>&#966;</italic><sub><italic>m</italic></sub>]<sup><italic>T</italic></sup> (where <italic>m</italic> = 1,&#8230;,<italic>n</italic><sub><italic>k</italic></sub>) record the key-point information. Set <italic>K</italic><sub><italic>m</italic></sub> is used at a later stage of feature extraction. Parameters <italic>r</italic> and <italic>t</italic> are empirically chosen as <italic>r</italic> = 5 mm and <italic>t</italic> = 2 mm.</p><p><xref ref-type="fig" rid="pone.0166204.g008">Fig 8</xref> shows examples of key-points detected on four different point clouds scanned from two individuals. It illustrates that key-points are stable in the ear data of the same individual, and distinguishable for the ear data of different individuals.</p><p>After key-point detection, features are extracted from set <italic>K</italic><sub><italic>m</italic></sub> (as shown in <xref ref-type="fig" rid="pone.0166204.g009">Fig 9</xref>).</p><fig id="pone.0166204.g009" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g009</object-id><label>Fig 9</label><caption><title>Extraction of the point feature vector.</title></caption><graphic xlink:href="pone.0166204.g009"/></fig><p>First, the normalized ear was divided into 12 average fan-shaped parts, where each sector is further divided into four equidistant parts. Each part is marked as <italic>F</italic><sub><italic>l</italic></sub>, where <italic>l</italic> = 1,&#8230;,48. Thus, the <italic>x-y-z</italic> values of key-point set <italic>K</italic><sub><italic>m</italic></sub> fall within these 48 parts.</p><p>Second, for each <italic>F</italic><sub><italic>l</italic></sub>, the statistical histograms of <italic>d</italic> and <italic>&#966;</italic> were calculated. The histogram bins of <italic>d</italic> are set to 2, 3, 4, 5, 6, and 7, and the bins of <italic>&#966;</italic> are set to 0, 1, 2, and 3. Next, the number in each bin was counted to obtain a 10-dimensional vector. If there is no key-point in <italic>F</italic><sub><italic>l</italic></sub>, the vector was set to [0,0,0,0,0,0,0,0,0,0].</p><p>Finally, all 48 vectors were connected to obtain a 480-dimensional vector <italic>V</italic><sub><italic>p</italic></sub> as the final point feature vector. The difference between two ears is calculated using the Euclidean distance between their <italic>V</italic><sub><italic>p</italic></sub> vectors.</p><p><xref ref-type="fig" rid="pone.0166204.g010">Fig 10</xref> shows the point feature vectors extracted from different samples. Sample 1 (S1) and 2 (S2) are from the same ear, and Sample 3 (S3) is from a different ear. The red curve is the point feature vector of S1, the blue curve is the point feature vector of S2, and the black curve is the point feature vector of S3. The distance between S1 and S2 is 33.7, and the distance between S1 and S3 is 127.4. It can be seen that the point feature vectors from the same ear are very similar, and those from different ears are dissimilar.</p><fig id="pone.0166204.g010" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g010</object-id><label>Fig 10</label><caption><title>Discriminating between the same and different ears using the point feature vector.</title></caption><graphic xlink:href="pone.0166204.g010"/></fig></sec><sec id="sec009"><title>3.2.2 Line Feature</title><p>To calculate the line feature, a rectangle was fitted on the normalized ear in the <italic>X-Y</italic> coordinates, define (<italic>M</italic> + <italic>N</italic>) lines, <italic>V</italic><sub>1</sub>, &#8230;, <italic>V</italic><sub><italic>m</italic></sub> (which divides the rectangle equally in the horizontal direction), and <italic>H</italic><sub>1</sub>, &#8230;, <italic>H</italic><sub><italic>n</italic></sub> (which divides the rectangle equally in the vertical direction), as shown in <xref ref-type="fig" rid="pone.0166204.g011">Fig 11</xref>. Next, the 3D points on each line were obtained and their <italic>z</italic> values were recorded. Each line was then divided equally and the <italic>z</italic> crossing point values were marked as <italic>z</italic><sub><italic>1</italic></sub>, <italic>z</italic><sub><italic>2</italic></sub>, &#8230;, <italic>z</italic><sub><italic>10</italic></sub> (or <italic>z</italic><sub><italic>1</italic></sub>, <italic>z</italic><sub><italic>2</italic></sub>, &#8230;, <italic>z</italic><sub><italic>20</italic></sub> for <italic>V</italic><sub>1</sub>, &#8230;, <italic>V</italic><sub><italic>m</italic></sub>). These <italic>z</italic> values were used to form the line feature vector <italic>L</italic> (<italic>V</italic><sub>1</sub>, &#8230;, <italic>V</italic><sub><italic>m</italic></sub>, <italic>H</italic><sub>1</sub>, &#8230;, <italic>H</italic><sub><italic>n</italic></sub>), where the vector is of length (20 &#215; m + 10 &#215; n).</p><fig id="pone.0166204.g011" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g011</object-id><label>Fig 11</label><caption><title>Extraction of the line feature vector.</title></caption><graphic xlink:href="pone.0166204.g011"/></fig><p><xref ref-type="fig" rid="pone.0166204.g012">Fig 12</xref> shows the line feature vectors extracted from the same samples as those in <xref ref-type="fig" rid="pone.0166204.g011">Fig 11</xref>. Parameters <italic>m</italic> = 2 and <italic>n</italic> = 3 were used in the experiment to test the discrimination of the line feature. The distance between S1 and S2 using the line feature is 7.02, and the distance between S1 and S3 is 41.12. It can be seen that the line feature vectors from the same ear are very close, and the line feature vectors from different ears are further apart.</p><fig id="pone.0166204.g012" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g012</object-id><label>Fig 12</label><caption><title>Discriminating between the same and different ears using the line feature vector.</title></caption><graphic xlink:href="pone.0166204.g012"/></fig></sec><sec id="sec010"><title>3.2.3 Area Feature</title><p>In order to compute the area feature, the 3D ear was fitted into a fixed block and divided into m &#215; n equal areas (see <xref ref-type="fig" rid="pone.0166204.g013">Fig 13</xref>). All coordinate points in the area are defined as (<italic>x</italic><sub><italic>i</italic></sub>,<italic>y</italic><sub><italic>i</italic></sub>,<italic>z</italic><sub><italic>i</italic></sub>) <italic>i</italic> = 1,&#8230;,<italic>N</italic>, where <italic>N</italic> is the number of the points in the area. All the coordinates of these points constitute an N &#215; 3 matrix <italic>W</italic> as follows:
<disp-formula id="pone.0166204.e005"><alternatives><graphic xlink:href="pone.0166204.e005.jpg" id="pone.0166204.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>&#8230;</mml:mo></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives><label>(4)</label></disp-formula></p><fig id="pone.0166204.g013" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g013</object-id><label>Fig 13</label><caption><title>Extraction of the area feature vector.</title></caption><graphic xlink:href="pone.0166204.g013"/></fig><p>Principle component analysis [<xref rid="pone.0166204.ref030" ref-type="bibr">30</xref>, <xref rid="pone.0166204.ref031" ref-type="bibr">31</xref>] is performed on <italic>W</italic> and the resulting normal vector is represented as <italic>V</italic><sub><italic>N</italic></sub>(<italic>i</italic>,<italic>j</italic>,<italic>k</italic>).</p><p>The average is calculated using
<disp-formula id="pone.0166204.e006"><alternatives><graphic xlink:href="pone.0166204.e006.jpg" id="pone.0166204.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(5)</label></disp-formula></p><p>The scatter matrix is given as <inline-formula id="pone.0166204.e007"><alternatives><graphic xlink:href="pone.0166204.e007.jpg" id="pone.0166204.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>&#8721;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#215;</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8722;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, the eigenvectors of <italic>S</italic> are &#934;, and the first column of &#934; is the normal vector <italic>V</italic><sub><italic>N</italic></sub>(<italic>i</italic>,<italic>j</italic>,<italic>k</italic>). It is clear that <italic>V</italic><sub><italic>N</italic></sub>(<italic>i</italic>,<italic>j</italic>,<italic>k</italic>) can be thought of as the direction of matrix <italic>W</italic>. In addition, the center of gravity of <italic>W</italic> can be represented as <inline-formula id="pone.0166204.e008"><alternatives><graphic xlink:href="pone.0166204.e008.jpg" id="pone.0166204.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>&#175;</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. As a result, the normal vector V<sub>N</sub>, center of gravity V<sub>C</sub>, and min/max z values V<sub>Z</sub> are calculated and joined to form a vector A<sub>N</sub> for each area. The area feature subsequently becomes the vector consisting of all m &#215; n vectors A, (A<sub>11</sub>, A<sub>12</sub>, &#8230;, A<sub>mn</sub>). <xref ref-type="fig" rid="pone.0166204.g014">Fig 14</xref> shows the area feature vectors extracted from S1, S2, and S3. The distance between S1 and S2 is 6.89, and the distance between S2 and S3 is 27.78, which indicates that the area feature vectors from the same ear are alike and those from different ears are not alike.</p><fig id="pone.0166204.g014" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g014</object-id><label>Fig 14</label><caption><title>Discriminating between the same and different ears using the area feature vector.</title></caption><graphic xlink:href="pone.0166204.g014"/></fig></sec></sec></sec><sec id="sec011"><title>4. Experimental Results and Discussion</title><p>The experiments were divided into two parts: feature optimization and verification experiments. As mentioned above, our database contains a total of 2,000 different samples from 500 individual ears. A PC with Intel Core 2 CPU @2.33 GHz and 2 GB memory was used in our experiments.</p><sec id="sec012"><title>4.1 Feature Optimization</title><p>Because the parameters used in the definition of each local feature may influence the length of the feature vector as well as the equal error rate (EER) of the verification experiments, the feature optimization experiments were performed to determine the most effective values for these parameters.</p><p>In our point feature, the number and distribution of the key-points determines the point feature vector. Hence, threshold <italic>t</italic> is the parameter that needs to be optimized. <xref ref-type="fig" rid="pone.0166204.g015">Fig 15</xref> shows the different key-points extracted using different thresholds, while <xref rid="pone.0166204.t002" ref-type="table">Table 2</xref> shows the EER for different thresholds. Considering the time consumed, the feature optimization experiments were performed on a sub-dataset that contains 100 different sample ears. From <xref rid="pone.0166204.t002" ref-type="table">Table 2</xref>, it can be seen that the best result is achieved when <italic>t</italic> = 2.</p><fig id="pone.0166204.g015" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g015</object-id><label>Fig 15</label><caption><title>Point feature optimization.</title></caption><graphic xlink:href="pone.0166204.g015"/></fig><table-wrap id="pone.0166204.t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.t002</object-id><label>Table 2</label><caption><title>Point features with different <italic>t</italic> parameters.</title></caption><alternatives><graphic id="pone.0166204.t002g" xlink:href="pone.0166204.t002"/></alternatives></table-wrap><p>The line feature vector is determined by the number of horizontal and vertical lines. Therefore, the line number is the parameter that needs to be optimized here. <xref ref-type="fig" rid="pone.0166204.g016">Fig 16</xref> shows the different lines across the ear. <xref rid="pone.0166204.t003" ref-type="table">Table 3</xref> shows the EER obtained using different line numbers, where 12 lines obtains the lowest EER.</p><fig id="pone.0166204.g016" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g016</object-id><label>Fig 16</label><caption><title>Line feature optimization.</title></caption><graphic xlink:href="pone.0166204.g016"/></fig><table-wrap id="pone.0166204.t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.t003</object-id><label>Table 3</label><caption><title>Line features for different line numbers.</title></caption><alternatives><graphic id="pone.0166204.t003g" xlink:href="pone.0166204.t003"/></alternatives></table-wrap><p>Because the number of blocks determines the area feature vector, this parameter is the one that must be optimized. <xref ref-type="fig" rid="pone.0166204.g017">Fig 17</xref> shows the different blocks on the ear and <xref rid="pone.0166204.t004" ref-type="table">Table 4</xref> shows the EER obtained using different block numbers. It can be seen that the best result is achieved when there are 48 blocks.</p><fig id="pone.0166204.g017" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g017</object-id><label>Fig 17</label><caption><title>Area feature optimization.</title></caption><graphic xlink:href="pone.0166204.g017"/></fig><table-wrap id="pone.0166204.t004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.t004</object-id><label>Table 4</label><caption><title>Area features with different block numbers.</title></caption><alternatives><graphic id="pone.0166204.t004g" xlink:href="pone.0166204.t004"/></alternatives></table-wrap></sec><sec id="sec013"><title>4.2 Matching Using Local Features</title><p>The matching experiments were carried on all 2,000 samples, and performed using the local feature class (point, line, and area features) as well as their feature-level fusion. Since all the local features (point, line, and area features) are defined in form of vectors (<italic>V</italic><sub><italic>P</italic></sub>, <italic>V</italic><sub><italic>L</italic></sub>, <italic>V</italic><sub><italic>A</italic></sub>), the most direct strategy for feature-level fusion is to joint different vectors into one fusion feature vector. Therefore, the fusion feature vectors can be described as follows:
<disp-formula id="pone.0166204.e009"><alternatives><graphic xlink:href="pone.0166204.e009.jpg" id="pone.0166204.e009g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M9"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mspace width="0.5em"/><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="0.5em"/><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="0.5em"/><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives><label>(6)</label></disp-formula></p><p>The function <italic>normalization</italic> normalizes the feature vector into unit vector. The function <italic>joint</italic> combines two feature vectors into one fusion feature vector. <xref rid="pone.0166204.t005" ref-type="table">Table 5</xref> shows the EER results of different local features and their combinations. It can be seen that the optimal result is achieved when all local features are fused together.</p><table-wrap id="pone.0166204.t005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.t005</object-id><label>Table 5</label><caption><title>Matching results for different local features.</title></caption><alternatives><graphic id="pone.0166204.t005g" xlink:href="pone.0166204.t005"/></alternatives></table-wrap></sec><sec id="sec014"><title>4.3 Recognition with Global Feature Indexing</title><p>Different from the weighted fusion method, the global and local features fusion is implemented in a hierarchical procedure. The 3D ears are pre-classified using global features and then recognized using local features. Thus, much time can be saved and accuracy can be improved in 3D ear recognition. The flowchart of the overall recognition with global feature indexing is shown in <xref ref-type="fig" rid="pone.0166204.g018">Fig 18A</xref>. For a given ear sample, the procedure is as follows:</p><list list-type="order"><list-item><p>Extract the global features of the test sample Angle(Gt), Center(Gt).</p></list-item><list-item><p>Compare Angle(Gt) with global features Angle(Gi) i = 1, &#8230;, N of all ear models (in our experiments, N = 500) to obtain the matching distance Dist(Angle(Gt), Angle(Gi)).</p></list-item><list-item><p>If Dist(Angle(Gt), Angle(Gi)) is smaller than threshold T(&#946;), the ear model is treated as a matched candidate and place it into a sub-database.</p></list-item><list-item><p>Match test ear Gt with the sub-database ears using the empty center feature and adjust the candidate sub-database Gi accordingly.</p></list-item><list-item><p>Extract the local features of VLocal_t and the local features of the ear models in the candidate sub-database VLocal_i (i = 1,&#8230;,k), where k is the total number of ears it contains.</p></list-item><list-item><p>Match local features between VLocal_t and VLocal_i to measure the differences between the test ear and candidate ears (in our experiments, the Euclidean distance was used).</p></list-item><list-item><p>The candidate ear that is closest to the test ear is the recognition result.</p></list-item></list><fig id="pone.0166204.g018" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.g018</object-id><label>Fig 18</label><caption><title>Fusion of global and local features:</title><p>(A) flowchart of recognition with global feature indexing and (B) receiver operating characteristic curve of the global and local feature fusion.</p></caption><graphic xlink:href="pone.0166204.g018"/></fig><p><xref ref-type="fig" rid="pone.0166204.g018">Fig 18B</xref> shows the receiver operating characteristic curve of the results obtained by combining both global and local features together, where the EER is 2.2%. It can be seen that the fusion of global and local features achieves the smallest EER of all schemes, and is even better than single feature matching. This is reasonable, because more information usually leads to more accurate recognition.</p></sec><sec id="sec015"><title>4.4 Performance Analysis</title><p>To better measure the performance of the proposed method, six criteria (database, acquisition device, feature extraction method, average matching time, EER, and online properties) were used to compare the proposed method with other 3D ear recognition methods.</p><p>The results are shown in <xref rid="pone.0166204.t006" ref-type="table">Table 6</xref>.</p><table-wrap id="pone.0166204.t006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0166204.t006</object-id><label>Table 6</label><caption><title>Comparison with existing 3D ear recognition methods.</title></caption><alternatives><graphic id="pone.0166204.t006g" xlink:href="pone.0166204.t006"/></alternatives></table-wrap><p>From <xref rid="pone.0166204.t001" ref-type="table">Table 1</xref> and <xref rid="pone.0166204.t006" ref-type="table">Table 6</xref>, it can be seen that our 3D ear scanner has a lower price (approximately 5% that of the Vivid 910), and a smaller size (approximately 25% that of the vivid 910). Meanwhile, the overall recognition time (including acquisition and recognition time) is less than 2.5 s, and the EER on a database with 2,000 samples is 2.2%. So far, our 3D ear recognition system is the only system offering an overall solution for both 3D ear data acquisition and optimized recognition. Its performance is sufficient to meet the online system requirements for a real-time application.</p></sec></sec><sec id="sec016"><title>5. Conclusions</title><p>In this paper, two novel feature classes, global and local features, were defined and extracted from 3D ear point clouds. The global feature class includes the empty center and ear-parotic area angle, whereas the local feature class consists of point, line, and area features. The experimental results show that all features are stable for the same ear and distinguishable between different ears. Furthermore, global features can be used for indexing, while the combination of both global and local features produces matching results with an EER of 2.2% on our 3D ear database of 2,000 samples. Using our own developed scanner and the optimized recognition method, a real-time 3D ear recognition system is achieved.</p></sec></body><back><ack><p>The authors would like to thank the editor and anonymous reviewers for their help in improving the paper.</p></ack><ref-list><title>References</title><ref id="pone.0166204.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Pang</surname><given-names>Y</given-names></name>, <name><surname>Yuan</surname><given-names>Y</given-names></name>, <name><surname>Li</surname><given-names>X</given-names></name>, <name><surname>Pan</surname><given-names>J</given-names></name>. <article-title>Efficient HOG human detection</article-title>. <source>Signal Processing</source>. <year>2011</year>; <volume>91</volume>(<issue>4</issue>): <fpage>773</fpage>&#8211;<lpage>81</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref002"><label>2</label><mixed-citation publication-type="book"><name><surname>Zhang</surname><given-names>D</given-names></name>. <chapter-title>Automated biometrics: Technologies and systems</chapter-title>: <publisher-name>Springer Science &amp; Business Media</publisher-name>; <year>2000</year>.</mixed-citation></ref><ref id="pone.0166204.ref003"><label>3</label><mixed-citation publication-type="book"><name><surname>Jain</surname><given-names>AK</given-names></name>, <name><surname>Bolle</surname><given-names>R</given-names></name>, <name><surname>Pankanti</surname><given-names>S</given-names></name>. <chapter-title>Biometrics: Personal identification in networked society</chapter-title>: <publisher-name>Springer Science &amp; Business Media</publisher-name>; <year>1999</year>.</mixed-citation></ref><ref id="pone.0166204.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>D</given-names></name>, <name><surname>Kong</surname><given-names>W-K</given-names></name>, <name><surname>You</surname><given-names>J</given-names></name>, <name><surname>Wong</surname><given-names>M</given-names></name>. <article-title>Online palmprint identification</article-title>. <source>Pattern Analysis and Machine Intelligence, IEEE Transactions on</source>. <year>2003</year>; <volume>25</volume>(<issue>9</issue>): <fpage>1041</fpage>&#8211;<lpage>50</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref005"><label>5</label><mixed-citation publication-type="book"><name><surname>Zhang</surname><given-names>Y-B</given-names></name>, <name><surname>Li</surname><given-names>Q</given-names></name>, <name><surname>You</surname><given-names>J</given-names></name>, <name><surname>Bhattacharya</surname><given-names>P</given-names></name>. <chapter-title>Palm vein extraction and matching for personal authentication</chapter-title> In: <source>Advances in Visual Information Systems</source>; <publisher-name>Springer</publisher-name>; <year>2007</year> p. <fpage>154</fpage>&#8211;<lpage>64</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Abaza</surname><given-names>A</given-names></name>, <name><surname>Ross</surname><given-names>A</given-names></name>, <name><surname>Hebert</surname><given-names>C</given-names></name>, <name><surname>Harrison</surname><given-names>MAF</given-names></name>, <name><surname>Nixon</surname><given-names>MS</given-names></name>. <article-title>A survey on ear biometrics</article-title>. <source>ACM Computing Surveys (CSUR)</source>. <year>2013</year>; <volume>45</volume>(<issue>2</issue>): <fpage>22</fpage>.</mixed-citation></ref><ref id="pone.0166204.ref007"><label>7</label><mixed-citation publication-type="other">Abaza A, Ross A. Towards understanding the symmetry of human ears: A biometric perspective. In: Biometrics: Theory Applications and Systems (BTAS), 2010 Fourth IEEE International Conference on; IEEE; 2010. p. 1&#8211;7.</mixed-citation></ref><ref id="pone.0166204.ref008"><label>8</label><mixed-citation publication-type="other">Burge M, Burger W, Ear biometrics in computer vision. In: Pattern Recognition, 2000 15th International Conference on; IEEE; 2000. p. 822&#8211;826.</mixed-citation></ref><ref id="pone.0166204.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Purkait</surname><given-names>R</given-names></name>, <name><surname>Singh</surname><given-names>P</given-names></name>. <article-title>A test of individuality of human external ear pattern: Its application in the field of personal identification</article-title>. <source>Forensic Science International</source>. <year>2008</year>; <volume>178</volume>(<issue>2</issue>): <fpage>112</fpage>&#8211;<lpage>8</lpage>.<pub-id pub-id-type="pmid">18423922</pub-id></mixed-citation></ref><ref id="pone.0166204.ref010"><label>10</label><mixed-citation publication-type="book"><name><surname>Iannarelli</surname><given-names>AV</given-names></name>. <chapter-title>Ear identification</chapter-title>: <publisher-name>Paramont Publishing</publisher-name>; <year>1989</year>.</mixed-citation></ref><ref id="pone.0166204.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Hurley</surname><given-names>DJ</given-names></name>, <name><surname>Nixon</surname><given-names>MS</given-names></name>, <name><surname>Carter</surname><given-names>JN</given-names></name>. <article-title>Force field feature extraction for ear biometrics</article-title>. <source>Computer Vision and Image Understanding</source>. <year>2005</year>; <volume>98</volume>(<issue>3</issue>): <fpage>491</fpage>&#8211;<lpage>512</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Chora&#347;</surname><given-names>M</given-names></name>. <article-title>Ear biometrics based on geometrical feature extraction</article-title>. <source>ELCVIA: Electronic Letters on Computer Vision and Image analysis</source>; <year>2005</year>; <volume>5</volume>(<issue>3</issue>): <fpage>84</fpage>&#8211;<lpage>95</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Kakadiaris</surname><given-names>IA</given-names></name>, <name><surname>Passalis</surname><given-names>G</given-names></name>, <name><surname>Toderici</surname><given-names>G</given-names></name>, <name><surname>Murtuza</surname><given-names>MN</given-names></name>, <name><surname>Lu</surname><given-names>Y</given-names></name>, <name><surname>Karampatziakis</surname><given-names>N</given-names></name>, <etal>et al</etal>
<article-title>Three-dimensional face recognition in the presence of facial expressions: An annotated deformable model approach</article-title>. <source>Pattern Analysis and Machine Intelligence, IEEE Transactions on</source>. <year>2007</year>;<volume>29</volume>(<issue>4</issue>):<fpage>640</fpage>&#8211;<lpage>9</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Samir</surname><given-names>C</given-names></name>, <name><surname>Srivastava</surname><given-names>A</given-names></name>, <name><surname>Daoudi</surname><given-names>M</given-names></name>. <article-title>Three-dimensional face recognition using shapes of facial curves</article-title>. <source>Pattern Analysis and Machine Intelligence, IEEE Transactions on</source>. <year>2006</year>;<volume>28</volume>(<issue>11</issue>):<fpage>1858</fpage>&#8211;<lpage>63</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>D</given-names></name>, <name><surname>Lu</surname><given-names>G</given-names></name>, <name><surname>Li</surname><given-names>W</given-names></name>, <name><surname>Zhang</surname><given-names>D</given-names></name>, <name><surname>Luo</surname><given-names>N</given-names></name>. <article-title>Palmprint recognition using 3-D information</article-title>. <source>Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on</source>. <year>2009</year>;<volume>39</volume>(<issue>5</issue>):<fpage>505</fpage>&#8211;<lpage>19</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>D</given-names></name>, <name><surname>Kanhangad</surname><given-names>V</given-names></name>, <name><surname>Luo</surname><given-names>N</given-names></name>, <name><surname>Kumar</surname><given-names>A</given-names></name>. <article-title>Robust palmprint verification using 2D and 3D features</article-title>. <source>Pattern Recognition</source>. <year>2010</year>;<volume>43</volume>(<issue>1</issue>):<fpage>358</fpage>&#8211;<lpage>68</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>W</given-names></name>, <name><surname>Zhang</surname><given-names>D</given-names></name>, <name><surname>Lu</surname><given-names>G</given-names></name>, <name><surname>Luo</surname><given-names>N</given-names></name>. <article-title>A novel 3-D palmprint acquisition system</article-title>. <source>Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on</source>. <year>2012</year>;<volume>42</volume>(<issue>2</issue>):<fpage>443</fpage>&#8211;<lpage>52</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Yan</surname><given-names>P</given-names></name>, <name><surname>Bowyer</surname><given-names>KW</given-names></name>. <article-title>Biometric recognition using 3D ear shape</article-title>. <source>Pattern Analysis and Machine Intelligence, IEEE Transactions on</source>. <year>2007</year>;<volume>29</volume>(<issue>8</issue>):<fpage>1297</fpage>&#8211;<lpage>308</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>H</given-names></name>, <name><surname>Bhanu</surname><given-names>B</given-names></name>. <article-title>Human ear recognition in 3D</article-title>. <source>Pattern Analysis and Machine Intelligence, IEEE Transactions on</source>. <year>2007</year>;<volume>29</volume>(<issue>4</issue>):<fpage>718</fpage>&#8211;<lpage>37</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>H</given-names></name>, <name><surname>Bhanu</surname><given-names>B</given-names></name>. <article-title>Efficient recognition of highly similar 3D objects in range images</article-title>. <source>Pattern Analysis and Machine Intelligence, IEEE Transactions on</source>. <year>2009</year>;<volume>31</volume>(<issue>1</issue>):<fpage>172</fpage>&#8211;<lpage>9</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>Islam</surname><given-names>SM</given-names></name>, <name><surname>Davies</surname><given-names>R</given-names></name>, <name><surname>Bennamoun</surname><given-names>M</given-names></name>, <name><surname>Mian</surname><given-names>AS</given-names></name>. <article-title>Efficient detection and recognition of 3D ears</article-title>. <source>International Journal of Computer Vision</source>. <year>2011</year>;<volume>95</volume>(<issue>1</issue>):<fpage>52</fpage>&#8211;<lpage>73</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Islam</surname><given-names>S</given-names></name>, <name><surname>Bennamoun</surname><given-names>M</given-names></name>, <name><surname>Owens</surname><given-names>RA</given-names></name>, <name><surname>Davies</surname><given-names>R</given-names></name>. <article-title>A review of recent advances in 3D ear-and expression-invariant face biometrics</article-title>. <source>ACM Computing Surveys (CSUR)</source>. <year>2012</year>; <volume>44</volume>(<issue>3</issue>): <fpage>14</fpage>.</mixed-citation></ref><ref id="pone.0166204.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>J</given-names></name>, <name><surname>Cadavid</surname><given-names>S</given-names></name>, <name><surname>Abdel-Mottaleb</surname><given-names>M</given-names></name>. <article-title>An efficient 3-D ear recognition system employing local and holistic features</article-title>. <source>Information Forensics and Security, IEEE Transactions on</source>. <year>2012</year>;<volume>7</volume>(<issue>3</issue>):<fpage>978</fpage>&#8211;<lpage>91</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref024"><label>24</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>L</given-names></name>, <name><surname>Ding</surname><given-names>Z</given-names></name>, <name><surname>Li</surname><given-names>H</given-names></name>, <name><surname>Shen</surname><given-names>Y</given-names></name>. <article-title>3D Ear Identification Based on Sparse Representation</article-title>. <source>PLoS One</source>. <year>2014</year>; <volume>9</volume>(<issue>4</issue>): <fpage>e95506</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pone.0095506</pub-id>
<?supplied-pmid 24740247?><pub-id pub-id-type="pmid">24740247</pub-id></mixed-citation></ref><ref id="pone.0166204.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>L</given-names></name>, <name><surname>Mu</surname><given-names>Z</given-names></name>, <name><surname>Zhang</surname><given-names>B</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>. <article-title>Ear Recognition from One Sample Per Person</article-title>. <source>PLoS One</source>. <year>2015</year>; <volume>10</volume>(<issue>5</issue>): <fpage>e0129505</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pone.0129505</pub-id>
<?supplied-pmid 26024226?><pub-id pub-id-type="pmid">26024226</pub-id></mixed-citation></ref><ref id="pone.0166204.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Lu</surname><given-names>G</given-names></name>, <name><surname>Zhang</surname><given-names>D</given-names></name>. <article-title>An effective 3D ear acquisition system</article-title>. <source>PLoS One</source>. <year>2015</year>; <volume>10</volume>(<issue>6</issue>): <fpage>e0129439</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pone.0129439</pub-id>
<?supplied-pmid 26061553?><pub-id pub-id-type="pmid">26061553</pub-id></mixed-citation></ref><ref id="pone.0166204.ref027"><label>27</label><mixed-citation publication-type="other">Huang C, Lu G, Liu Y. Coordinate direction normalization using point cloud projection density for 3D Ear. In: Fourth International Conference on Computer Sciences and Convergence Information Technology; IEEE; 2009. p. 511&#8211;5</mixed-citation></ref><ref id="pone.0166204.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Zhang</surname><given-names>B</given-names></name>, <name><surname>Zhang</surname><given-names>D</given-names></name>. <article-title>Ear-Parotic face angle: A unique feature for 3D ear recognition</article-title>. <source>Pattern Recognition Letters</source>. <year>2014</year>;<volume>53</volume>:<fpage>9</fpage>&#8211;<lpage>15</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Mian</surname><given-names>AS</given-names></name>, <name><surname>Bennamoun</surname><given-names>M</given-names></name>, <name><surname>Owens</surname><given-names>R</given-names></name>. <article-title>Keypoint detection and local feature matching for textured 3D face recognition</article-title>. <source>International Journal of Computer Vision</source>. <year>2008</year>;<volume>79</volume>(<issue>1</issue>):<fpage>1</fpage>&#8211;<lpage>12</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Pang</surname><given-names>Y</given-names></name>, <name><surname>Li</surname><given-names>X</given-names></name>, <name><surname>Yuan</surname><given-names>Y</given-names></name>. <article-title>Robust Tensor tensor Analysis analysis With with L1-Normnorm</article-title>. <source>IEEE Transactions on Circuits &amp; Systems for Video Technology</source>. <year>2010</year>;<volume>20</volume>(<issue>2</issue>):<fpage>172</fpage>&#8211;<lpage>8</lpage>.</mixed-citation></ref><ref id="pone.0166204.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Pang</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>L</given-names></name>, <name><surname>Yuan</surname><given-names>Y</given-names></name>. <article-title>Generalized KPCA by adaptive rules in feature space</article-title>. <source>International Journal of Computer Mathematics</source>. <year>2010</year>;<volume>87</volume>(<issue>5</issue>):<fpage>956</fpage>&#8211;<lpage>68</lpage>.</mixed-citation></ref></ref-list></back></article>