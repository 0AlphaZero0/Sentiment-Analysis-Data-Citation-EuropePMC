<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName journalpublishing.dtd?><?SourceDTD.Version 2.3?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Ear Hear</journal-id><journal-id journal-id-type="iso-abbrev">Ear Hear</journal-id><journal-id journal-id-type="publisher-id">AUD</journal-id><journal-title-group><journal-title>Ear and Hearing</journal-title></journal-title-group><issn pub-type="ppub">0196-0202</issn><issn pub-type="epub">1538-4667</issn><publisher><publisher-name>Williams And Wilkins</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">5839701</article-id><article-id pub-id-type="art-access-id">00006</article-id><article-id pub-id-type="doi">10.1097/AUD.0000000000000480</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Articles</subject></subj-group></article-categories><title-group><article-title>Discrimination of Voice Pitch and Vocal-Tract Length in Cochlear Implant Users</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Gaudrain</surname><given-names>Etienne</given-names></name><xref ref-type="aff" rid="aff1">1,2</xref></contrib><contrib contrib-type="author"><name><surname>Ba&#351;kent</surname><given-names>Deniz</given-names></name><xref ref-type="aff" rid="aff1">1,3</xref></contrib><aff id="aff1"><label>1</label>University of Groningen, University Medical Center Groningen, Department of Otorhinolaryngology-Head and Neck Surgery, Groningen, The Netherlands; <label>2</label>CNRS UMR 5292, Lyon Neuroscience Research Center, Auditory Cognition and Psychoacoustics, Universit&#233; Lyon, Lyon, France; and <label>3</label>Research School of Behavioral and Cognitive Neurosciences, Graduate School of Medical Sciences, University of Groningen, Groningen, The Netherlands.</aff></contrib-group><author-notes><corresp id="c1">Address for correspondence: Etienne Gaudrain, Department of Otorhinolaryngology-Head and Neck Surgery, University Medical Center Groningen, University of Groningen, Huispostcode BB20, P.O. Box 30.001, 9700 RB, Groningen, The Netherlands. E-mail: <email xlink:href="etienne.gaudrain@cnrs.fr">etienne.gaudrain@cnrs.fr</email></corresp></author-notes><pub-date pub-type="ppub"><month>3</month><year>2018</year></pub-date><pub-date pub-type="epub"><day>23</day><month>2</month><year>2018</year></pub-date><volume>39</volume><issue>2</issue><fpage>226</fpage><lpage>237</lpage><history><date date-type="received"><day>9</day><month>1</month><year>2017</year></date><date date-type="accepted"><day>29</day><month>6</month><year>2017</year></date></history><permissions><copyright-statement>Copyright &#169; 2017 The Author(s). Ear &amp; Hearing is published on behalf of the American Auditory Society, by Wolters Kluwer Health, Inc.</copyright-statement><copyright-year>2017</copyright-year><license license-type="open-access"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-Non Commercial-No Derivatives License 4.0 (CCBY-NC-ND)</ext-link>, where it is permissible to download and share the work provided it is properly cited. The work cannot be changed in any way or used commercially without permission from the journal.</license-p></license></permissions><self-uri xlink:type="simple" xlink:href="aud-39-226.pdf"/><abstract><sec><title>Objectives:</title><p>When listening to two competing speakers, normal-hearing (NH) listeners can take advantage of voice differences between the speakers. Users of cochlear implants (CIs) have difficulty in perceiving speech on speech. Previous literature has indicated sensitivity to voice pitch (related to fundamental frequency, F0) to be poor among implant users, while sensitivity to vocal-tract length (VTL; related to the height of the speaker and formant frequencies), the other principal voice characteristic, has not been directly investigated in CIs. A few recent studies evaluated F0 and VTL perception indirectly, through voice gender categorization, which relies on perception of both voice cues. These studies revealed that, contrary to prior literature, CI users seem to rely exclusively on F0 while not utilizing VTL to perform this task. The objective of the present study was to directly and systematically assess raw sensitivity to F0 and VTL differences in CI users to define the extent of the deficit in voice perception.</p></sec><sec><title>Design:</title><p>The just-noticeable differences (JNDs) for F0 and VTL were measured in 11 CI listeners using triplets of consonant&#8211;vowel syllables in an adaptive three-alternative forced choice method.</p></sec><sec><title>Results:</title><p>The results showed that while NH listeners had average JNDs of 1.95 and 1.73 semitones (st) for F0 and VTL, respectively, CI listeners showed JNDs of 9.19 and 7.19 st. These JNDs correspond to differences of 70% in F0 and 52% in VTL. For comparison to the natural range of voices in the population, the F0 JND in CIs remains smaller than the typical male&#8211;female F0 difference. However, the average VTL JND in CIs is about twice as large as the typical male&#8211;female VTL difference.</p></sec><sec><title>Conclusions:</title><p>These findings, thus, directly confirm that CI listeners do not seem to have sufficient access to VTL cues, likely as a result of limited spectral resolution, and, hence, that CI listeners&#8217; voice perception deficit goes beyond poor perception of F0. These results provide a potential common explanation not only for a number of deficits observed in CI listeners, such as voice identification and gender categorization, but also for competing speech perception.</p></sec></abstract><kwd-group><kwd>Fundamental frequency</kwd><kwd>Psychophysics</kwd><kwd>Voice perception</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>OPEN-ACCESS</meta-name><meta-value>TRUE</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro"><title>INTRODUCTION</title><p>When trying to understand two competing talkers, normal-hearing (NH) listeners greatly benefit from voice differences among the talkers. <xref rid="R7" ref-type="bibr">Brungart (2001)</xref> showed that this benefit could reach 50 percentage-points for speakers of differing sexes. For cochlear implant (CI) users, in a similar task, <xref rid="R75" ref-type="bibr">Stickney et al. (2004)</xref> showed that this benefit was less than 20 percentage-points. Although it is debatable whether these percentage-point differences are indeed comparable because both the reference performance and target-to-masker ratios were not the same (<xref rid="R18" ref-type="bibr">Deroche et al. 2017</xref>), this observation nevertheless raises the question whether CI listeners have difficulties in perceiving and effectively utilizing voice cues.</p><p>Most of the literature on perception of voice cues by CI users indicates that CI listeners notoriously suffer from poor pitch perception (see <xref rid="R60" ref-type="bibr">Moore &amp; Carlyon 2005 for a review</xref>). CI listeners have greater difficulties than NH listeners with distinguishing questions from statements (<xref rid="R38" ref-type="bibr">Green et al. 2005</xref>; <xref rid="R10" ref-type="bibr">Chatterjee &amp; Peng 2008</xref>), discriminating pitch contours in tonal languages (<xref rid="R39" ref-type="bibr">He et al. 2016</xref>), and do not benefit from fundamental frequency (F0) differences between competing speakers (<xref rid="R76" ref-type="bibr">Stickney et al. 2007</xref>). F0 gives rise to the voice-pitch percept and may be coded either through temporal cues or through place cues. The perception of temporal pitch is based on the periodicity of the signal or of the temporal envelope of the signal. Temporal pitch perception seems to be relatively preserved in CI listeners, and they demonstrate functional use of this pitch cue similar to that of NH listeners (<xref rid="R43" ref-type="bibr">Hong &amp; Turner 2009</xref>; <xref rid="R17" ref-type="bibr">Deroche et al. 2014</xref>; <xref rid="R29" ref-type="bibr">Gaudrain et al. 2017</xref>), as long as F0 remains below 300 Hz (<xref rid="R70" ref-type="bibr">Shannon 1983</xref>; <xref rid="R9" ref-type="bibr">Carlyon et al. 2002</xref>, <xref rid="R8" ref-type="bibr">2010</xref>; <xref rid="R82" ref-type="bibr">Zeng 2002</xref>). In contrast, place pitch, that is, the pitch that results from exciting different segments of the cochlea, seems to be more difficult to use for CI listeners (<xref rid="R32" ref-type="bibr">Geurts &amp; Wouters 2001</xref>; <xref rid="R47" ref-type="bibr">Laneau et al. 2004</xref>), in particular in speech-like stimuli where dynamic spectral envelope fluctuations may interfere with spectral changes induced by F0 differences (<xref rid="R36" ref-type="bibr">Green et al. 2002</xref>, <xref rid="R37" ref-type="bibr">2004</xref>).</p><p>However, F0 is not the only cue that differentiates voices. Among many potential speaker-specific characteristics (<xref rid="R1" ref-type="bibr">Abercrombie 1982</xref>), not one but two principal vocal characteristics seem to mostly contribute to voice differences: F0, but also vocal-tract length (VTL). VTL is highly correlated with the height of the speaker (<xref rid="R21" ref-type="bibr">Fitch &amp; Giedd 1999</xref>) and is related to the formant frequencies in the spectral envelope of the speech signal. Many voice-related perceptual phenomena observed in NH listeners can only be explained by considering both F0 and VTL together. For instance, the concurrent speech intelligibility increase observed by <xref rid="R7" ref-type="bibr">Brungart (2001)</xref> cannot be explained by F0 differences alone but can be replicated when F0 and VTL differences are combined (<xref rid="R16" ref-type="bibr">Darwin et al. 2003</xref>). F0 and VTL are also the two principal voice characteristics that are associated with gender and age perception (<xref rid="R72" ref-type="bibr">Smith &amp; Patterson 2005</xref>; <xref rid="R73" ref-type="bibr">Smith et al. 2005</xref>; <xref rid="R71" ref-type="bibr">Skuk &amp; Schweinberger 2014</xref>), or more generally, with voice identity (<xref rid="R85" ref-type="bibr">Gaudrain et al, Reference Note 3</xref>).</p><p>It is, thus, clear that not only F0 but also VTL plays an important role in voice perception in NH listeners. However, the relative importance of these two voice cues may be different in CI listeners. Using different talkers, <xref rid="R75" ref-type="bibr">Stickney et al. (2004)</xref> found that CI listeners could, modestly, benefit from speaker differences in a speech-on-speech task. However, when the voices only differed in F0, no such benefit was observed (<xref rid="R76" ref-type="bibr">Stickney et al. 2007</xref>). Although it can be debated whether the NH and CI data can be directly compared, as discussed above, this discrepancy led the authors to speculate that VTL differences may play a role for speech-on-speech perception in CI users. In contrast, some studies on voice gender perception suggested that, unlike NH listeners, CI listeners may be relying strongly on F0 cues to identify the sex of a speaker (<xref rid="R24" ref-type="bibr">Fu et al. 2004</xref>, <xref rid="R25" ref-type="bibr">2005</xref>; <xref rid="R46" ref-type="bibr">Kovaci&#263; &amp; Balaban 2009</xref>; <xref rid="R49" ref-type="bibr">Li &amp; Fu 2011</xref>).</p><p>Studies involving real speakers may give general indications of which voice dimensions may contribute to specific phenomena. However, only with careful manipulation of these voice characteristics is it possible to ascertain, and quantify, the role that F0 and VTL may play. <xref rid="R56" ref-type="bibr">Massida et al. (2013)</xref> showed, using morphing between recorded male and female voices, that CI listeners were less sensitive to voice gender differences than NH listeners. To clarify the role of F0 and VTL, <xref rid="R26" ref-type="bibr">Fuller et al. (2014)</xref> varied the two variables orthogonally in a voice gender categorization task and found that while NH listeners made use of both F0 and VTL, CI listeners relied exclusively on the 1-octave F0 difference separating the male voice from the female voice. Fuller et al., hence, concluded that the abnormal voice gender categorization in CI listeners is mainly due to a deficit in VTL perception. This conclusion was recently confirmed by <xref rid="R58" ref-type="bibr">Meister et al. (2016)</xref> using a wider range of stimuli than words only, also including word quadruples and sentences. Hence, unlike F0, VTL has been seldom studied in CI users, and very little is known about the factors that may limit its perception.</p><p>The acoustic cue for VTL lies in the formant frequencies, which result from resonances in the vocal tract. For a given formant configuration, shortening the VTL by a given ratio <italic>r</italic> results in shifting all formants up in frequency by that same ratio (Fig. <xref ref-type="fig" rid="F1">1</xref>). This results into a translation of all formants&#8212;as a unit&#8212;on a logarithmic frequency axis. If represented onto a linear frequency axis, a VTL change is reflected as an expansion/contraction of the formant distances. To make it a frequency-compatible unit, this ratio can be expressed in semitones (st, the 12th of an octave) using 12&#183;log<sub>2</sub> (<italic>r</italic>). Semitones are used in music and represent an intuitive frequency increment, thus providing a perceptually relevant unit while not relying on a specific perceptual model like Bark or ERB (equivalent rectangular bandwidth). Note that the VTL is a distance, related to wavelength, and, thus, inversely related to frequency; positive VTL ratios in semitones correspond to negative formant frequency shifts in semitones. In adult speakers, VTL is, on average, longer in men than in women by about 23%, leading male formants to be about 3.6 st lower in frequency than female formants. Examining the output of the implant, <xref rid="R26" ref-type="bibr">Fuller et al. (2014)</xref> observed that this 3.6-st frequency shift in formant frequency was roughly equivalent to a one-electrode shift in the electrical pattern of excitation. Because speech has a complex spectral structure, it is hard to predict whether such a VTL difference can be detected by CI users.</p><fig id="F1" position="float"><label>Fig. 1.</label><caption><p>Harmonic spectra (vertical lines) and vocal-tract resonances (dashed line) of an idealized vowel. The vocal-tract resonance profiles were generated using a three-tube model (mimicking different vocal-tract lengths) and were then used to define the spectral envelope of the harmonics. Comparing (<bold>B</bold>) and (<bold>A</bold>) shows the effect of increasing F0 by 12 st. Comparing (<bold>B</bold>) and (<bold>C</bold>) shows the effect of decreasing vocal-tract length by 3.6 st. The vertical dash-dotted lines mark the formant positions in the original vowel.</p></caption><graphic xlink:href="aud-39-226-g001"/></fig><p>A first potential limitation to the transmission of VTL cues through the implant is the poor spectral resolution available due to the limitations of the electrode&#8211;neuron interface (see <xref rid="R2" ref-type="bibr">Ba&#351;kent et al. 2016 for a review</xref>). This may severely reduce precise access to individual formants and make it impossible for listeners to detect small&#8212;but consistent&#8212;changes in their position. An additional complication in the implant comes from the fact that, because the spectral envelope is quantized into a number of discrete frequency bands in the implant, not all formants shift by the same amount. Depending on the formant frequency, a 3.6-st frequency shift can remain within a frequency band or jump up to two electrodes. In other words, the VTL cues may not be transmitted due to coarse spectral resolution available, and if transmitted, the VTL cues could still be severely distorted.</p><p>Previous studies using acoustic simulations of CIs gave only a partial answer. <xref rid="R28" ref-type="bibr">Gaudrain and Ba&#351;kent (2015)</xref> showed perception of VTL to be more vulnerable than that of F0 to the kind of degradations that the CI stimulation imposes on the speech signal. However, the simulations of <xref rid="R26" ref-type="bibr">Fuller et al. (2014)</xref> showed a pattern that differed from that of actual CI users. In their study, the uses of F0 and VTL for gender categorization were both significantly reduced as a result of vocoding; however, NH listeners still seemed to make some use of both cues. CI users, on the other hand, only exclusively utilized F0 cues and no VTL cues. Hence, it remains unclear whether the CI participants in Fuller et al.&#8217;s study were unable to use VTL cues for gender categorization because VTL cues are not represented through electrical stimulation, or if VTL cues are represented, but in a form that is too distorted to be used for the subjective labeling of the gender of a voice.</p><p>In the present study, we aimed to directly answer the question of to what degree actual CI users can hear F0 and VTL cues. More specifically, in a design similar to <xref rid="R28" ref-type="bibr">Gaudrain and Ba&#351;kent (2015)</xref>, we measured F0 and VTL just-noticeable differences (JNDs) in actual CI users. In a gender categorization task, the listener has to correctly hear and interpret the F0 and VTL cues as such to make the correct labeling of the sex. Here, unlike in the gender task, we aimed to directly measure the raw sensitivity to hearing any perceptual difference that results from a F0 and VTL change, in an odd-one-out task. On the basis of previous literature on pitch perception, gender categorization, and the simulation study on F0 and VTL perception, we hypothesized that CI users would show a higher threshold for perception of F0 and VTL than NH individuals; however, this difference would be more pronounced for VTL perception.</p></sec><sec sec-type="methods"><title>METHODS</title><sec><title>Participants</title><p>Eleven CI listeners were recruited (6 men), aged 47 to 74 years (average: 60.1 years). They all had more than 1 year experience with their implant (details provided in Table <xref ref-type="table" rid="T1">1</xref>). The participants had no substantial residual hearing, except CI1 who had postoperative thresholds of 35 and 50 dB HL at 250 and 500 Hz, respectively, in the nonimplanted ear. CI1 was not wearing a hearing aid in the nonimplanted ear during testing, and that ear was also not plugged. All participants were native speakers of Dutch. No other speech perception or language-processing performance was used for inclusion criterion. As a result, the participants had a range of scores from the clinical speech perception assessment (Table <xref ref-type="table" rid="T1">1</xref>, rightmost column). This range ensured that the study population represented good, but not only star, users of CIs.</p><table-wrap id="T1" position="float"><label>TABLE 1.</label><caption><p>Details of the Participants of the Study</p></caption><graphic xlink:href="aud-39-226-g002"/></table-wrap><p>Each participant provided signed informed consent. The experiment was approved by the ethics committee of the University Medical Center Groningen (METc 2012.392). Finally, the participants received an hourly wage for their participation.</p></sec><sec><title>Stimuli</title><p>The JNDs were measured using a 3I-3AFC adaptive odd-one-out procedure where the participants had to point to which of three consecutive stimuli was different from the two others. The listeners could use any cue available to perform the task. Depending on the specific condition, the odd one out differed in F0 only, in VTL only, or both together.</p><p>To ensure that the stimuli used in this experiment remain relevant for speech perception, each interval of the 3I-3AFC procedure was made of a syllable triplet&#8212;a sequence of three short syllables, similar to a pseudo-word. The syllables were consonant&#8211;vowel (CV) tokens spliced from meaningful consonant&#8211;vowel&#8211;consonant words taken from the Nederlandse Vereniging voor Audiologie corpus (<xref rid="R6" ref-type="bibr">Bosman &amp; Smoorenburg 1995</xref>). The words were uttered by a female speaker, the same speaker as for the words used by <xref rid="R26" ref-type="bibr">Fuller et al. (2014)</xref> to study gender categorization. The average F0 of the speaker&#8217;s voice measured on all the words was 242 Hz. From a selection of the available words in the corpus, 61 CV syllables were obtained, with durations ranging from 142 to 200 ms.</p><p>In the course of the adaptive procedure, the F0 and VTL of individual syllables were manipulated using <sc>Straight</sc> (<xref rid="R45" ref-type="bibr">Kawahara &amp; Irino 2004</xref>). During this process, the duration of each CV token was also normalized to 200 ms.</p><p>The syllable triplets were created by forming a sequence where the 200-ms syllables were separated by a 50-ms silence. To make the triplet sound more natural and to force the participants to focus on average&#8212;rather than instantaneous&#8212;F0, the F0s of the three syllables were not kept identical, but a slight F0 contour was imposed over the sequence (<xref rid="R73" ref-type="bibr">Smith et al. 2005</xref>). This was done by imposing random steps of 1/3 st between consecutive tokens in the sequence, while maintaining the average F0 (over the sequence) equal to the value set by the adaptive procedure (<xref rid="R28" ref-type="bibr">Gaudrain &amp; Ba&#351;kent 2015</xref>). While in previous studies the CV syllables changed from one interval to the next (<xref rid="R44" ref-type="bibr">Ives et al. 2005</xref>; <xref rid="R73" ref-type="bibr">Smith et al. 2005</xref>), we took the approach to keep the same syllable triplet over the three intervals. This adaptation was necessary to keep the task relatively easy, because we were testing CI users, and also to reduce the risk that changes in syllables could be interpreted as changes in voice. In our design, the participants only had to judge three voice intervals, two of which were identical, while in previous studies, it could be argued that the participants had to judge&#8212;and thus memorize&#8212;as many intervals as there were vowels or syllables (8 syllables in <xref rid="R44" ref-type="bibr">Ives et al. 2005</xref>; 8 vowels in <xref rid="R73" ref-type="bibr">Smith et al. 2005</xref>).</p><p>Finally, the intensity level of each triplet was roved by &#177;2 dB to prevent the use of loudness cues by the participants. This 2-dB value corresponds to the SD of the maximum loudness evaluated across all the syllables with different F0 and VTL values, as evaluated using the method described by <xref rid="R35" ref-type="bibr">Glasberg and Moore (2002)</xref>.</p></sec><sec sec-type="methods"><title>Procedure and Apparatus</title><p>The JNDs were measured with a two-down, one-up adaptive procedure, yielding an estimate of the voice difference corresponding to 70.7% correct discrimination on the psychometric function (<xref rid="R48" ref-type="bibr">Levitt 1971</xref>). In each trial, the participants heard three stimuli as they saw three buttons, numbered one to three, on a computer screen light up as each corresponding stimulus was being played. They were then instructed to choose the one stimulus that differed from the other two by clicking on the corresponding button on the computer screen. Visual feedback was provided by making the correct answer blink either green if the answer was correct or red if it was incorrect.</p><p>In all measurements, the test voice became progressively more similar, in terms of F0 and VTL, to the reference female voice, but the reference voice was approached along different axes in the F0-VTL plane and from different directions along these axes, as shown in Figure <xref ref-type="fig" rid="F2">2</xref>. VTL JNDs were measured in two directions: starting from larger (+VTL) and smaller (&#8722;VTL) VTL values than the reference voice. Similarly, F0 JNDs were measured starting from lower (&#8722;F0) and from higher (+F0) F0 values than the reference voice. In a fifth condition, combining changes on both F0 and VTL, JNDs were measured along a continuum between an artificial man&#8217;s voice and the reference female voice. The man&#8217;s voice was defined as having a VTL 24.5% (3.8 st) longer than that of the reference female voice, and an F0 half (&#8722;12 st) of that of the reference voice (as used by <xref rid="R28" ref-type="bibr">Gaudrain &amp; Ba&#351;kent 2015</xref>). The JNDs were measured in only one direction along this axis: from the man&#8217;s voice toward the reference female voice.</p><fig id="F2" position="float"><label>Fig. 2.</label><caption><p>F0-vocal-tract length (VTL) plane with the reference female voice in the center, and the man&#8217;s voice in the lower left corner. The axes are shown with dashed lines, while the approach directions and their labels are shown with thick arrows. Note that the VTL axis is represented upside down as it represents a length, which is associated with a wavelength dimension, while F0 represents a frequency. The orthogonality of the two axes illustrates their independence in the physical domain, where they are manipulated.</p></caption><graphic xlink:href="aud-39-226-g003"/></fig><p>For all axes and directions, the initial voice difference was 12 st, calculated as the Euclidian distance in the F0-VTL plane represented in semitones relative to the reference voice. After two consecutive correct answers, the voice difference was reduced by a certain step size, while after every incorrect answer, the voice difference was increased by that same step size. The initial step size was 2 st but was also modified during the procedure. After every 15 trials, or when the voice difference became smaller than twice the step size, the step size was reduced by a factor of <inline-graphic xlink:href="aud-39-226-i001.jpg"/>. The procedure ended after 8 reversals or after 150 trials. The JND was calculated as the mean of the voice difference from the last 6 reversals. For each axis and direction, 3 repetitions of the JNDs were obtained per participant. The 5 Directions &#215; 3 Repetitions = a total of 15 JND measurements were tested in random order for each participant.</p><p>The participants were seated in an anechoic room about 1 m away from a loudspeaker (Precision 80, Tannoy, Coatbridge, United Kingdom). The stimuli were played through an AudioFire4 soundcard (Echo Digital Audio Corp, Santa Barbara, CA) connected to a DA10 D/A converter (Lavry Engineering, Poulsbo, WA) through S/PDIF (Sony/Philips Digital Interconnect Format). The sound level was calibrated to 63 dB SPL. At the beginning of each testing session, the participant took part in a short training consisting of the same adaptive procedure but limited to 8 trials. During this training, they were instructed to adjust the gain setting on their implant so that the sound level would be comfortable and to keep it the same throughout the data collection. The average testing time was 1 hour 35 minutes (SD 15.4 minutes) to which was added about 15 to 20 minutes for reading and completing the informed consent form and for training for the task.</p></sec></sec><sec sec-type="results"><title>RESULTS</title><p>Individual results are shown in Figure <xref ref-type="fig" rid="F3">3</xref>, ordered according to increasing VTL JNDs. The approach direction along the F0 or VTL axes had no significant effect on the CI listeners&#8217; JNDs [(+F0) &#8722; (&#8722;F0) = &#8722;0.12 st, <italic>t</italic>(10) = &#8722;0.12; <italic>p</italic> = 0.92; (+VTL) &#8722; (&#8722;VTL) = 0.51 st, <italic>t</italic>(10) = &#8722;0.55; <italic>p</italic> = 0.59]; hence, in subsequent analyses, and on Figure 3, these data were collapsed by averaging.</p><fig id="F3" position="float"><label>Fig. 3.</label><caption><p><bold>A</bold>, Just-noticeable difference (JND) for vocal-tract length (VTL), shown in average for normal-hearing (NH) listeners (dark/purple) and individually for cochlear implant (CI) listeners (light/yellow). The JNDs have been averaged across positive and negative VTL differences (shown in Fig. <xref ref-type="fig" rid="F2">2</xref>). The data of the CI participants are ordered by increasing VTL JNDs. The error bars for the NH data show the SE across participants. The error bars for the CI individuals show the SE across measurements (directions and repetitions). The dashed lines show the average JNDs for the CI participants. The solid horizontal line represents a 3.6-st VTL difference, corresponding to the average difference between the man and woman voices in <xref rid="R26" ref-type="bibr">Fuller et al. (2014)</xref>. <bold>B</bold>, Same as the top panel but shown for the &#8220;man&#8221; voice axis. The solid line represents a 6.3-st difference along the &#8220;man&#8221; voice axis. <bold>C</bold>, Same as the top panel, but shown for F0. The solid line represents a 12-st F0 difference, corresponding to the average difference between the man and woman voices in <xref rid="R26" ref-type="bibr">Fuller et al. (2014)</xref>.</p></caption><graphic xlink:href="aud-39-226-g004"/></fig><p>For the interpretation of the results, the JNDs from the CI listeners are compared with those of NH listeners (tested without acoustic CI simulations) from <xref rid="R28" ref-type="bibr">Gaudrain and Ba&#351;kent (2015)</xref>. In addition, we also compared the JNDs to the typical F0 and VTL differences used to distinguish male from female speakers in the gender categorization study of <xref rid="R26" ref-type="bibr">Fuller et al. (2014)</xref>.</p><p>The mean F0 JND was 9.19 st for CI users, against 1.95 st for NH listeners [<italic>t</italic>(12.24) = 4.03; <italic>p</italic> &lt; 0.01]. The mean VTL JND was 7.19 st for CI listeners, against 1.73 st for NH listeners [<italic>t</italic>(11.11) = 6.28; <italic>p</italic> &lt; 0.001]. Check Table <xref ref-type="table" rid="T2">2</xref> to see these JND values expressed in other units than semitones. The mean JND along the man voice axis was 9.67 st for CI listeners against 1.71 st for NH listeners [<italic>t</italic>(10.82) = 3.84; <italic>p</italic> &lt; 0.01]. This JND, in CI users, can be decomposed as 9.22 st along the F0 axis (almost identical to the measured F0 JND), and 2.92 st along the VTL axis (much smaller than the VTL JND), thus suggesting that CI listeners rely on the F0 cue to perform the task in this condition. In NH users, the 1.71 st JND can be decomposed into 1.63 st along the F0 axis and 0.52 st along the VTL axis, both smaller than the JNDs for F0 alone (2.68 st in the direction of lower F0s) and for VTL alone (1.62 st in the direction of longer VTLs) and thus suggesting an additive effect of the two dimensions (<xref rid="R28" ref-type="bibr">Gaudrain &amp; Ba&#351;kent 2015</xref>).</p><table-wrap id="T2" position="float"><label>TABLE 2.</label><caption><p>Average F0 and VTL JNDs Expressed in Various Units, in NH and CI Listeners</p></caption><graphic xlink:href="aud-39-226-g005"/></table-wrap><p>This conclusion is based on the observation of the average JNDs over CI participants. To assess whether this relationship holds at the individual level, the correlations of the individual JNDs across the different axes were examined. The average F0 JNDs (across directions along this axis) for each participant were correlated with their average VTL JNDs [<italic>r</italic><sup>2</sup> = 0.45; <italic>t</italic>(9) = 2.70; <italic>p</italic> &lt; 0.05], indicating that participants with larger F0 JNDs also tended to have larger VTL JNDs. The man JNDs seem more strongly correlated with F0 [<italic>r</italic><sup>2</sup> = 0.91; <italic>t</italic>(9) = 9.27; <italic>p</italic> &lt; 0.001] than with VTL [<italic>r</italic><sup>2</sup> = 0.63; <italic>t</italic>(9) = 3.91; <italic>p</italic> &lt; 0.01], although the comparison of these two correlation coefficients only approached significance [<italic>z</italic> = 1.89; <italic>p</italic> = 0.059; <xref rid="R74" ref-type="bibr">Steiger 1980</xref>].</p><p>In addition to comparing the JNDs measured in CI users to those collected with NH listeners, they can also be compared with actual VTL and F0 differences found between voices in the population. The solid horizontal lines in Figure <xref ref-type="fig" rid="F3">3</xref> represent typical F0 and VTL differences separating men voices from women voices and were used to create the man&#8217;s voice from the original woman&#8217;s recording in the gender categorization experiment of <xref rid="R26" ref-type="bibr">Fuller et al. (2014)</xref>. Only 1 of our 11 CI participants had an average VTL JND smaller than the typical man&#8211;woman VTL difference (Fig. <xref ref-type="fig" rid="F3">3</xref>A). Having a JND larger than this typical difference is an indication that these 10 CI participants would not be able to use VTL differences to perceive the sex of a voice. In contrast, 7 of our 11 CI participants had an average F0 JND smaller than the typical man&#8211;woman F0 difference (Fig. <xref ref-type="fig" rid="F3">3</xref>C).</p><p>Although the observation is rather anecdotal, it is worth noting that CI1, who had substantial residual hearing in the nonimplanted ear, had F0 JNDs on par with those of the NH listeners. In contrast, their VTL JNDs were at least twice as large as that of the NH listeners, thus suggesting that residual hearing may not contribute as much to VTL perception as it contributes to F0 perception. However, because these results are limited to a single participant, caution is required in drawing that conclusion until a larger population of CI users with residual hearing has been tested. A first element of response is provided by a recent study using noise-band vocoders to simulate electroacoustic stimulation and confirming the present results (<xref rid="R83" ref-type="bibr">Ba&#351;kent et al., Reference Note 1</xref>).</p><p>Similarly, the 3 participants equipped with an Advanced Bionics device using a current steering strategy (CI2, CI8, and CI9) seem to perform well on both F0 and VTL JNDs. However, the collected data are too sparse to be able to control for possible confounds and draw reliable conclusions regarding the effect of brand, processor, or strategy.</p></sec><sec sec-type="discussion"><title>DISCUSSION</title><p>In this study, we have directly and systematically investigated the perception of two principal voice characteristics, F0 and VTL, in CI listeners. Our results have shown that voice perception in CI users not only differs from that of NH, tending to be poorer in general, but also, this deficiency is more complex and serious than was previously reported in the literature, which had mostly focused on F0 perception.</p><sec><title>F0 Perception With Cochlear Implants</title><p>Pitch perception in CI users has attracted a lot of attention from researchers and has been shown to be poorer than that of NH listeners. In line with this idea, the present JND data indeed show that the F0 JNDs in CI listeners are more than 4 times larger than those observed in NH listeners. However, most studies have used very artificial stimuli and only a few have used broadband speech-like stimuli spanning across all the electrodes.</p><p><xref rid="R33" ref-type="bibr">Gfeller et al. (2002)</xref> used 1-second piano tones on a semitone scale and observed JNDs of 1.13 st in NH and 7.56 st in CI listeners. <xref rid="R32" ref-type="bibr">Geurts and Wouters (2001)</xref> observed in CI listeners strikingly small F0 JNDs ranging from 0.22 to 2.4 st in steady state synthetic vowels /a/ and /i/, but loudness could have played a role in these JNDs. <xref rid="R47" ref-type="bibr">Laneau et al. (2004)</xref> found F0 discrimination JNDs larger than 17 st when only place cues were available in a single-formant idealized vowel and using a clinical Cochlear Corp. ACE filterbank. After making temporal pitch cues available, the JNDs fell to around 4 st. <xref rid="R79" ref-type="bibr">Vandali et al. (2005)</xref> found that a 6-st difference yielded 70% correct discrimination&#8212;thus equivalent to the JND&#8212;for steady state sung vowels. These performances improved when temporal cues were enhanced with various strategies. <xref rid="R37" ref-type="bibr">Green et al. (2004)</xref> measured identification of F0 glides on English diphthongs and found thresholds between 7 and 11 st in CI listeners using a standard continuous interleaved sampling strategy. These authors also developed strategies aiming at enhancing temporal F0 cues and, like Vandali et al., observed significant improvements. However, these improvements only modestly transferred to question/statement discrimination (<xref rid="R38" ref-type="bibr">Green et al. 2005</xref>). <xref rid="R10" ref-type="bibr">Chatterjee and Peng (2008)</xref> also found JNDs in the order of 6.5 to 7.5 st in CI listeners, for question/statement discrimination. More recently, <xref rid="R39" ref-type="bibr">He et al. (2016)</xref> found pitch contour discrimination thresholds to be around 10 to 11 st for the syllable /ma/ and 4 to 8 st for artificial complex tones.</p><p>The consensus view in these studies is that temporal pitch cues play a major role in F0 perception in CI listeners. Figure <xref ref-type="fig" rid="F4">4</xref>A and B show how temporal modulation in a single channel changed when the F0 was increased by 9.19 st. The reference voice in our experiment had an average F0 of 242 Hz, which is rather close to the 300-Hz limit for temporal pitch perception reported in the literature (<xref rid="R70" ref-type="bibr">Shannon 1983</xref>; <xref rid="R9" ref-type="bibr">Carlyon et al. 2002</xref>, <xref rid="R8" ref-type="bibr">2010</xref>; <xref rid="R82" ref-type="bibr">Zeng 2002</xref>). Therefore, in the conditions where the test stimulus had a smaller F0, it could be expected that temporal pitch cues would become more available, yielding smaller JNDs than when the test voice had a larger F0. Indeed, &#8722;9.19 st from 242 Hz means the F0 of the test voice was 142 Hz, while +9.19 st from 242 Hz means it was 411 Hz. However, there was no difference between positive and negative F0 JNDs (the 2 directions only differed by 0.12 st, and in 10 of 11 participants, the intrasubject variability was larger than the individual direction effect). This could mean that temporal pitch cues were still sufficiently salient to perform the task at 411 Hz. Single-channel temporal pitch JNDs (based on stimulation rate or on temporal envelope periodicity) reported in the literature range from 3 to 6 st for base rates close to 242 Hz (<xref rid="R57" ref-type="bibr">McDermott &amp; McKay 1997</xref>; <xref rid="R82" ref-type="bibr">Zeng 2002</xref>; <xref rid="R4" ref-type="bibr">Baumann &amp; Nobbe 2004</xref>; <xref rid="R10" ref-type="bibr">Chatterjee &amp; Peng 2008</xref>; <xref rid="R8" ref-type="bibr">Carlyon et al. 2010</xref>; <xref rid="R29" ref-type="bibr">Gaudrain et al. 2017</xref>).</p><fig id="F4" position="float"><label>Fig. 4.</label><caption><p>Excerpt of channel 10 of the electrodogram of the syllable /ki/, for the original voice (<bold>B</bold>), for the F0 just-noticeable difference (JND; <bold>A</bold>) and for the vocal-tract length (VTL) JND (<bold>C</bold>).</p></caption><graphic xlink:href="aud-39-226-g006"/></fig><p>Complementing temporal pitch cues, it is also arguable that place pitch cues may have contributed to the observed JNDs. <xref rid="R20" ref-type="bibr">Fielden et al. (2015)</xref>, using steady state vowels, found that 6-st differences could be reliably discriminated by CI listeners, but not 3 st ones. These authors suggested that participants used a shift in spectral centroid resulting from F0 differences to do the task. The larger centroid shifts they observed were around 0.4%. Because we used different, non&#8211;steady state syllables, the spectral centroid in our experiment was constantly shifting. Calculating the distribution of centroids across all the available tokens (while keeping F0 and VTL constant), we obtained a distribution whose SD was 22% of the average (<xref rid="R27" ref-type="bibr">Gaudrain 2016</xref>). Spectral centroid is, thus, unlikely to have played a significant role in the F0 JNDs reported here, as has also been previously argued by <xref rid="R36" ref-type="bibr">Green et al. (2002</xref>, <xref rid="R37" ref-type="bibr">2004</xref>) in similar conditions. However, place cues, rather than truly pulling the whole center of gravity of the electric stimulation, may still arise in individual channels. Figure <xref ref-type="fig" rid="F5">5</xref> shows the summary electrodograms, that is, the average power in each channel of the implant, for three different syllables. The top and middle curves in each parts of Figure <xref ref-type="fig" rid="F5">5</xref> differ by 9.19 st. While most of the channels are not strongly affected by the F0 change, a few channels in each panel are affected by the F0 change (e.g., channel 2 and 3 for /ba/, 3 and 4 for /ki/, and 1 to 4 for /po/). While it might be difficult to quantify how much these place cues may have contributed to the observed JNDs, it seems likely that this contribution was not negligible.</p><fig id="F5" position="float"><label>Fig. 5.</label><caption><p>Summary electrodograms for three syllables. The summary electrodogram shows the average power in each channel over the course of the syllable. The middle curve (B) shows the original version of the stimulus, while the top (A) and bottom (C) curves correspond to an F0 shift of 1 just-noticeable difference (JND) and a vocal-tract length (VTL) shift of 1 JND, respectively. The curves corresponding to the original version of the stimulus are repeated, as dashed lines, behind the F0 and VTL shifted version to facilitate comparison. The average power is normalized and, thus, presented without units here.</p></caption><graphic xlink:href="aud-39-226-g007"/></fig><p>Other factors that could have potentially contributed to the difference between NH and CI listeners are task difficulty and age. The difficulty of the 3AFC task depends on the sensory input that the participant receives&#8212;and this is the effect we aim to capture&#8212;but also depends on the proficiency of cognitive mechanisms. While there is no way to absolutely rule out the possibility that individual differences in cognitive function, for example, due to aging, may have contributed to our pattern of results, it can be argued that this is rather unlikely. Indeed, 3AFC tasks similar to the one used in the present study have been used in other studies where performance of the CI group was found to be equivalent or better than that of the NH group (gap detection in words: <xref rid="R84" ref-type="bibr">Gaudrain et al., Reference Note 2</xref>; rate pitch discrimination: <xref rid="R29" ref-type="bibr">Gaudrain et al. 2017</xref>). As for age, the NH listeners were on average 22.7 years younger than the CI listeners. In their vocoder study, <xref rid="R28" ref-type="bibr">Gaudrain and Ba&#351;kent (2015)</xref> assessed whether age could play a role in the F0-VTL JND task by including a number of older participants. While their study population did not allow a systematic study of a potential age effect, they did report that age did not significantly contribute to intersubject variability. On the basis of this report, it, thus, seems unlikely that age would have played a major role in the JND differences observed between the two groups, although a more detailed study would be required to definitely answer this question.</p></sec><sec><title>VTL Perception With Cochlear Implants</title><p>The cues used for VTL perception in NH listeners are most likely spectral in nature, and this is likely true as well in CI listeners. Using vocoders, <xref rid="R28" ref-type="bibr">Gaudrain and Ba&#351;kent (2015)</xref> have shown that while the nature of temporal cues did not affect VTL JNDs, reducing spectral resolution (by reducing the number of channels or by increasing channel overlap) drastically increased VTL JNDs. Examining the output of the implant, it seems likely to also be the case with actual implants. Figure <xref ref-type="fig" rid="F4">4</xref>B and C show the output of one channel of a standard continuous interleaved sampling strategy for a VTL change of 7.19 st. The amplitude modulation pattern of the channel is largely unaffected by the change in VTL. In contrast, the middle and bottom curves of Figure <xref ref-type="fig" rid="F5">5</xref> show average stimulation profiles along the electrode array for the same change in VTL and for three different syllables. Changing VTL seems to result in a shift of the stimulation profile by 2 to 3 channels.</p><p>We could not identify, in the literature, any psychophysical measurement that could be directly compared with VTL JNDs of the present study. The best comparison likely lies in broadband estimates of spectral resolution in implants. The observation that, at threshold, the stimulation profile shifts by 2 to 3 channels implies that only 6 to 8 independent channels are really used by the CI listeners to perform this task. This is in agreement with an equivalent estimate obtained from spectral-ripple discrimination concluding that only 8 independent channels contribute to spectral resolution (<xref rid="R40" ref-type="bibr">Henry &amp; Turner 2003</xref>). With a similar method, <xref rid="R41" ref-type="bibr">Henry et al. (2005)</xref> reported an average spectral-ripple discrimination threshold of 0.62 ripple/octave, which would be equivalent to a JND of 9.67 st. However, spectral-ripple discrimination data obtained by applying rectified sinusoidal ripples to the logarithm of the magnitude (in dB) rather than to the magnitude directly yields smaller equivalent JNDs of 3.4 st on average (<xref rid="R81" ref-type="bibr">Won et al. 2007</xref>).</p><p>Another method that might be worth comparing our results to is formant discrimination. <xref rid="R22" ref-type="bibr">Fitzgerald et al. (2007)</xref> measured F1, F2, and F3 discrimination by altering the frequencies of individual formants while holding the other formants constant. Unfortunately, the data are only reported in millimeters that were specific to individual frequency maps and electrode array. <xref rid="R68" ref-type="bibr">Sagi et al. (2010)</xref> reported that the JNDs found by Fitzgerald et al. were &#8220;about 50 to 100 Hz in the F1 frequency range,&#8221; which corresponds to 3.2 to 5.8 st given the base F1 of 250 Hz used in their experiment. For F2, they report a 10% JND, which corresponds to 1.7 st. Note that <xref rid="R68" ref-type="bibr">Sagi et al. (2010)</xref> also measured formant discrimination, in a more systematic way (varying F1 frequency over a range), but they reported the data in the form of an averaged difference in Hertz (over all reference F1 values), which, thus, cannot be compared with the other data in the literature or with the data presently reported. More recently, <xref rid="R80" ref-type="bibr">Winn et al. (2012)</xref> reported psychometric functions for &#8220;heat&#8221;&#8211;&#8220;hit&#8221; discrimination from which we estimated a JND of 1.24 st for F2 discrimination (at onset), although the cue was mixed with other cues, such as F1/F2 profile frequency and duration.</p><p>These estimates of spectral resolution vary greatly depending on whether they are local (single channel or single formant) or global (spectral ripple). Limited spectral resolution in the implant is often pointed at as the culprit for poor speech understanding, especially in noise (<xref rid="R23" ref-type="bibr">Friesen et al. 2001</xref>; <xref rid="R66" ref-type="bibr">Qin &amp; Oxenham 2003</xref>; <xref rid="R75" ref-type="bibr">Stickney et al. 2004</xref>; <xref rid="R13" ref-type="bibr">Clarke et al. 2016</xref>). As a result, much effort has been spent to try to increase the spectral resolution in the implant (<xref rid="R62" ref-type="bibr">Nogueira et al. 2009</xref>, <xref rid="R63" ref-type="bibr">2016</xref>; <xref rid="R5" ref-type="bibr">Bhattacharya et al. 2011</xref>). To evaluate these new strategies, researchers have used either one of the aforementioned techniques or speech understanding in quiet or in noise. The latter can capture the full benefits of these new strategies only after their chronic use over a few months, because speech understanding requires a rather long adaptation. For this reason, the other measures mentioned above might be more appealing. One advantage of the local, single-formant measures is that they are based on speech stimuli. However, because of their local nature, they incur the risk of being strongly sensitive to small-frequency allocation map variations, which are unlikely to matter for CI users once they have adapted to it. Therefore, these measures might not be reliable predictors of success of a strategy for general speech perception. In contrast, the global spectral-ripple measure is likely less sensitive to local processing modifications but does not focus on spectral contrasts that are specifically relevant for speech.</p><p>The measure investigated here, VTL discrimination thresholds, constitutes a global, objective spectral resolution measure while maintaining relevance to speech perception. It could, thus, prove particularly useful for the evaluation of novel speech-processing strategies and for clinical assessment of CI patients, especially if the relationship between VTL JNDs and speech intelligibility (in quiet or in noise) can be demonstrated.</p></sec><sec><title>Voice Discrimination<sup><xref ref-type="fn" rid="fn01">1</xref></sup> in Implants</title><p>Our present data show a more complex picture than was previously indicated by studies that mostly reduced voice discrimination to voice-pitch ranking. Instead, our results are more in line with the gender categorization data of <xref rid="R26" ref-type="bibr">Fuller et al. (2014)</xref> who showed that while NH listeners use both F0 and VTL to determine the sex of a voice, CI listeners use almost exclusively F0 and rely very little on VTL.</p><p>To understand the importance of the relative roles of F0 and VTL, it is worth noting that the typical F0 difference between male and female speakers is about 1 octave&#8212;or 12 st&#8212;while the average F0 JND in CI users is just above 9 st, that is, 4.7 times larger than for the NH listeners. In other words, although pitch perception is greatly reduced in CI listeners, it seems to remain sufficient for sex categorization purposes. Unfortunately, the same cannot be said of VTL: while the VTL JNDs are also just 4.2 times larger in CI than in NH listeners, the typical VTL difference between male and female speakers is much smaller. Estimates in the literature vary from 13% (<xref rid="R21" ref-type="bibr">Fitch &amp; Giedd 1999</xref>) or 15% (<xref rid="R19" ref-type="bibr">Fant 1970</xref>) to about 18% (<xref rid="R78" ref-type="bibr">Turner et al. 2009</xref>, based on the data of <xref rid="R64" ref-type="bibr">Peterson &amp; Barney 1952</xref>). To produce a clear distinction between their female and male voices, <xref rid="R26" ref-type="bibr">Fuller et al. (2014)</xref> used 23%. Expressed in semitones, these male&#8211;female VTL differences represent, respectively, the following: 2.0, 2.4, 2.9, and 3.6 st. In other words, even when using this last, more conservative estimate, the present JND data indicate that CI listeners do not perceive the VTL difference between male and female speakers.</p><p><xref rid="R26" ref-type="bibr">Fuller et al. (2014)</xref> discussed the possible origins for the abnormal gender categorization pattern they observed in CI listeners: (1) either VTL cues are not transmitted through the implant and the electrode&#8211;neuron interface or (2) the VTL cues are transmitted, but in a distorted form that makes it impossible to correctly interpret them as speaker size information and use for categorizing speaker&#8217;s gender. The conclusions from this study suggest that the first hypothesis is more likely than the second and that the deficit originates at the sensory level. Because a 3AFC task was used here, to do the task, the participants were free to use any type of cue available in the signal, which requires no or minimal interpretation of the VTL-related cues into speaker size information. The large VTL JNDs observed, therefore, suggest that the VTL cues are too degraded in electric hearing to be picked up even at a rather primitive level of the auditory system.</p><p>This study measured voice cue discrimination from the recording of a single speaker. Yet, the large JNDs observed in this experiment will likely have consequences beyond gender categorization, extending to speaker identification in general. A number of studies have shown that CI listeners had greater difficulty discriminating actual speakers than NH listeners (<xref rid="R14" ref-type="bibr">Cleary &amp; Pisoni 2002</xref>; <xref rid="R61" ref-type="bibr">M&#252;hler et al. 2009</xref>). <xref rid="R85" ref-type="bibr">Gaudrain et al. (Reference Note 3)</xref> investigated that F0 and VTL difference would lead participants to judging they were hearing two different speakers. They found that speakers were judged as different when their voice differed by 3.8 st in F0 or 2.2 st in VTL or more. Because the JNDs observed in the present study for CI listeners are larger than these values, one may predict that speaker discrimination based on F0 and VTL differences would be altered in CI users. Indeed, <xref rid="R15" ref-type="bibr">Cleary et al. (2005)</xref> reported that pediatric CI users needed larger combined F0 and VTL differences than their NH peers to discriminate voices. Unfortunately, the two cues were manipulated together so their individual contribution cannot be separated in these data.</p><p>Intuitively, it is assumed that good voice discrimination should entail large voice difference benefit in speech-on-speech perception. A few studies have shown that the sensitivity to F0 differences does correlate with the F0-difference advantage in a concurrent speech-listening situation (<xref rid="R77" ref-type="bibr">Summers &amp; Leek 1998</xref>; <xref rid="R54" ref-type="bibr">Mackersie et al. 2001</xref>; <xref rid="R52" ref-type="bibr">Mackersie 2003</xref>; <xref rid="R31" ref-type="bibr">Gaudrain et al. 2012</xref>), and systematic voice differences have been shown to improve perception of concurrent speech (<xref rid="R7" ref-type="bibr">Brungart 2001</xref>; <xref rid="R16" ref-type="bibr">Darwin et al. 2003</xref>; <xref rid="R53" ref-type="bibr">Mackersie et al. 2011</xref>; <xref rid="R3" ref-type="bibr">Ba&#351;kent &amp; Gaudrain 2016</xref>). A deficit in voice discrimination is, thus, likely to hinder concurrent speech perception. Again, this was shown for F0 or actual voice differences (<xref rid="R75" ref-type="bibr">Stickney et al. 2004</xref>, <xref rid="R76" ref-type="bibr">2007</xref>; <xref rid="R67" ref-type="bibr">Qin &amp; Oxenham, 2005</xref>; <xref rid="R30" ref-type="bibr">Gaudrain et al. 2008</xref>; <xref rid="R51" ref-type="bibr">Luo et al. 2009</xref>), but such consequences for a deficit in VTL sensitivity has not been documented yet.</p></sec><sec><title>Relation to Acoustic Simulations of Implants</title><p><xref rid="R28" ref-type="bibr">Gaudrain and Ba&#351;kent (2015)</xref> measured F0 and VTL JNDs using a collection of vocoders to systematically investigate which parameters had an effect on VTL and F0 JNDs. The main conclusion that they drew was that VTL JNDs are more affected by a loss of spectral resolution than F0 JNDs (their experiment 1). They, thus, predicted that VTL JNDs would be more likely than F0 JNDs to be larger than the typical male&#8211;female difference along these respective dimensions. In actual CI users, when we compared JNDs directly to each other, we found that F0 and VTL JNDs are equally degraded in CI listening, relatively to NH, that is, contrary to the prediction, VTL JNDs are not more degraded than F0 JNDs. However, the second part of the data analysis verified the prediction that VTL JNDs would be more likely than F0 JNDs to be larger than the typical male&#8211;female difference.</p><p>As mentioned in Introduction, the vocoder simulations of CIs had produced differing patterns in utilization of F0 and VTL cues for gender categorization than the patterns of actual CI users in the study by <xref rid="R26" ref-type="bibr">Fuller et al. (2014)</xref>. The results from actual CI users of the present study, in contrast, partially overlapped with the results from simulations used for F0 and VTL JNDs in the study by <xref rid="R28" ref-type="bibr">Gaudrain and Ba&#351;kent (2015)</xref>. This difference perhaps is closely related to the different perceptual processes related to these tasks. For JNDs, especially with the design of the odd-one-out experiment, detecting any voice difference is sufficient. For gender categorization, this is not sufficient&#8212;the listener has to make a correct interpretation of this difference, such as extracting the height of the speaker, as well. Hence, it is possible that JNDs are more governed by the degradations in the sensory processes, such as degraded temporal&#8211;spectral cues, factors that are more easily captured in vocoder simulations, while gender categorization is governed by both these degradations and the higher-level cognitive mechanisms needed for further interpretation, which are not captured by these simulations.</p><p>One advantage of this overlap between actual and simulated CI data, however, is that this situation offers a good opportunity for developing a robust simulation tool that can be used for further research. Here, for this purpose, we examine which of the vocoders used by <xref rid="R28" ref-type="bibr">Gaudrain and Ba&#351;kent (2015)</xref> best match the present CI data. Among the tested vocoders, the 4-band sine wave or noise vocoders (with filter slopes of 48 to 72 dB/octave) they used in experiments 1 and 3 seem to provide the closest match to the actual CI VTL JND data, both in terms of average values and across-subject variability. The closest match for the F0 JNDs was the 6-band noise vocoder used in experiment 2. The 4-band sine wave vocoder they used in experiment 1 yielded F0 JNDs that are markedly better than the average F0 JND obtained by the present CI participants. Unfortunately, <xref rid="R28" ref-type="bibr">Gaudrain and Ba&#351;kent (2015)</xref> did not measure F0 JNDs with their 4-band noise vocoder. However, it is likely that performance would have been slightly better with that vocoder than with the 6-band noise vocoder because wider noise bands allow for deeper amplitude modulation, which means better coding of the F0.</p><p>It, thus, seems that noise vocoders with 4 to 6 bands should be able to capture the gist of the behavior of actual CI listeners in the VTL and F0 JND tasks. These numbers of bands in the vocoder are similar to those reported by <xref rid="R75" ref-type="bibr">Stickney et al. (2004)</xref> as yielding speech comprehension performance in NH listeners that is comparable to that of actual CI listeners.</p></sec></sec><sec sec-type="conclusions"><title>CONCLUSIONS</title><p>The perception of F0 and VTL, together or separately, is not only related to gender perception (<xref rid="R72" ref-type="bibr">Smith &amp; Patterson 2005</xref>; <xref rid="R42" ref-type="bibr">Hillenbrand &amp; Clark 2009</xref>; <xref rid="R59" ref-type="bibr">Meister et al. 2009</xref>; <xref rid="R26" ref-type="bibr">Fuller et al. 2014</xref>; <xref rid="R71" ref-type="bibr">Skuk &amp; Schweinberger 2014</xref>) but also related to speaker size perception (<xref rid="R44" ref-type="bibr">Ives et al. 2005</xref>; <xref rid="R73" ref-type="bibr">Smith et al. 2005</xref>), speaker identification (<xref rid="R85" ref-type="bibr">Gaudrain et al, Reference Note 3</xref>), pragmatic prosody (<xref rid="R10" ref-type="bibr">Chatterjee &amp; Peng 2008</xref>; <xref rid="R59" ref-type="bibr">Meister et al. 2009</xref>), and emotion perception (<xref rid="R12" ref-type="bibr">Chuenwattanapranithi et al. 2008</xref>; <xref rid="R69" ref-type="bibr">Sauter et al. 2010</xref>). In addition to providing all of these extra information supplementing speech communication, the vocal cues also play an important role for understanding speech in noise&#8212;F0 and VTL differences between competing voices represent crucial cues that NH listeners use to segregate one voice from the other, which seem to significantly improve speech-on-speech perception (<xref rid="R16" ref-type="bibr">Darwin et al. 2003</xref>; <xref rid="R53" ref-type="bibr">Mackersie et al. 2011</xref>; <xref rid="R3" ref-type="bibr">Ba&#351;kent &amp; Gaudrain 2016</xref>).</p><p>Cochlear implant users have been shown to have specific difficulties with perceiving speaker gender (<xref rid="R56" ref-type="bibr">Massida et al. 2013</xref>; <xref rid="R26" ref-type="bibr">Fuller et al. 2014</xref>) and vocal emotions (<xref rid="R50" ref-type="bibr">Luo et al. 2007</xref>; <xref rid="R11" ref-type="bibr">Chatterjee et al. 2015</xref>; <xref rid="R34" ref-type="bibr">Gilbers et al. 2015</xref>) and with taking advantage of voice differences for competing speech perception (<xref rid="R75" ref-type="bibr">Stickney et al. 2004</xref>, <xref rid="R76" ref-type="bibr">2007</xref>; <xref rid="R65" ref-type="bibr">Pyschny et al. 2011</xref>). The F0 and VTL JNDs we are reporting provide a common explanation to all these deficits: the principal voice cues, although represented through the implant, are not available with sufficient precision to make them useful for many real-life situations.</p><p>This is particularly true for VTL whose variations, both within and across speakers, are relatively small compared with those of F0. Yet, within this small range of variation, VTL produces perceptual effects of the same magnitude as F0 does over a much larger range. For instance, in <xref rid="R3" ref-type="bibr">Ba&#351;kent and Gaudrain (2016)</xref>, a VTL difference of 1 st produced the same speech-on-speech intelligibility increase as an F0 difference of 2.3 st. In <xref rid="R26" ref-type="bibr">Fuller et al. (2014)</xref>, a VTL difference of 1 st affected perceived voice sex as much as an F0 difference of 4.8 st. Therefore, while CI listeners present abnormally enlarged JNDs both for VTL and F0, in some situations, the loss in VTL sensitivity could have more dramatic consequences on voice and speech perception than the reduction in F0 sensitivity does.</p><p>Finally, because VTL perception relies primarily on spectral resolution (<xref rid="R28" ref-type="bibr">Gaudrain &amp; Ba&#351;kent 2015</xref>), VTL JNDs constitute a measure of spectral resolution, which could be compared with spectral-ripple detection, but more directly related to speech perception. This measure could, thus, be used in clinical setting for fitting purposes or in developing of the new strategies.</p></sec><sec><title>ACKNOWLEDGMENTS</title><p>We thank Julia Verbist and Floor Burgerhof for their assistance with data collection; Ria Woldhuis and Nadine Tuinman for general support; and Gerda Boven, Bert Maat, and Rolien Free from the Cochleair Implantatieteam Noord Nederland of the University Medical Center Groningen for their insight and support in recruiting and testing cochlear implant participants. We also thank Robert P. Morse and Waldo Nogueira for providing the Matlab code used to produce the electrodograms.</p></sec></body><back><fn-group><fn fn-type="other" id="fn01"><label>1</label><p>By &#8220;voice discrimination,&#8221; we mean the discrimination of voices emanating from different speakers, real, or simulated through vocal cue manipulation. We do not refer to the ability to distinguish whether a sound is a human voice or not, as was done by <xref rid="R55" ref-type="bibr">Massida et al. (2011)</xref>.</p></fn></fn-group><fn-group><fn fn-type="supported-by"><p>This study was supported by a Rosalind Franklin Fellowship from the University Medical Center Groningen, University of Groningen, and the VIDI grant number 016.096.397 from the Netherlands Organization for Scientific Research and the Netherlands Organization for Health Research and Development (ZonMw).</p></fn><fn fn-type="other"><p>This work was conducted in the framework of the LabEx CeLyA (&#8220;Centre Lyonnais d&#8217;Acoustique&#8221;, ANR-10-LABX-0060/ANR-11-IDEX-0007) operated by the French National Research Agency and is also part of the research program of the Otorhinolaryngoly Department of the University Medical Center Groningen: Healthy Aging and Communication.</p></fn><fn fn-type="COI-statement"><p>The authors have no conflicts of interest to disclose.</p></fn></fn-group><ref-list><title>REFERENCES</title><ref id="R1"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Abercrombie</surname><given-names>D</given-names></name></person-group><source>Elements of General Phonetics</source>. <year>1982</year>). <publisher-loc>Edinburgh</publisher-loc>: <publisher-name>University Press</publisher-name>.</mixed-citation></ref><ref id="R2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ba&#351;kent</surname><given-names>D.</given-names></name><name><surname>Gaudrain</surname><given-names>E</given-names></name></person-group><article-title>Musician advantage for speech-on-speech perception.</article-title>
<source>J Acoust Soc Am</source>, <year>2016</year>). <volume>139</volume>, <fpage>EL51</fpage><lpage>EL56</lpage>.<pub-id pub-id-type="pmid">27036287</pub-id></mixed-citation></ref><ref id="R3"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ba&#351;kent</surname><given-names>D.</given-names></name><name><surname>Gaudrain</surname><given-names>E.</given-names></name><name><surname>Tamati</surname><given-names>T. N.</given-names></name><name><surname>Wagner</surname><given-names>A</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Cacace</surname><given-names>A. T.</given-names></name><name><surname>de Kleine</surname><given-names>E.</given-names></name><name><surname>Holt</surname><given-names>A. G.</given-names></name><name><surname>van Dijk</surname><given-names>P</given-names></name></person-group><article-title>Perception and psychoacoustics of speech in cochlear implant users.</article-title>
<source>In Scientific Foundations of Audiology: Perspectives From Physics, Biology, Modeling, and Medicine</source> (pp. <year>2016</year>). <publisher-loc>San Diego, CA</publisher-loc>: <publisher-name>Plural Publishing, Inc</publisher-name><fpage>285</fpage><lpage>319</lpage>). </mixed-citation></ref><ref id="R4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baumann</surname><given-names>U.</given-names></name><name><surname>Nobbe</surname><given-names>A</given-names></name></person-group><article-title>Pulse rate discrimination with deeply inserted electrode arrays.</article-title>
<source>Hear Res</source>, <year>2004</year>). <volume>196</volume>, <fpage>49</fpage><lpage>57</lpage>.<pub-id pub-id-type="pmid">15464301</pub-id></mixed-citation></ref><ref id="R5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhattacharya</surname><given-names>A.</given-names></name><name><surname>Vandali</surname><given-names>A.</given-names></name><name><surname>Zeng</surname><given-names>F. G</given-names></name></person-group><article-title>Combined spectral and temporal enhancement to improve cochlear-implant speech perception.</article-title>
<source>J Acoust Soc Am</source>, <year>2011</year>). <volume>130</volume>, <fpage>2951</fpage><lpage>2960</lpage>.<pub-id pub-id-type="pmid">22087923</pub-id></mixed-citation></ref><ref id="R6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bosman</surname><given-names>A. J.</given-names></name><name><surname>Smoorenburg</surname><given-names>G. F</given-names></name></person-group><article-title>Intelligibility of Dutch CVC syllables and sentences for listeners with normal hearing and with three types of hearing impairment.</article-title>
<source>Audiology</source>, <year>1995</year>). <volume>34</volume>, <fpage>260</fpage><lpage>284</lpage>.<pub-id pub-id-type="pmid">8837785</pub-id></mixed-citation></ref><ref id="R7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brungart</surname><given-names>D. S</given-names></name></person-group><article-title>Informational and energetic masking effects in the perception of two simultaneous talkers.</article-title>
<source>J Acoust Soc Am</source>, <year>2001</year>). <volume>109</volume>, <fpage>1101</fpage><lpage>1109</lpage>.<pub-id pub-id-type="pmid">11303924</pub-id></mixed-citation></ref><ref id="R8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlyon</surname><given-names>R. P.</given-names></name><name><surname>Deeks</surname><given-names>J. M.</given-names></name><name><surname>McKay</surname><given-names>C. M</given-names></name></person-group><article-title>The upper limit of temporal pitch for cochlear-implant listeners: Stimulus duration, conditioner pulses, and the number of electrodes stimulated.</article-title>
<source>J Acoust Soc Am</source>, <year>2010</year>). <volume>127</volume>(<issue>3</issue>), <fpage>1469</fpage><lpage>1478</lpage>.<pub-id pub-id-type="pmid">20329847</pub-id></mixed-citation></ref><ref id="R9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlyon</surname><given-names>R. P.</given-names></name><name><surname>van Wieringen</surname><given-names>A.</given-names></name><name><surname>Long</surname><given-names>C. J.</given-names></name><etal/></person-group><article-title>Temporal pitch mechanisms in acoustic and electric hearing.</article-title>
<source>J Acoust Soc Am</source>, <year>2002</year>). <volume>112</volume>, <fpage>621</fpage><lpage>633</lpage>.<pub-id pub-id-type="pmid">12186042</pub-id></mixed-citation></ref><ref id="R10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chatterjee</surname><given-names>M.</given-names></name><name><surname>Peng</surname><given-names>S. C</given-names></name></person-group><article-title>Processing F0 with cochlear implants: Modulation frequency discrimination and speech intonation recognition.</article-title>
<source>Hear Res</source>, <year>2008</year>). <volume>235</volume>, <fpage>143</fpage><lpage>156</lpage>.<pub-id pub-id-type="pmid">18093766</pub-id></mixed-citation></ref><ref id="R11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chatterjee</surname><given-names>M.</given-names></name><name><surname>Zion</surname><given-names>D. J.</given-names></name><name><surname>Deroche</surname><given-names>M. L.</given-names></name><etal/></person-group><article-title>Voice emotion recognition by cochlear-implanted children and their normally-hearing peers.</article-title>
<source>Hear Res</source>, <year>2015</year>). <volume>322</volume>, <fpage>151</fpage><lpage>162</lpage>.<pub-id pub-id-type="pmid">25448167</pub-id></mixed-citation></ref><ref id="R12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chuenwattanapranithi</surname><given-names>S.</given-names></name><name><surname>Xu</surname><given-names>Y.</given-names></name><name><surname>Thipakorn</surname><given-names>B.</given-names></name><etal/></person-group><article-title>Encoding emotions in speech with the size code. A perceptual investigation.</article-title>
<source>Phonetica</source>, <year>2008</year>). <volume>65</volume>, <fpage>210</fpage><lpage>230</lpage>.<pub-id pub-id-type="pmid">19221452</pub-id></mixed-citation></ref><ref id="R13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>J.</given-names></name><name><surname>Ba&#351;kent</surname><given-names>D.</given-names></name><name><surname>Gaudrain</surname><given-names>E</given-names></name></person-group><article-title>Pitch and spectral resolution: A systematic comparison of bottom-up cues for top-down repair of degraded speech.</article-title>
<source>J Acoust Soc Am</source>, <year>2016</year>). <volume>139</volume>, <fpage>395</fpage><lpage>405</lpage>.<pub-id pub-id-type="pmid">26827034</pub-id></mixed-citation></ref><ref id="R14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cleary</surname><given-names>M.</given-names></name><name><surname>Pisoni</surname><given-names>D. B</given-names></name></person-group><article-title>Talker discrimination by prelingually deaf children with cochlear implants: Preliminary results.</article-title>
<source>Ann Otol Rhinol Laryngol Suppl</source>, <year>2002</year>). <volume>189</volume>, <fpage>113</fpage><lpage>118</lpage>.<pub-id pub-id-type="pmid">12018337</pub-id></mixed-citation></ref><ref id="R15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cleary</surname><given-names>M.</given-names></name><name><surname>Pisoni</surname><given-names>D. B.</given-names></name><name><surname>Kirk</surname><given-names>K. I</given-names></name></person-group><article-title>Influence of voice similarity on talker discrimination in children with normal hearing and children with cochlear implants.</article-title>
<source>J Speech Lang Hear Res</source>, <year>2005</year>). <volume>48</volume>, <fpage>204</fpage><lpage>223</lpage>.<pub-id pub-id-type="pmid">15938065</pub-id></mixed-citation></ref><ref id="R16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Darwin</surname><given-names>C. J.</given-names></name><name><surname>Brungart</surname><given-names>D. S.</given-names></name><name><surname>Simpson</surname><given-names>B. D</given-names></name></person-group><article-title>Effects of fundamental frequency and vocal-tract length changes on attention to one of two simultaneous talkers.</article-title>
<source>J Acoust Soc Am</source>, <year>2003</year>). <volume>114</volume>, <fpage>2913</fpage><lpage>2922</lpage>.<pub-id pub-id-type="pmid">14650025</pub-id></mixed-citation></ref><ref id="R17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deroche</surname><given-names>M. L.</given-names></name><name><surname>Lu</surname><given-names>H. P.</given-names></name><name><surname>Limb</surname><given-names>C. J.</given-names></name><etal/></person-group><article-title>Deficits in the pitch sensitivity of cochlear-implanted children speaking English or Mandarin.</article-title>
<source>Front Neurosci</source>, <year>2014</year>). <volume>8</volume>, <fpage>282</fpage>.<pub-id pub-id-type="pmid">25249932</pub-id></mixed-citation></ref><ref id="R18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deroche</surname><given-names>M. L.</given-names></name><name><surname>Culling</surname><given-names>J. F.</given-names></name><name><surname>Lavandier</surname><given-names>M.</given-names></name><etal/></person-group><article-title>Reverberation limits the release from informational masking obtained in the harmonic and binaural domains.</article-title>
<source>Atten Percept Psychophys</source>, <year>2017</year>). <volume>79</volume>, <fpage>363</fpage><lpage>379</lpage>.<pub-id pub-id-type="pmid">27645216</pub-id></mixed-citation></ref><ref id="R19"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fant</surname><given-names>G. C. M</given-names></name></person-group><source>Acoustic Theory of Speech Production</source>. <year>1970</year>). <publisher-loc>The Hague</publisher-loc>: <publisher-name>Mouton</publisher-name>.</mixed-citation></ref><ref id="R20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fielden</surname><given-names>C. A.</given-names></name><name><surname>Kluk</surname><given-names>K.</given-names></name><name><surname>Boyle</surname><given-names>P. J.</given-names></name><etal/></person-group><article-title>The perception of complex pitch in cochlear implants: A comparison of monopolar and tripolar stimulation.</article-title>
<source>J Acoust Soc Am</source>, <year>2015</year>). <volume>138</volume>, <fpage>2524</fpage><lpage>2536</lpage>.<pub-id pub-id-type="pmid">26520335</pub-id></mixed-citation></ref><ref id="R21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitch</surname><given-names>W. T.</given-names></name><name><surname>Giedd</surname><given-names>J</given-names></name></person-group><article-title>Morphology and development of the human vocal tract: A study using magnetic resonance imaging.</article-title>
<source>J Acoust Soc Am</source>, <year>1999</year>). <volume>106</volume>(<issue>3 Pt 1</issue>), <fpage>1511</fpage><lpage>1522</lpage>.<pub-id pub-id-type="pmid">10489707</pub-id></mixed-citation></ref><ref id="R22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitzgerald</surname><given-names>M. B.</given-names></name><name><surname>Shapiro</surname><given-names>W. H.</given-names></name><name><surname>McDonald</surname><given-names>P. D.</given-names></name><etal/></person-group><article-title>The effect of perimodiolar placement on speech perception and frequency discrimination by cochlear implant users.</article-title>
<source>Acta Otolaryngol</source>, <year>2007</year>). <volume>127</volume>, <fpage>378</fpage><lpage>383</lpage>.<pub-id pub-id-type="pmid">17453457</pub-id></mixed-citation></ref><ref id="R23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friesen</surname><given-names>L. M.</given-names></name><name><surname>Shannon</surname><given-names>R. V.</given-names></name><name><surname>Baskent</surname><given-names>D.</given-names></name><etal/></person-group><article-title>Speech recognition in noise as a function of the number of spectral channels: Comparison of acoustic hearing and cochlear implants.</article-title>
<source>J Acoust Soc Am</source>, <year>2001</year>). <volume>110</volume>, <fpage>1150</fpage><lpage>1163</lpage>.<pub-id pub-id-type="pmid">11519582</pub-id></mixed-citation></ref><ref id="R24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>Q. J.</given-names></name><name><surname>Chinchilla</surname><given-names>S.</given-names></name><name><surname>Galvin</surname><given-names>J. J</given-names></name></person-group><article-title>The role of spectral and temporal cues in voice gender discrimination by normal-hearing listeners and cochlear implant users.</article-title>
<source>J Assoc Res Otolaryngol</source>, <year>2004</year>). <volume>5</volume>, <fpage>253</fpage><lpage>260</lpage>.<pub-id pub-id-type="pmid">15492884</pub-id></mixed-citation></ref><ref id="R25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>Q. J.</given-names></name><name><surname>Chinchilla</surname><given-names>S.</given-names></name><name><surname>Nogaki</surname><given-names>G.</given-names></name><etal/></person-group><article-title>Voice gender identification by cochlear implant users: The role of spectral and temporal resolution.</article-title>
<source>J Acoust Soc Am</source>, <year>2005</year>). <volume>118</volume>(<issue>3 Pt 1</issue>), <fpage>1711</fpage><lpage>1718</lpage>.<pub-id pub-id-type="pmid">16240829</pub-id></mixed-citation></ref><ref id="R26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuller</surname><given-names>C. D.</given-names></name><name><surname>Gaudrain</surname><given-names>E.</given-names></name><name><surname>Clarke</surname><given-names>J. N.</given-names></name><etal/></person-group><article-title>Gender categorization is abnormal in cochlear implant users.</article-title>
<source>J Assoc Res Otolaryngol</source>, <year>2014</year>). <volume>15</volume>, <fpage>1037</fpage><lpage>1048</lpage>.<pub-id pub-id-type="pmid">25172111</pub-id></mixed-citation></ref><ref id="R27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaudrain</surname><given-names>E</given-names></name></person-group><article-title>Can spectral centroid explain voice pitch and vocal-tract length perception in normal-hearing and cochlear implant listeners?</article-title>
<source>J Acoust Soc Am</source>, <year>2016</year>). <volume>140</volume>, <fpage>3439</fpage><lpage>3439</lpage>. </mixed-citation></ref><ref id="R28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaudrain</surname><given-names>E.</given-names></name><name><surname>Ba&#351;kent</surname><given-names>D</given-names></name></person-group><article-title>Factors limiting vocal-tract length discrimination in cochlear implant simulations.</article-title>
<source>J Acoust Soc Am</source>, <year>2015</year>). <volume>137</volume>, <fpage>1298</fpage><lpage>1308</lpage>.<pub-id pub-id-type="pmid">25786943</pub-id></mixed-citation></ref><ref id="R29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaudrain</surname><given-names>E.</given-names></name><name><surname>Deeks</surname><given-names>J. M.</given-names></name><name><surname>Carlyon</surname><given-names>R. P</given-names></name></person-group><article-title>Temporal regularity detection and rate discrimination in cochlear-implant listeners.</article-title>
<source>J Assoc Res Otolaryngol</source>, <year>2017</year>). <volume>18</volume>, <fpage>387</fpage><lpage>397</lpage>.<pub-id pub-id-type="pmid">27687041</pub-id></mixed-citation></ref><ref id="R30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaudrain</surname><given-names>E.</given-names></name><name><surname>Grimault</surname><given-names>N.</given-names></name><name><surname>Healy</surname><given-names>E. W.</given-names></name><etal/></person-group><article-title>Streaming of vowel sequences based on fundamental frequency in a cochlear-implant simulation.</article-title>
<source>J Acoust Soc Am</source>, <year>2008</year>). <volume>124</volume>, <fpage>3076</fpage><lpage>3087</lpage>.<pub-id pub-id-type="pmid">19045793</pub-id></mixed-citation></ref><ref id="R31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaudrain</surname><given-names>E.</given-names></name><name><surname>Grimault</surname><given-names>N.</given-names></name><name><surname>Healy</surname><given-names>E. W.</given-names></name><name><surname>B&#233;ra</surname><given-names>J.-C</given-names></name></person-group><article-title>The relationship between concurrent speech segregation, pitch-based streaming of vowel sequences, and frequency selectivity.</article-title>
<source>Acta Acust United Acust</source>, (<year>2012</year>). <volume>98</volume>, <fpage>317</fpage><lpage>327</lpage>.</mixed-citation></ref><ref id="R32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geurts</surname><given-names>L.</given-names></name><name><surname>Wouters</surname><given-names>J</given-names></name></person-group><article-title>Coding of the fundamental frequency in continuous interleaved sampling processors for cochlear implants.</article-title>
<source>J Acoust Soc Am</source>, <year>2001</year>). <volume>109</volume>, <fpage>713</fpage><lpage>726</lpage>.<pub-id pub-id-type="pmid">11248975</pub-id></mixed-citation></ref><ref id="R33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gfeller</surname><given-names>K.</given-names></name><name><surname>Turner</surname><given-names>C.</given-names></name><name><surname>Mehr</surname><given-names>M.</given-names></name><etal/></person-group><article-title>Recognition of familiar melodies by adult cochlear implant recipients and normal-hearing adults.</article-title>
<source>Cochlear Implants Int</source>, <year>2002</year>). <volume>3</volume>, <fpage>29</fpage><lpage>53</lpage>.<pub-id pub-id-type="pmid">18792110</pub-id></mixed-citation></ref><ref id="R34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbers</surname><given-names>S.</given-names></name><name><surname>Fuller</surname><given-names>C.</given-names></name><name><surname>Gilbers</surname><given-names>D.</given-names></name><etal/></person-group><article-title>Normal-hearing listeners&#8217; and cochlear implant users&#8217; perception of pitch cues in emotional speech.</article-title>
<source>i-Perception</source>, <year>2015</year>). <volume>6</volume>, <fpage>0301006615599139</fpage>.<pub-id pub-id-type="pmid">27648210</pub-id></mixed-citation></ref><ref id="R35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasberg</surname><given-names>B. R.</given-names></name><name><surname>Moore</surname><given-names>B. C. J</given-names></name></person-group><article-title>A model of loudness applicable to time-varying sounds.</article-title>
<source>J Audio Eng Soc</source>, <year>2002</year>). <volume>50</volume>(<issue>5</issue>), <fpage>331</fpage><lpage>342</lpage>.</mixed-citation></ref><ref id="R36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname><given-names>T.</given-names></name><name><surname>Faulkner</surname><given-names>A.</given-names></name><name><surname>Rosen</surname><given-names>S</given-names></name></person-group><article-title>Spectral and temporal cues to pitch in noise-excited vocoder simulations of continuous-interleaved-sampling cochlear implants.</article-title>
<source>J Acoust Soc Am</source>, <year>2002</year>). <volume>112</volume>(<issue>5 Pt 1</issue>), <fpage>2155</fpage><lpage>2164</lpage>.<pub-id pub-id-type="pmid">12430827</pub-id></mixed-citation></ref><ref id="R37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname><given-names>T.</given-names></name><name><surname>Faulkner</surname><given-names>A.</given-names></name><name><surname>Rosen</surname><given-names>S</given-names></name></person-group><article-title>Enhancing temporal cues to voice pitch in continuous interleaved sampling cochlear implants.</article-title>
<source>J Acoust Soc Am</source>, <year>2004</year>). <volume>116</volume>(<issue>4 Pt 1</issue>), <fpage>2298</fpage><lpage>2310</lpage>.<pub-id pub-id-type="pmid">15532661</pub-id></mixed-citation></ref><ref id="R38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname><given-names>T.</given-names></name><name><surname>Faulkner</surname><given-names>A.</given-names></name><name><surname>Rosen</surname><given-names>S.</given-names></name><etal/></person-group><article-title>Enhancement of temporal periodicity cues in cochlear implants: Effects on prosodic perception and vowel identification.</article-title>
<source>J Acoust Soc Am</source>, <year>2005</year>). <volume>118</volume>, <fpage>375</fpage><lpage>385</lpage>.<pub-id pub-id-type="pmid">16119358</pub-id></mixed-citation></ref><ref id="R39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>A.</given-names></name><name><surname>Deroche</surname><given-names>M. L.</given-names></name><name><surname>Doong</surname><given-names>J.</given-names></name><etal/></person-group><article-title>Mandarin tone identification in cochlear implant users using exaggerated pitch contours.</article-title>
<source>Otol Neurotol</source>, <year>2016</year>). <volume>37</volume>, <fpage>324</fpage><lpage>331</lpage>.<pub-id pub-id-type="pmid">26890043</pub-id></mixed-citation></ref><ref id="R40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henry</surname><given-names>B. A.</given-names></name><name><surname>Turner</surname><given-names>C. W</given-names></name></person-group><article-title>The resolution of complex spectral patterns by cochlear implant and normal-hearing listeners.</article-title>
<source>J Acoust Soc Am</source>, <year>2003</year>). <volume>113</volume>, <fpage>2861</fpage><lpage>2873</lpage>.<pub-id pub-id-type="pmid">12765402</pub-id></mixed-citation></ref><ref id="R41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henry</surname><given-names>B. A.</given-names></name><name><surname>Turner</surname><given-names>C. W.</given-names></name><name><surname>Behrens</surname><given-names>A</given-names></name></person-group><article-title>Spectral peak resolution and speech recognition in quiet: Normal hearing, hearing impaired, and cochlear implant listeners.</article-title>
<source>J Acoust Soc Am</source>, <year>2005</year>). <volume>118</volume>, <fpage>1111</fpage><lpage>1121</lpage>.<pub-id pub-id-type="pmid">16158665</pub-id></mixed-citation></ref><ref id="R42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hillenbrand</surname><given-names>J. M.</given-names></name><name><surname>Clark</surname><given-names>M. J</given-names></name></person-group><article-title>The role of f0 and formant frequencies in distinguishing the voices of men and women.</article-title>
<source>Atten Percept Psychophys</source>, <year>2009</year>). <volume>71</volume>, <fpage>1150</fpage><lpage>1166</lpage>.<pub-id pub-id-type="pmid">19525544</pub-id></mixed-citation></ref><ref id="R43"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>R. S.</given-names></name><name><surname>Turner</surname><given-names>C. W</given-names></name></person-group><article-title>Sequential stream segregation using temporal periodicity cues in cochlear implant recipients.</article-title>
<source>J Acoust Soc Am</source>, <year>2009</year>). <volume>126</volume>, <fpage>291</fpage><lpage>299</lpage>.<pub-id pub-id-type="pmid">19603885</pub-id></mixed-citation></ref><ref id="R44"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ives</surname><given-names>D. T.</given-names></name><name><surname>Smith</surname><given-names>D. R.</given-names></name><name><surname>Patterson</surname><given-names>R. D</given-names></name></person-group><article-title>Discrimination of speaker size from syllable phrases.</article-title>
<source>J Acoust Soc Am</source>, <year>2005</year>). <volume>118</volume>, <fpage>3816</fpage><lpage>3822</lpage>.<pub-id pub-id-type="pmid">16419826</pub-id></mixed-citation></ref><ref id="R45"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kawahara</surname><given-names>H.</given-names></name><name><surname>Irino</surname><given-names>T</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Divenyi</surname><given-names>P. L</given-names></name></person-group><article-title>Underlying principles of a high-quality speech manipulation system STRAIGHT and its application to speech segregation.</article-title>
<source>In Speech Separation by Humans and Machines</source> (pp. <year>2004</year>). <publisher-loc>Massachusetts</publisher-loc>: <publisher-name>Kluwer Academic</publisher-name><fpage>167</fpage><lpage>180</lpage>). </mixed-citation></ref><ref id="R46"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kovaci&#263;</surname><given-names>D.</given-names></name><name><surname>Balaban</surname><given-names>E</given-names></name></person-group><article-title>Voice gender perception by cochlear implantees.</article-title>
<source>J Acoust Soc Am</source>, <year>2009</year>). <volume>126</volume>, <fpage>762</fpage><lpage>775</lpage>.<pub-id pub-id-type="pmid">19640042</pub-id></mixed-citation></ref><ref id="R47"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laneau</surname><given-names>J.</given-names></name><name><surname>Wouters</surname><given-names>J.</given-names></name><name><surname>Moonen</surname><given-names>M</given-names></name></person-group><article-title>Relative contributions of temporal and place pitch cues to fundamental frequency discrimination in cochlear implantees.</article-title>
<source>J Acoust Soc Am</source>, <year>2004</year>). <volume>116</volume>, <fpage>3606</fpage><lpage>3619</lpage>.<pub-id pub-id-type="pmid">15658711</pub-id></mixed-citation></ref><ref id="R48"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levitt</surname><given-names>H</given-names></name></person-group><article-title>Transformed up-down methods in psychoacoustics.</article-title>
<source>J Acoust Soc Am</source>, <year>1971</year>). <volume>49</volume>(<issue>2B</issue>), <fpage>467</fpage><lpage>477</lpage>. </mixed-citation></ref><ref id="R49"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>T</given-names></name><name><surname>Fu</surname><given-names>Q.-J</given-names></name></person-group><article-title>Voice gender discrimination provides a measure of more than pitch-related perception in cochlear implant users.</article-title>
<source>Int J Audiol</source>, <year>2011</year>). <volume>50</volume>, <fpage>498</fpage><lpage>502</lpage>.<pub-id pub-id-type="pmid">21696330</pub-id></mixed-citation></ref><ref id="R50"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>X.</given-names></name><name><surname>Fu</surname><given-names>Q. J.</given-names></name><name><surname>Galvin</surname><given-names>J. J.</given-names><suffix>3rd</suffix></name></person-group><article-title>Vocal emotion recognition by normal-hearing listeners and cochlear implant users.</article-title>
<source>Trends Amplif</source>, <year>2007</year>). <volume>11</volume>, <fpage>301</fpage><lpage>315</lpage>.<pub-id pub-id-type="pmid">18003871</pub-id></mixed-citation></ref><ref id="R51"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>X.</given-names></name><name><surname>Fu</surname><given-names>Q. J.</given-names></name><name><surname>Wu</surname><given-names>H. P.</given-names></name><etal/></person-group><article-title>Concurrent-vowel and tone recognition by Mandarin-speaking cochlear implant users.</article-title>
<source>Hear Res</source>, <year>2009</year>). <volume>256</volume>, <fpage>75</fpage><lpage>84</lpage>.<pub-id pub-id-type="pmid">19595753</pub-id></mixed-citation></ref><ref id="R52"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mackersie</surname><given-names>C. L</given-names></name></person-group><article-title>Talker separation and sequential stream segregation in listeners with hearing loss: Patterns associated with talker gender.</article-title>
<source>J Speech Lang Hear Res</source>, <year>2003</year>). <volume>46</volume>, <fpage>912</fpage><lpage>918</lpage>.<pub-id pub-id-type="pmid">12959469</pub-id></mixed-citation></ref><ref id="R53"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mackersie</surname><given-names>C. L.</given-names></name><name><surname>Dewey</surname><given-names>J.</given-names></name><name><surname>Guthrie</surname><given-names>L. A</given-names></name></person-group><article-title>Effects of fundamental frequency and vocal-tract length cues on sentence segregation by listeners with hearing loss.</article-title>
<source>J Acoust Soc Am</source>, <year>2011</year>). <volume>130</volume>, <fpage>1006</fpage><lpage>1019</lpage>.<pub-id pub-id-type="pmid">21877813</pub-id></mixed-citation></ref><ref id="R54"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mackersie</surname><given-names>C. L.</given-names></name><name><surname>Prida</surname><given-names>T. L.</given-names></name><name><surname>Stiles</surname><given-names>D</given-names></name></person-group><article-title>The role of sequential stream segregation and frequency selectivity in the perception of simultaneous sentences by listeners with sensorineural hearing loss.</article-title>
<source>J Speech Lang Hear Res</source>, <year>2001</year>). <volume>44</volume>, <fpage>19</fpage><lpage>28</lpage>.<pub-id pub-id-type="pmid">11218102</pub-id></mixed-citation></ref><ref id="R55"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Massida</surname><given-names>Z.</given-names></name><name><surname>Belin</surname><given-names>P.</given-names></name><name><surname>James</surname><given-names>C.</given-names></name><etal/></person-group><article-title>Voice discrimination in cochlear-implanted deaf subjects.</article-title>
<source>Hear Res</source>, <year>2011</year>). <volume>275</volume>, <fpage>120</fpage><lpage>129</lpage>.<pub-id pub-id-type="pmid">21167924</pub-id></mixed-citation></ref><ref id="R56"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Massida</surname><given-names>Z.</given-names></name><name><surname>Marx</surname><given-names>M.</given-names></name><name><surname>Belin</surname><given-names>P.</given-names></name><etal/></person-group><article-title>Gender categorization in cochlear implant users.</article-title>
<source>J Speech Lang Hear Res</source>, <year>2013</year>). <volume>56</volume>, <fpage>1389</fpage><lpage>1401</lpage>.<pub-id pub-id-type="pmid">24023381</pub-id></mixed-citation></ref><ref id="R57"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDermott</surname><given-names>H. J.</given-names></name><name><surname>McKay</surname><given-names>C. M</given-names></name></person-group><article-title>Musical pitch perception with electrical stimulation of the cochlea.</article-title>
<source>J Acoust Soc Am</source>, <year>1997</year>). <volume>101</volume>, <fpage>1622</fpage><lpage>1631</lpage>.<pub-id pub-id-type="pmid">9069629</pub-id></mixed-citation></ref><ref id="R58"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meister</surname><given-names>H.</given-names></name><name><surname>F&#252;rsen</surname><given-names>K.</given-names></name><name><surname>Streicher</surname><given-names>B.</given-names></name><etal/></person-group><article-title>The use of voice cues for speaker gender recognition in cochlear implant recipients.</article-title>
<source>J Speech Lang Hear Res</source>, <year>2016</year>). <volume>59</volume>, <fpage>546</fpage><lpage>556</lpage>.<pub-id pub-id-type="pmid">27135985</pub-id></mixed-citation></ref><ref id="R59"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meister</surname><given-names>H.</given-names></name><name><surname>Landwehr</surname><given-names>M.</given-names></name><name><surname>Pyschny</surname><given-names>V.</given-names></name><etal/></person-group><article-title>The perception of prosody and speaker gender in normal-hearing listeners and cochlear implant recipients.</article-title>
<source>Int J Audiol</source>, <year>2009</year>). <volume>48</volume>, <fpage>38</fpage><lpage>48</lpage>.<pub-id pub-id-type="pmid">19173112</pub-id></mixed-citation></ref><ref id="R60"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>B. C. J.</given-names></name><name><surname>Carlyon</surname><given-names>R. P</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Plack</surname><given-names>C. J.</given-names></name><name><surname>Oxenham</surname><given-names>A. J.</given-names></name><name><surname>Fay</surname><given-names>R. R.</given-names></name><name><surname>Popper</surname><given-names>A. N</given-names></name></person-group><article-title>Perception of pitch by people with cochlear hearing loss and by cochlear implant users.</article-title>
<source>In Pitch: Neural Coding and Perception</source> (pp. <year>2005</year>). <publisher-loc>New-York, NY</publisher-loc>: <publisher-name>Springer/Birkh&#228;user</publisher-name><fpage>234</fpage><lpage>277</lpage>). </mixed-citation></ref><ref id="R61"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>M&#252;hler</surname><given-names>R.</given-names></name><name><surname>Ziese</surname><given-names>M.</given-names></name><name><surname>Rostalski</surname><given-names>D</given-names></name></person-group><article-title>Development of a speaker discrimination test for cochlear implant users based on the Oldenburg Logatome corpus.</article-title>
<source>ORL J Otorhinolaryngol Relat Spec</source>, <year>2009</year>). <volume>71</volume>, <fpage>14</fpage><lpage>20</lpage>.<pub-id pub-id-type="pmid">18946229</pub-id></mixed-citation></ref><ref id="R62"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nogueira</surname><given-names>W.</given-names></name><name><surname>Litvak</surname><given-names>L. M.</given-names></name><name><surname>Edler</surname><given-names>B.</given-names></name><name><surname>Ostermann</surname><given-names>J.</given-names></name><name><surname>B&#252;chner</surname><given-names>A</given-names></name></person-group><article-title>Signal processing strategies for cochlear implants using current steering.</article-title>
<source>EURASIP J Adv Signal Process</source>, (<year>2009</year>). <year>2009</year>, <fpage>1</fpage><lpage>21</lpage>.</mixed-citation></ref><ref id="R63"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nogueira</surname><given-names>W.</given-names></name><name><surname>Rode</surname><given-names>T.</given-names></name><name><surname>B&#252;chner</surname><given-names>A</given-names></name></person-group><article-title>Spectral contrast enhancement improves speech intelligibility in noise for cochlear implants.</article-title>
<source>J Acoust Soc Am</source>, <year>2016</year>). <volume>139</volume>, <fpage>728</fpage><lpage>739</lpage>.<pub-id pub-id-type="pmid">26936556</pub-id></mixed-citation></ref><ref id="R64"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peterson</surname><given-names>G. E.</given-names></name><name><surname>Barney</surname><given-names>H. L</given-names></name></person-group><article-title>Control methods used in a study of the vowels.</article-title>
<source>J Acoust Soc Am</source>, (<year>1952</year>). <volume>24</volume>(<issue>2</issue>), <fpage>175</fpage><lpage>184</lpage>.</mixed-citation></ref><ref id="R65"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pyschny</surname><given-names>V.</given-names></name><name><surname>Landwehr</surname><given-names>M.</given-names></name><name><surname>Hahn</surname><given-names>M.</given-names></name><etal/></person-group><article-title>Bimodal hearing and speech perception with a competing talker.</article-title>
<source>J Speech Lang Hear Res</source>, <year>2011</year>). <volume>54</volume>, <fpage>1400</fpage><lpage>1415</lpage>.<pub-id pub-id-type="pmid">21498577</pub-id></mixed-citation></ref><ref id="R66"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qin</surname><given-names>M. K.</given-names></name><name><surname>Oxenham</surname><given-names>A. J</given-names></name></person-group><article-title>Effects of simulated cochlear-implant processing on speech reception in fluctuating maskers.</article-title>
<source>J Acoust Soc Am</source>, <year>2003</year>). <volume>114</volume>, <fpage>446</fpage><lpage>454</lpage>.<pub-id pub-id-type="pmid">12880055</pub-id></mixed-citation></ref><ref id="R67"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qin</surname><given-names>M. K.</given-names></name><name><surname>Oxenham</surname><given-names>A. J</given-names></name></person-group><article-title>Effects of envelope-vocoder processing on F0 discrimination and concurrent-vowel identification.</article-title>
<source>Ear Hear</source>, <year>2005</year>). <volume>26</volume>, <fpage>451</fpage><lpage>460</lpage>.<pub-id pub-id-type="pmid">16230895</pub-id></mixed-citation></ref><ref id="R68"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sagi</surname><given-names>E.</given-names></name><name><surname>Meyer</surname><given-names>T. A.</given-names></name><name><surname>Kaiser</surname><given-names>A. R.</given-names></name><etal/></person-group><article-title>A mathematical model of vowel identification by users of cochlear implants.</article-title>
<source>J Acoust Soc Am</source>, <year>2010</year>). <volume>127</volume>, <fpage>1069</fpage><lpage>1083</lpage>.<pub-id pub-id-type="pmid">20136228</pub-id></mixed-citation></ref><ref id="R69"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sauter</surname><given-names>D. A.</given-names></name><name><surname>Eisner</surname><given-names>F.</given-names></name><name><surname>Calder</surname><given-names>A. J.</given-names></name><etal/></person-group><article-title>Perceptual cues in nonverbal vocal expressions of emotion.</article-title>
<source>Q J Exp Psychol (Hove)</source>, <year>2010</year>). <volume>63</volume>, <fpage>2251</fpage><lpage>2272</lpage>.<pub-id pub-id-type="pmid">20437296</pub-id></mixed-citation></ref><ref id="R70"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname><given-names>R. V</given-names></name></person-group><article-title>Multichannel electrical stimulation of the auditory nerve in man. I. Basic psychophysics.</article-title>
<source>Hear Res</source>, <year>1983</year>). <volume>11</volume>, <fpage>157</fpage><lpage>189</lpage>.<pub-id pub-id-type="pmid">6619003</pub-id></mixed-citation></ref><ref id="R71"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skuk</surname><given-names>V. G.</given-names></name><name><surname>Schweinberger</surname><given-names>S. R</given-names></name></person-group><article-title>Influences of fundamental frequency, formant frequencies, aperiodicity, and spectrum level on the perception of voice gender.</article-title>
<source>J Speech Lang Hear Res</source>, <year>2014</year>). <volume>57</volume>, <fpage>285</fpage><lpage>296</lpage>.<pub-id pub-id-type="pmid">23882002</pub-id></mixed-citation></ref><ref id="R72"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>D. R.</given-names></name><name><surname>Patterson</surname><given-names>R. D</given-names></name></person-group><article-title>The interaction of glottal-pulse rate and vocal-tract length in judgements of speaker size, sex, and age.</article-title>
<source>J Acoust Soc Am</source>, <year>2005</year>). <volume>118</volume>, <fpage>3177</fpage><lpage>3186</lpage>.<pub-id pub-id-type="pmid">16334696</pub-id></mixed-citation></ref><ref id="R73"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>D. R.</given-names></name><name><surname>Patterson</surname><given-names>R. D.</given-names></name><name><surname>Turner</surname><given-names>R.</given-names></name><etal/></person-group><article-title>The processing and perception of size information in speech sounds.</article-title>
<source>J Acoust Soc Am</source>, <year>2005</year>). <volume>117</volume>, <fpage>305</fpage><lpage>318</lpage>.<pub-id pub-id-type="pmid">15704423</pub-id></mixed-citation></ref><ref id="R74"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steiger</surname><given-names>J. H</given-names></name></person-group><article-title>Tests for comparing elements of a correlation matrix.</article-title>
<source>Psychol Bull</source>, <year>1980</year>). <volume>87</volume>(<issue>2</issue>), <fpage>245</fpage><lpage>251</lpage>.</mixed-citation></ref><ref id="R75"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stickney</surname><given-names>G. S.</given-names></name><name><surname>Zeng</surname><given-names>F. G.</given-names></name><name><surname>Litovsky</surname><given-names>R.</given-names></name><etal/></person-group><article-title>Cochlear implant speech recognition with speech maskers.</article-title>
<source>J Acoust Soc Am</source>, <year>2004</year>). <volume>116</volume>, <fpage>1081</fpage><lpage>1091</lpage>.<pub-id pub-id-type="pmid">15376674</pub-id></mixed-citation></ref><ref id="R76"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stickney</surname><given-names>G. S.</given-names></name><name><surname>Assmann</surname><given-names>P. F.</given-names></name><name><surname>Chang</surname><given-names>J.</given-names></name><etal/></person-group><article-title>Effects of cochlear implant processing and fundamental frequency on the intelligibility of competing sentences.</article-title>
<source>J Acoust Soc Am</source>, <year>2007</year>). <volume>122</volume>, <fpage>1069</fpage><lpage>1078</lpage>.<pub-id pub-id-type="pmid">17672654</pub-id></mixed-citation></ref><ref id="R77"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summers</surname><given-names>V.</given-names></name><name><surname>Leek</surname><given-names>M. R</given-names></name></person-group><article-title>FO processing and the separation of competing speech signals by listeners with normal hearing and with hearing loss.</article-title>
<source>J Speech Lang Hear Res</source>, <year>1998</year>). <volume>41</volume>, <fpage>1294</fpage><lpage>1306</lpage>.<pub-id pub-id-type="pmid">9859885</pub-id></mixed-citation></ref><ref id="R78"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>R. E.</given-names></name><name><surname>Walters</surname><given-names>T. C.</given-names></name><name><surname>Monaghan</surname><given-names>J. J.</given-names></name><etal/></person-group><article-title>A statistical, formant-pattern model for segregating vowel type and vocal-tract length in developmental formant data.</article-title>
<source>J Acoust Soc Am</source>, <year>2009</year>). <volume>125</volume>, <fpage>2374</fpage><lpage>2386</lpage>.<pub-id pub-id-type="pmid">19354411</pub-id></mixed-citation></ref><ref id="R79"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vandali</surname><given-names>A. E.</given-names></name><name><surname>Sucher</surname><given-names>C.</given-names></name><name><surname>Tsang</surname><given-names>D. J.</given-names></name><etal/></person-group><article-title>Pitch ranking ability of cochlear implant recipients: A comparison of sound-processing strategies.</article-title>
<source>J Acoust Soc Am</source>, <year>2005</year>). <volume>117</volume>, <fpage>3126</fpage><lpage>3138</lpage>.<pub-id pub-id-type="pmid">15957780</pub-id></mixed-citation></ref><ref id="R80"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winn</surname><given-names>M. B.</given-names></name><name><surname>Chatterjee</surname><given-names>M.</given-names></name><name><surname>Idsardi</surname><given-names>W. J</given-names></name></person-group><article-title>The use of acoustic cues for phonetic identification: Effects of spectral degradation and electric hearing.</article-title>
<source>J Acoust Soc Am</source>, <year>2012</year>). <volume>131</volume>, <fpage>1465</fpage><lpage>1479</lpage>.<pub-id pub-id-type="pmid">22352517</pub-id></mixed-citation></ref><ref id="R81"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Won</surname><given-names>J. H.</given-names></name><name><surname>Drennan</surname><given-names>W. R.</given-names></name><name><surname>Rubinstein</surname><given-names>J. T</given-names></name></person-group><article-title>Spectral-ripple resolution correlates with speech reception in noise in cochlear implant users.</article-title>
<source>J Assoc Res Otolaryngol</source>, <year>2007</year>). <volume>8</volume>, <fpage>384</fpage><lpage>392</lpage>.<pub-id pub-id-type="pmid">17587137</pub-id></mixed-citation></ref><ref id="R82"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>F. G</given-names></name></person-group><article-title>Temporal pitch in electric hearing.</article-title>
<source>Hear Res</source>, <year>2002</year>). <volume>174</volume>, <fpage>101</fpage><lpage>106</lpage>.<pub-id pub-id-type="pmid">12433401</pub-id></mixed-citation></ref></ref-list><ref-list><title>REFERENCE NOTES</title><ref id="R83"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ba&#351;kent</surname><given-names>D.</given-names></name><name><surname>Luckmann</surname><given-names>A.</given-names></name><name><surname>Ceha</surname><given-names>J. M.</given-names></name><name><surname>Gaudrain</surname><given-names>E.</given-names></name><name><surname>Tamati</surname><given-names>T. N</given-names></name></person-group><source>The Discrimination of Voice Cues in Simulations of Bimodal Electroacoustic Cochlear-Implant Hearing</source>. <year>2017</year>). <publisher-loc>Oldenburg, Germany</publisher-loc>
<publisher-name>Poster presented at the 9th Speech in Noise Workshop</publisher-name>, <comment><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.5142280.v1">https://doi.org/10.6084/m9.figshare.5142280.v1</ext-link></comment>.</mixed-citation></ref><ref id="R84"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gaudrain</surname><given-names>E.</given-names></name><name><surname>Bhargava</surname><given-names>P.</given-names></name><name><surname>Ba&#351;kent</surname><given-names>D</given-names></name></person-group><article-title>Why does gap detection performance in cochlear implant users differ between free-field and direct-stimulation?</article-title>
<year>2016</year>). <conf-name>Presented at the Meeting of the Acoustical Society of America</conf-name>.</mixed-citation></ref><ref id="R85"><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gaudrain</surname><given-names>E.</given-names></name><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Ban</surname><given-names>V.</given-names></name><name><surname>Patterson</surname><given-names>R</given-names></name></person-group><article-title>The role of glottal pulse rate and vocal tract length in the perception of speaker identity.</article-title>
<year>2009</year>). <conf-name>Interspeech 2009: 10th Annual Conference of the International Speech Communication Association</conf-name>, <conf-date>1&#8211;5</conf-date>, <fpage>152</fpage><lpage>155</lpage>.</mixed-citation></ref></ref-list></back></article>