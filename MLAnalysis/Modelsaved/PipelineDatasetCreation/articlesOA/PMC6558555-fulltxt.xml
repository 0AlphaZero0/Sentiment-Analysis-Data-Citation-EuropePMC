<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Philos Trans R Soc Lond B Biol Sci</journal-id><journal-id journal-id-type="iso-abbrev">Philos. Trans. R. Soc. Lond., B, Biol. Sci</journal-id><journal-id journal-id-type="publisher-id">RSTB</journal-id><journal-id journal-id-type="hwp">royptb</journal-id><journal-title-group><journal-title>Philosophical Transactions of the Royal Society B: Biological Sciences</journal-title></journal-title-group><issn pub-type="ppub">0962-8436</issn><issn pub-type="epub">1471-2970</issn><publisher><publisher-name>The Royal Society</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6558555</article-id><article-id pub-id-type="pmid">31104604</article-id><article-id pub-id-type="doi">10.1098/rstb.2018.0277</article-id><article-id pub-id-type="publisher-id">rstb20180277</article-id><article-categories><subj-group subj-group-type="hwp-journal-coll"><subject>1001</subject><subject>44</subject><subject>87</subject></subj-group><subj-group subj-group-type="heading"><subject>Articles</subject></subj-group><subj-group subj-group-type="leader"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Context matters: using reinforcement learning to develop human-readable, state-dependent outbreak response policies</article-title><alt-title alt-title-type="short">Context dependent outbreak management</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Probert</surname><given-names>W. J. M.</given-names></name><xref ref-type="aff" rid="af1">1</xref></contrib><contrib contrib-type="author"><name><surname>Lakkur</surname><given-names>S.</given-names></name><xref ref-type="aff" rid="af2">2</xref></contrib><contrib contrib-type="author"><name><surname>Fonnesbeck</surname><given-names>C. J.</given-names></name><xref ref-type="aff" rid="af2">2</xref></contrib><contrib contrib-type="author"><name><surname>Shea</surname><given-names>K.</given-names></name><xref ref-type="aff" rid="af3">3</xref></contrib><contrib contrib-type="author"><name><surname>Runge</surname><given-names>M. C.</given-names></name><xref ref-type="aff" rid="af4">4</xref></contrib><contrib contrib-type="author"><name><surname>Tildesley</surname><given-names>M. J.</given-names></name><xref ref-type="aff" rid="af5">5</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-5251-8168</contrib-id><name><surname>Ferrari</surname><given-names>M. J.</given-names></name><xref ref-type="aff" rid="af3">3</xref><xref ref-type="corresp" rid="cor1"/></contrib></contrib-group><aff id="af1"><label>1</label><institution>Big Data Institute, Li Ka Shing Centre for Health Information and Discovery, Nuffield Department of Medicine, University of Oxford</institution>, <addr-line>Oxford OX3 7LF</addr-line>, <country>UK</country></aff><aff id="af2"><label>2</label><institution>Department of Biostatistics, Vanderbilt University</institution>, <addr-line>Nashville, TN 37203</addr-line>, <country>USA</country></aff><aff id="af3"><label>3</label><institution>Department of Biology, Center for Infectious Disease Dynamics, The Pennsylvania State University</institution>, <addr-line>University Park, PA 16802</addr-line>, <country>USA</country></aff><aff id="af4"><label>4</label><institution>US Geological Survey, Patuxent Wildlife Research Center</institution>, <addr-line>Laurel, MD 20708</addr-line>, <country>USA</country></aff><aff id="af5"><label>5</label><institution>Department of Life Sciences and Mathematics Institute, University of Warwick</institution>, <addr-line>Coventry CV4 7AL</addr-line>, <country>UK</country></aff><author-notes><corresp id="cor1">e-mail: <email>mferrari@psu.edu</email></corresp><fn fn-type="other"><p>One contribution of 16 to a theme issue &#8216;<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb/374/1776">Modelling infectious disease outbreaks in humans, animals and plants: epidemic forecasting and control</ext-link>&#8217;.</p></fn><fn fn-type="other"><p>Electronic supplementary material is available online at <uri xlink:href="https://dx.doi.org/10.6084/m9.figshare.c.4457807">https://dx.doi.org/10.6084/m9.figshare.c.4457807</uri>.</p></fn></author-notes><pub-date pub-type="ppub"><day>8</day><month>7</month><year>2019</year></pub-date><pub-date pub-type="epub"><day>20</day><month>5</month><year>2019</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>5</month><year>2019</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. --><volume>374</volume><issue>1776</issue><issue-title>Theme issue &#8216;Modelling infectious disease outbreaks in humans, animals and plants: epidemic forecasting and control&#8217; compiled and edited by Robin N. Thompson and Ellen Brooks-Pollock</issue-title><elocation-id>20180277</elocation-id><history><date date-type="accepted"><day>26</day><month>2</month><year>2019</year></date></history><permissions><copyright-statement>&#169; 2019 The Authors.</copyright-statement><copyright-year>2019</copyright-year><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>Published by the Royal Society under the terms of the Creative Commons Attribution License <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>, which permits unrestricted use, provided the original author and source are credited.</license-p></license><?release-delay 0|0?></permissions><self-uri content-type="pdf" xlink:href="rstb20180277.pdf"/><abstract><p>The number of all possible epidemics of a given infectious disease that could occur on a given landscape is large for systems of real-world complexity. Furthermore, there is no guarantee that the control actions that are optimal, on average, over all possible epidemics are also best for each possible epidemic. Reinforcement learning (RL) and Monte Carlo control have been used to develop machine-readable context-dependent solutions for complex problems with many possible realizations ranging from video-games to the game of Go. RL could be a valuable tool to generate context-dependent policies for outbreak response, though translating the resulting policies into simple rules that can be read and interpreted by human decision-makers remains a challenge. Here we illustrate the application of RL to the development of context-dependent outbreak response policies to minimize outbreaks of foot-and-mouth disease. We show that control based on the resulting context-dependent policies, which adapt interventions to the specific outbreak, result in smaller outbreaks than static policies. We further illustrate two approaches for translating the complex machine-readable policies into simple heuristics that can be evaluated by human decision-makers.</p><p>This article is part of the theme issue &#8216;Modelling infectious disease outbreaks in humans, animals and plants: epidemic forecasting and control&#8217;. This theme issue is linked with the earlier issue &#8216;Modelling infectious disease outbreaks in humans, animals and plants: approaches and important themes&#8217;.</p></abstract><kwd-group><kwd>machine learning</kwd><kwd>reinforcement learning</kwd><kwd>outbreak response</kwd><kwd>vaccination</kwd><kwd>optimal control</kwd><kwd>FMD</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution>National Institute of Allergy and Infectious Diseases</institution><institution-id>http://dx.doi.org/10.13039/100000060</institution-id></institution-wrap></funding-source><award-id>1 R01 GM105247-01</award-id></award-group><award-group><funding-source><institution-wrap><institution>Biotechnology and Biological Sciences Research Council</institution><institution-id>http://dx.doi.org/10.13039/501100000268</institution-id></institution-wrap></funding-source><award-id>BB/K010972/4</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>cover-date</meta-name><meta-value>July 8, 2019</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1"><label>1.</label><title>Introduction</title><p>Computational models of disease spread and control have been widely used in preparedness planning for outbreaks of infectious disease to both forecast outbreak severity [<xref rid="RSTB20180277C1" ref-type="bibr">1</xref>&#8211;<xref rid="RSTB20180277C4" ref-type="bibr">4</xref>] and decide among competing control interventions [<xref rid="RSTB20180277C5" ref-type="bibr">5</xref>&#8211;<xref rid="RSTB20180277C10" ref-type="bibr">10</xref>]. A conventional approach for determining optimal interventions has been to evaluate the expected performance of different potential interventions across a large number of stochastic simulations [<xref rid="RSTB20180277C5" ref-type="bibr">5</xref>,<xref rid="RSTB20180277C11" ref-type="bibr">11</xref>&#8211;<xref rid="RSTB20180277C13" ref-type="bibr">13</xref>]; thus, interventions are ranked according to expected performance over all possible outbreaks. In a real outbreak, however, only one realization is of concern and there is no guarantee that the control action that performs best, on average, is also optimal for any specific outbreak. The current state of an outbreak dramatically reduces the set of all possible future trajectories. The decision-maker should therefore seek to find the optimal action conditional on the current state of the outbreak [<xref rid="RSTB20180277C14" ref-type="bibr">14</xref>,<xref rid="RSTB20180277C15" ref-type="bibr">15</xref>]. Thus, we define a &#8216;state-dependent policy&#8217; as a rule set that defines an action to be taken, conditional on the current state of the dynamical system (here an infectious disease outbreak).</p><p>While it is intuitive that outbreak response should be dependent on the specific realization of an outbreak, generating optimal policies <italic>a priori</italic> is computationally challenging. A brute force solution to this problem requires enumerating all possible states of the epidemic system&#8212;e.g. for all spatial distributions of susceptible, infected and controlled premises at a given point in time&#8212;and for each state simulating all possible futures conditional on each action one could take. The results of such an exercise could then be stored in a look-up table that returned the action that performed best over all future simulations for any given state. For even relatively small problems, this approach is computationally intractable and grossly inefficient, at least in part because a large amount of computation would be expended evaluating states that are only rarely observed (though function approximation methods can help to overcome this; e.g. [<xref rid="RSTB20180277C16" ref-type="bibr">16</xref>,<xref rid="RSTB20180277C17" ref-type="bibr">17</xref>]).</p><p>In lieu of identifying optimal policies, other computational methods have been used to provide guidance for management. Regression methods, for example, have been used to model associations between predefined predictors and the occurrence of a disease outbreak [<xref rid="RSTB20180277C18" ref-type="bibr">18</xref>,<xref rid="RSTB20180277C19" ref-type="bibr">19</xref>]. Similarly, network-based approaches have been implemented in contact tracing efforts to retrospectively learn about outbreak propagation [<xref rid="RSTB20180277C20" ref-type="bibr">20</xref>&#8211;<xref rid="RSTB20180277C22" ref-type="bibr">22</xref>]. Formally, a decision-analytic framework with a well-defined objective to be optimized is required to ensure optimality. Methods such as dynamic programming offer an exactly optimal solution, and heuristics such as simulated annealing or genetic algorithms can provide a competitive (though not guaranteed optimal) solution [<xref rid="RSTB20180277C23" ref-type="bibr">23</xref>&#8211;<xref rid="RSTB20180277C25" ref-type="bibr">25</xref>].</p><p>Reinforcement learning (RL), a class of machine learning algorithms, and Monte Carlo (MC) control use feedback from real or simulated systems to estimate optimal actions for states that are likely to be visited, as governed by a dynamic model [<xref rid="RSTB20180277C26" ref-type="bibr">26</xref>]. RL algorithms involve the interaction of some environment, which encapsulates a system that is to be managed, with an agent that learns how to manage the environment through direct experience (<xref ref-type="fig" rid="RSTB20180277F1">figure&#160;1</xref>). The state can be defined as a statistic of the environment, such that it includes all the information that is relevant to model the decision-making problem, which in an outbreak setting might include attributes such as the number of infected individuals, the available store of vaccine, etc. The agent chooses actions to take in order to achieve its objective, while responses to the agent's actions occur in the environment, including rewards. The output from an RL control algorithm is a policy, providing a rule set that maps states of the system to actions to take when the system is in that state [<xref rid="RSTB20180277C26" ref-type="bibr">26</xref>]. Underpinning a policy is a value function, <italic>Q</italic>(<italic>s,a</italic>), which returns the expected total future value of following an estimated optimal policy, across the space of all likely future states, conditional on selecting action <italic>a</italic> from the current state <italic>s</italic>. RL methods iteratively improve the estimate of <italic>Q</italic>(<italic>s</italic>,<italic>a</italic>) by repeated simulation using feedback from the system, choosing actions that balance the precise estimation of the current best policy with the search for better policies. For instance, actions may be chosen using an epsilon-greedy algorithm whereby the current best action is chosen most of the time but a small (epsilon) proportion of the time, an exploratory random action is chosen to diversify and potentially enhance understanding of the outcomes [<xref rid="RSTB20180277C26" ref-type="bibr">26</xref>]. A stochastic simulation model ensures that likely future states are explored, and the RL algorithm ensures that the range of actions is evaluated for each state. This approach is especially advantageous in larger decision spaces, where it may be infeasible to visit each state&#8211;action pair many times. Computational runtime can be improved by using function approximation in place of a look-up table, allowing for generalization across neighbouring states and actions, and the corresponding reward. Thus, without sampling the full state-action space, RL may reveal novel patterns learned over the course of training [<xref rid="RSTB20180277C27" ref-type="bibr">27</xref>].
<fig id="RSTB20180277F1" orientation="portrait" position="float"><label>Figure 1.</label><caption><p>Schematic of Monte Carlo control for solving RL problem (adapted from Sutton and Barto [<xref rid="RSTB20180277C26" ref-type="bibr">26</xref>]). (Online version in colour.)</p></caption><graphic xlink:href="rstb20180277-g1"/></fig></p><p>Although we are not aware of a formal proof of MC control algorithms guaranteeing an optimal policy [<xref rid="RSTB20180277C26" ref-type="bibr">26</xref>,<xref rid="RSTB20180277C28" ref-type="bibr">28</xref>], MC control and RL have an impressive track record, having been successfully applied to win games, such as backgammon [<xref rid="RSTB20180277C16" ref-type="bibr">16</xref>], Atari computer games [<xref rid="RSTB20180277C27" ref-type="bibr">27</xref>] and Go [<xref rid="RSTB20180277C29" ref-type="bibr">29</xref>], and in the control of robots [<xref rid="RSTB20180277C30" ref-type="bibr">30</xref>]. In these applications, the intended user of the policy is a computer, rather than a human, and so, because of the ease of storing and looking-up large multi-dimensional policies on a computer, there is little or no constraint on the number of state variables used to construct the policy. These areas of application have thus been concerned with demonstrating how RL methods can be used to replace human decision-makers. Here, our aim is to generate policies that will support human decision-making by presenting a low-dimensional representation of a context-dependent policy that can help to inform more nuanced decision-making in outbreak situations.</p><p>We present two case studies that use RL and MC control to develop state-dependent response policies in the context of a livestock outbreak, based on the dynamics of the 2001 foot-and-mouth disease (FMD) outbreak in the UK. In the first case study, we consider the initial stages of an outbreak, where the state space is relatively small, and generate state-dependent RL policies using deep Q-learning. In the second case study, we consider a spatially large-scale epidemic and illustrate the development of a state-dependent policy using a reduced two-dimensional summary of the full state space; thus, the resulting policy is readable as a two-dimensional mapping of summary states to control interventions. Finally, we discuss the challenges and opportunities for the application of RL to outbreak control policies.</p></sec><sec sec-type="methods" id="s2"><label>2.</label><title>Material and methods</title><sec id="s2a"><label>(a)</label><title>The foot-and-mouth disease system</title><p>FMD, a viral disease of livestock, is a pathogen for which epidemiological models have been widely applied. For FMD-free countries, such as the UK and USA, emergency preventative measures aim to avoid the large economic ramifications of such outbreaks that result from the cessation of trade [<xref rid="RSTB20180277C31" ref-type="bibr">31</xref>,<xref rid="RSTB20180277C32" ref-type="bibr">32</xref>]. We use a stochastic, individual-based model of FMD spread, where the modelling unit is a premises (farm), based on Keeling <italic>et al</italic>. [<xref rid="RSTB20180277C5" ref-type="bibr">5</xref>] and Tildesley <italic>et al</italic>. [<xref rid="RSTB20180277C33" ref-type="bibr">33</xref>] (details are presented in the electronic supplementary material). The probability of virus spread between farms is modelled as both an increasing function of farm size and a decreasing function of physical/geographical separation [<xref rid="RSTB20180277C8" ref-type="bibr">8</xref>]. Each premises can be in one of four epidemiological states: susceptible, exposed, infectious or removed/immune. A fixed period of time is assumed from virus exposure to being infectious, from virus exposure to notification of FMD infection (in case study 2), and between notification and the time at which culling on a premises begins [<xref rid="RSTB20180277C5" ref-type="bibr">5</xref>,<xref rid="RSTB20180277C33" ref-type="bibr">33</xref>].</p></sec><sec id="s2b"><label>(b)</label><title>Case study 1: deep Q-networks</title><p>We used deep Q-networks (DQN) to derive FMD outbreak response policies on three different landscapes. DQN combine RL with convolutional neural networks (CNN), which serve as function approximators. DQN are particularly suited to image data inputs owing to their use of CNN, which are able to extract low-level feature information and combine it with other features to represent abstract concepts [<xref rid="RSTB20180277C28" ref-type="bibr">28</xref>,<xref rid="RSTB20180277C34" ref-type="bibr">34</xref>], such as nonlinear value functions. In this case study, the objective was to terminate the outbreak as quickly as possible, with minimal costs, specified by the immediate reward <italic>r</italic> in the action-value function. We defined the state at time <italic>t</italic> using an image of the disease outbreak to capture the spatial relationships between farm locations (<xref ref-type="fig" rid="RSTB20180277F2">figure&#160;2</xref>).
<fig id="RSTB20180277F2" orientation="portrait" position="float"><label>Figure 2.</label><caption><p>Three preprocessing steps to construct the state (<italic>a</italic>&#8211;<italic>c</italic>). (<italic>a</italic>) We first plot the farms and identify farm-level infection statuses: black = infected, white = susceptible; then (<italic>b</italic>) overlay a grid to &#8216;pixelate&#8217; the landscape so that no more than one farm occupies a pixel; then (<italic>c</italic>) construct a two-dimensional array of farm-level infection status: 0 = no farm, 1 = infected farm, 2 = susceptible farm. (<italic>d</italic>) Schematic of utility table, with flattened states as rows and actions as columns. Shaded cell represents the action with the highest utility for the state in each row. The RL methods in both case studies seek to approximate the value function represented in this &#8216;look-up table&#8217; representation of the state-action space.</p></caption><graphic xlink:href="rstb20180277-g2"/></fig></p><p>To better illustrate the motivation for DQN, we highlight scenario 1, in which the landscape comprised 30 farms, ranging in size from 25 to 500 animals (<xref ref-type="fig" rid="RSTB20180277F2">figure&#160;2</xref>). We assumed that only one farm could be culled per day, thus 30 possible culling actions were available at the first time step (<xref ref-type="fig" rid="RSTB20180277F2">figure&#160;2</xref>). Under this state&#8211;action space, expected utility updates for each state&#8211;action pair would be computationally intensive, both in terms of computational time and memory storage. Specifically, with three possible states for each farm, there are 3<sup>30</sup> possible states and 30 possible culling actions at the first time step. A look-up table would therefore require 6.17 &#215; 10<sup>15</sup> cells to be updated (e.g. rows in <xref ref-type="fig" rid="RSTB20180277F2">figure&#160;2</xref><italic>d</italic>). DQN were used instead to approximate the action-value function and are well suited to the image input. The FMD simulation model was coded in Python 3.6 and the DQN were implemented using TensorFlow and the Keras submodule (see electronic supplementary material for pseudo-code and details).</p><p>The action&#8211;value function <italic>Q</italic>(<italic>s,a</italic>) allows us to evaluate how a particular action, in the context of the state of the environment, contributes to achieving the encoded objective. The immediate reward (<italic>r</italic>) is a key component in calculating the expected utility in DQN:<disp-formula id="RSTB20180277UM1"><mml:math id="DM1"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#8242;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>&#947;</mml:mi><mml:mrow><mml:mi mathvariant="normal">ma</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#8242;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#8242;</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mi>s</mml:mi><mml:mspace width="-1pt"/><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>where the prime superscript indicates the subsequent state, <italic>s</italic>, (after taking action <italic>a</italic>). In our case study, <italic>r</italic> was defined as:<disp-formula id="RSTB20180277UM2"><mml:math id="DM2"><mml:mtable columnalign="right left" rowspacing=".5em" columnspacing="thickmathspace" displaystyle="true"><mml:mtr><mml:mtd columnalign="left"><mml:mi>r</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mo>&#8722;</mml:mo><mml:mn>100</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">number</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">of</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">farms</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">culled</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>&#8201;</mml:mo><mml:mo>+</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"/><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mo>&#8722;</mml:mo><mml:mn>500</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">number</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">of</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">times</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">out</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">break</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">resurges</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">due</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">to</mml:mi></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"/><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">neglecting</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">the</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">culling</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">of</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">exposed</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">farms</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"/><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mo>+</mml:mo><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">number</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">of</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">remaining</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">cattle</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">by</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">the</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">end</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">of</mml:mi></mml:mrow><mml:mo>&#8201;</mml:mo><mml:mrow><mml:mi mathvariant="normal">management</mml:mi></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>Resurgence here refers to an outbreak going from 0 to 1 infected farm due to the transition of an undetected exposed farm. Here, the weights were manually chosen to penalize culling of farms more severely than individual cattle and put a severe penalty on stopping culling when farms that are exposed, but not yet detected, are still present.</p></sec><sec id="s2c"><label>(c)</label><title>Case study 2: applying reinforcement learning to a summary state space</title><p>In the second case study, our objective is to minimize the duration of an FMD outbreak over a landscape of 4001 randomly distributed farms of 500 cattle each. Parameters governing both the seeding of the outbreak and of control interventions were similar to previous modelling studies (e.g. [<xref rid="RSTB20180277C8" ref-type="bibr">8</xref>,<xref rid="RSTB20180277C35" ref-type="bibr">35</xref>,<xref rid="RSTB20180277C36" ref-type="bibr">36</xref>]). Each simulation used the same four seeding premises.</p><p>We assume that managers can implement one of two control interventions&#8212;ring culling or ring vaccination&#8212;after an initial period of silent spread and obligatory culling of infected premises (IPs). Ring culling or ring vaccination designate premises to be culled (or vaccinated) that are within a predefined radius (here 3 km) of premises that are confirmed to be infected with FMD. The rate of ring culling was limited to 200 animals culled per day per premises. Disposal of culled carcasses was assumed to occur at a rate of 200 animals per day per premises. We assumed there was a global constraint on the number of animals culled per day because of the public health and public perception concerns of a large build-up of undisposed carcasses [<xref rid="RSTB20180277C37" ref-type="bibr">37</xref>]. This assumption limits new culling when the number of carcasses reaches a &#8216;carcass constraint&#8217;. For vaccination, we assumed that immunity was conferred 7 days after vaccination date and that vaccine efficacy was 100%. Premises exposed to FMD before conferral of immunity were assumed to progress to an infectious state. Vaccination was delivered at a rate of 200 animals per day for each premises and it was assumed there was a global constraint on the number of vaccines administered per day of 10 000 doses.</p><p>Here we assume that a single decision must be made to implement ring culling or ring vaccination at 3 km after 12 days of silent spread and 7 days of IPs culling. Thus, while the decision space is much smaller (two actions) than in case study 1, the number of premises (4001 farms) is much larger. We therefore generate a state-dependent policy that is human readable by constraining the state space to a two-dimensional summary. Two summary state variables were used to construct the policies: (1) number of IPs at the decision time point, and (2) area of the outbreak at the decision time point. Area of the outbreak was represented by the convex hull that included any culled, confirmed infected or exposed premises. We chose the number of IPs as a state variable because (a) it is correlated with time until the first detection, which has been cited elsewhere as important for predicting the severity of outbreaks [<xref rid="RSTB20180277C38" ref-type="bibr">38</xref>]; (b) it changes throughout the course of an outbreak; and (c) ring culling and ring vaccination strategies take place in areas surrounding IPs so the application and outcome of these actions will vary according to the number of infected premises. The area of the outbreak was chosen as a state variable because it increases monotonically as an outbreak progresses and it is therefore possible, in combination with the number of IPs, to distinguish between the start of an outbreak and the end of an outbreak using these two states.</p><p>We used epsilon-soft MC control to construct control policies using 100 000 outbreak simulations [<xref rid="RSTB20180277C26" ref-type="bibr">26</xref>]. Epsilon, which determines how often to choose a currently non-optimal action, was set at 0.1 (see electronic supplementary material for alternative values of epsilon). Pseudo-code for this algorithm is provided in the supplementary material. RL algorithms and epidemiological simulation models were coded in Python 3.5.2 using the packages numpy (1.14.1), pandas (0.22.0) and Cython (0.28.3). The RL code for case study 2 is available at the following repository: <uri xlink:href="https://github.com/p-robot/context_matters">https://github.com/p-robot/context_matters</uri>.</p></sec></sec><sec id="s3"><label>3.</label><title>Results</title><sec id="s3a"><label>(a)</label><title>Case study 1</title><p>We illustrate DQN using three scenarios, each reflecting a different population spatial structure. The first was a small-scale example with 30 farms distributed uniformly at random on a 10 &#215; 10 km grid with six farms initially infected (<xref ref-type="fig" rid="RSTB20180277F3">figure&#160;3</xref><italic>a</italic>(i)). We trained the DQN for 10 000 episodes, and the associated total reward trajectory illustrates a gradual increase in total reward over the course of training (<xref ref-type="fig" rid="RSTB20180277F3">figure&#160;3</xref><italic>a</italic>(ii)). This suggests that the DQN was able to choose better farm culling sequences following 10 000 episodes of training. Though the final optimal policy is difficult to illustrate due to the size of the state&#8211;action space, we can summarize the behaviour of the optimal policy by illustrating how often each farm was culled under many simulations of management under the optimal policy (<xref ref-type="fig" rid="RSTB20180277F3">figure&#160;3</xref><italic>a</italic>(iii)). The farms that were culled tended to be both larger and closer to the initially infected farms than the average, which is consistent with the underlying transmission model. The reward trajectory does not indicate convergence to an optimal policy, though over only 10 000 episodes the policy performs, on average, better than both a policy of either culling farms at random (which may remove as yet undetected premises) or a policy of culling only known IPs (which necessarily implements culling only after farms can potentially transmit, <xref ref-type="fig" rid="RSTB20180277F3">figure&#160;3</xref><italic>a</italic>(iv)), or a policy of ring culling. Due to the one-farm-per-day culling constraint, ring culling was implemented by ranking farms to be culled by the closest distance to an infected premises [<xref rid="RSTB20180277C8" ref-type="bibr">8</xref>].
<fig id="RSTB20180277F3" orientation="portrait" position="float"><label>Figure 3.</label><caption><p>The spatial distribution of farms for the three scenarios (rows <italic>a</italic>&#8211;<italic>c</italic> respectively); circle size scales with farm size (column i). (<italic>a</italic>(ii)-<italic>c</italic>(ii)) Performance of DQN, in terms of the reward, r, for each case study during training. (<italic>a</italic>(iii)-<italic>c</italic>(iii)) The frequency at which susceptible farms were culled during 2000 simulations of testing (colours) plotted as a function of the mean distance to the initially infected farms and the farm size. (<italic>a</italic>(iv)-<italic>c</italic>(iv)) The distribution of rewards for 2000 simulations using either the best DQN policy, a policy of culling farms at random (e.g. a null policy), or a policy of culling infected premises (IPs) or a policy of ring culling.</p></caption><graphic xlink:href="rstb20180277-g3"/></fig></p><p>The second scenario was a larger simulation with 120 farms distributed uniformly at random on a 15 &#215; 15 km grid with 10 farms initially infected (<xref ref-type="fig" rid="RSTB20180277F3">figure&#160;3</xref><italic>b</italic>(i)). Here we assume that all of the cattle at any five farms per day could be culled. As above, the DQN failed to converge on an approximately optimal policy (<xref ref-type="fig" rid="RSTB20180277F3">figure&#160;3</xref><italic>b</italic>(ii)), though, as in case study 1, the best policy found consistently prioritized culling of farms near infected premises that were larger than average (<xref ref-type="fig" rid="RSTB20180277F3">figure&#160;3</xref><italic>b</italic>(iii)). The best DQN policy did not perform better than a policy of culling only IPs (<xref ref-type="fig" rid="RSTB20180277F3">figure&#160;3</xref><italic>b</italic>(iv)) or ring culling. The possible state&#8211;action space for this case study is much larger, 2.16 &#215; 10<sup>59</sup> possible state&#8211;action pairs, and would likely require a considerably longer training period.</p><p>The third scenario also used 30 farms, with 6 farms initially infected and a daily culling capacity of one farm, but assumes a clustered spatial distribution of farms with the initial infections all in a single cluster, the farms in the lower left corner (<xref ref-type="fig" rid="RSTB20180277F3">figure&#160;3</xref><italic>c</italic>(i)) and a strongly distance-dependent transmission kernel; thus, infection must pass through the &#8216;bridge&#8217; farms to reach the second patch. Here, the reward for the DQN policy appears to plateau over a 10 000 episode training period (<xref ref-type="fig" rid="RSTB20180277F3">figure&#160;3</xref><italic>c</italic>(ii)). Here the optimal policy is simpler than above and the DQN learned to cull the bridge farms. In 2000 simulations of testing, at least one of the two bridging farms was culled 1744 times while it was still susceptible. Both bridging farms were always culled (100% of simulations) before they became infected, i.e. culled while susceptible or exposed, preventing the outbreak from spreading to the second cluster of farms. The pattern of susceptible farms culled was more correlated with location than farm size (<xref ref-type="fig" rid="RSTB20180277F3">figure&#160;3</xref><italic>c</italic>(iii)). The resulting DQN policy here performs better than both random culls, culling of IPs and ring culling because of the benefit of culling bridging premises while they are still susceptible (<xref ref-type="fig" rid="RSTB20180277F3">figure&#160;3</xref><italic>c</italic>(iv)). Additional challenges with DQN are discussed in the electronic supplementary material, figure S2.</p></sec><sec id="s3b"><label>(b)</label><title>Case study 2</title><p>Because we summarized the state space for this case study in two dimensions, using the spatial extent of the outbreak and the number of farms infected, we can plot a map of the resulting RL policy (<xref ref-type="fig" rid="RSTB20180277F4">figure&#160;4</xref><italic>a</italic>), indicating the best action to take for each position in the summary state space. For stringent carcass constraints (less than or equal to 12 000), vaccination was the optimal control intervention for almost all states (<xref ref-type="fig" rid="RSTB20180277F4">figure&#160;4</xref><italic>a</italic>(i)). With ample resources (carcass constraint greater than or equal to 18 000), culling was almost always optimal regardless of the state of the outbreak (<xref ref-type="fig" rid="RSTB20180277F4">figure&#160;4</xref><italic>a</italic>(iv); see electronic supplementary material for additional carcass constraints). Between these extremes in resources, the optimal policy was composed of a mix of culling and vaccination depending upon the number of infected premises and area of the outbreak at the time point in question (<xref ref-type="fig" rid="RSTB20180277F4">figure&#160;4</xref><italic>a</italic>(ii),(iii),<italic>b</italic>,<italic>d</italic>; see electronic supplementary material, figure S6 for policies for all carcass constraints from 10 000 to 20 000). Over much of the summarized state space, the expected difference in actions was small, but the difference was large for extreme, though rare, outbreaks (<xref ref-type="fig" rid="RSTB20180277F4">figure&#160;4</xref><italic>c,d</italic>). Notably, for intermediate culling constraints, culling is preferred for outbreaks that are small, in a number of premises, relative to their areal extent (i.e. more densely clustered), but vaccination is more likely favoured when outbreaks are sparse (<xref ref-type="fig" rid="RSTB20180277F4">figure&#160;4</xref><italic>a</italic>(iii),(iv); electronic supplementary material, figure S6). On average, over all carcass constraints (electronic supplementary material, figure S6), the RL strategy resulted in shorter outbreaks (<xref ref-type="fig" rid="RSTB20180277F4">figure&#160;4</xref><italic>e</italic>; mean difference (IQR): 2.6 (0,4) days); note that the small average difference occurs because outbreaks, where switching of actions is recommended and may achieve large differences in outcome, are relatively rare (<xref ref-type="fig" rid="RSTB20180277F4">figure&#160;4</xref><italic>d</italic>). Developing general rules using RL remains an active area of research (e.g. [<xref rid="RSTB20180277C39" ref-type="bibr">39</xref>]). The RL policies presented here are conditional on the starting conditions used for these simulations. In the electronic supplementary material, we show that these policies perform on par with, but not better than, the static strategies when outbreaks are seeded with random starting conditions that the learner was not exposed to.
<fig id="RSTB20180277F4" orientation="portrait" position="float"><label>Figure 4.</label><caption><p>Optimal policies to minimize outbreak duration as a function of outbreak area and number of infected premises. (<italic>a</italic>) Output policy for minimizing outbreak duration for different carcass constraints: (i) 11 000, (ii) 13 000, (iii) 15 000, (iv) 17 000 carcasses. (<italic>b</italic>) Histogram of outbreak duration following enacting ring culling or ring vaccination for states highlighted in <italic>a</italic>(iii). (<italic>c</italic>) Heatmap of the frequency of visits to each state throughout all simulations used to construct the RL policy in <italic>a</italic>(iii). (<italic>d</italic>) Heatmap of the difference in outbreak duration when using ring culling at 3 km or ring vaccination at 3 km for each state for the carcass constraint illustrated in <italic>a</italic>(iii). (<italic>e</italic>) Distribution of outbreak duration for simulations (using culling constraints ranging from 10 000 to 20 000; see electronic supplementary material, figure S6 for all policies) managed using the RL policy compared with static policies of ring culling and ring vaccination; circles give mean, bars give IQR. (Online version in colour.)</p></caption><graphic xlink:href="rstb20180277-g4"/></fig></p></sec></sec><sec id="s4"><label>4.</label><title>Discussion</title><p>Historically, management interventions have indeed changed as outbreaks have progressed [<xref rid="RSTB20180277C31" ref-type="bibr">31</xref>]. Constructing optimal policies that anticipate these changes, however, is a non-trivial computational task. Here we have shown that RL may be a useful tool for developing state-dependent policies that outperform static strategies and yet nevertheless can easily be interpreted by human decision-makers. The RL approach is a significant improvement over the conventional comparison of static policies, as it allows the discovery of optimal state-dependent control policies [<xref rid="RSTB20180277C26" ref-type="bibr">26</xref>] and the generation of non-intuitive control policies that are not limited to consideration only of control policies that can be defined <italic>a priori</italic>.</p><p>Through our first case study, we illustrated that RL can efficiently estimate approximately optimal state-dependent policies for outbreak response control problems where an exhaustive search through the state space is computationally infeasible. Though the resulting policy is itself too complicated to illustrate simply, we showed that the state-dependent RL policy produces intuitive control for simple landscapes&#8212;prioritizing culling on farms that are at high risk of infection because of proximity, or high potential for onward spread because of large farm size. On the clustered landscape (scenario 3), the RL policy also identified the &#8216;bridge nodes&#8217; between clusters as optimal sites for culling to prevent spread between clusters, highlighting the dependence of the optimal policy on the landscape [<xref rid="RSTB20180277C40" ref-type="bibr">40</xref>]. Thus, <italic>post hoc</italic> analysis of RL policies can help to develop heuristics that can inform policy decisions.</p><p>In our second case study, we presented an approach that uses MC control to generate low-dimensional, human-readable state-dependent policies. We used simulation first to illustrate the epidemic settings (here a constraint on culling capacity) for which a state-dependent policy results in an expected benefit, and second to show that <italic>a priori</italic> definition of a simple summary state representation can help to guarantee a human-readable policy. A human-readable policy is not as easily obtained with DQN (<xref ref-type="fig" rid="RSTB20180277F2">figure&#160;2</xref><italic>d</italic>). Utilities can be assigned to each state&#8211;action pair, thought of as a look-up table; however, visualization of this table becomes difficult when the state is large.</p><p>There are three key limitations in delivering practical state-dependent policies: computational challenges, challenges in interpretation and communication of the output policies, and challenges in implementation.</p><p>RL itself offers a solution to the computational challenge of an exhaustive search through the state&#8211;action space; here we illustrated the development of state-dependent policies using simulations over only a small fraction of the state space. However, as seen in our first case study, long training periods may be necessary to achieve approximately optimal policies for large state spaces; two weeks of training was required for scenario 2 of case study 1. Deploying such methods in a real outbreak may require parallelization of simulation models or highly efficient computational code, and the RL algorithms themselves may require tuning of hyperparameters (see further discussion in the electronic supplementary material). Some of these details could be tested in non-outbreak settings to improve reaction to real outbreaks.</p><p>The interpretation of policies, particularly those that are generated in settings with a high-dimensional state space, and the communication of output policies to policymakers remains a challenge. There are two pathways to producing human-readable policies: (1) by generating a full policy, then using statistical methods to reduce the policy itself to a manageable dimension (as in case study 1); and (2) by simplifying the state space prior to searching for the optimal policy (as in case study 2). We note that there is no guarantee that a summary state space (e.g. case study 2) will necessarily result in an improved expected performance benefit or a tractable state-dependent policy. Thus, the choice of this summary state representation requires careful thought and expert input. An RL policy can itself be subjected to further analysis; machine learning methods, such as classification and regression trees, have been used to highlight variables that have a large influence on the severity of outbreaks (e.g. [<xref rid="RSTB20180277C38" ref-type="bibr">38</xref>]) and to provide a starting point for the systematic selection of state variables.</p><p>The translation of the theoretical gains from using state-dependent control into real-world action requires operational mechanisms that may not yet exist; e.g. pre-existing data sharing agreements and transfer to allow real-time state updating or logistical infrastructure for switching response teams between control activities. Modelling and optimization can be used in scenario-planning exercises before any outbreak to investigate state-dependent preparedness plans and communicate findings to policymakers. During emergencies, systems must already be in place to allow rapid communication and dissemination of data on the state of the outbreak, and resources must be available to enable redeployment, or repurposing, of personnel.</p><p>Several research questions are opened up by our approach. It remains to be determined what is the limiting complexity of a policy; for example, what is the best low-dimensional representation of the state space, or what is the upper limit of complexity of the state space, to ensure the resultant policy is both interpretable and logistically feasible in the field? Simple state-dependent policies already exist for emergency response in the form of flow diagrams (e.g. [<xref rid="RSTB20180277C41" ref-type="bibr">41</xref>] figure E p. 72, or [<xref rid="RSTB20180277C42" ref-type="bibr">42</xref>]) and previous research regarding likelihood of adoption of computer-based aids for clinical decision-making identified the ability of a system to justify the advice it was providing as most important [<xref rid="RSTB20180277C43" ref-type="bibr">43</xref>]. The likelihood of adoption of state-dependent policies may depend critically on the complexity and communication of the policy, and recent interest in &#8216;explainable AI&#8217; may be the catalyst for initiating such investigations [<xref rid="RSTB20180277C39" ref-type="bibr">39</xref>].</p><p>Here, we have assumed the model is known, but in a real outbreak, parameter estimation and/or model selection may occur simultaneously with the construction of RL policies. It may be possible, however, to use state variables representing a measure of model uncertainty, thereby allowing RL methods to identify control actions that would reduce uncertainty through time (e.g. using active adaptive management; [<xref rid="RSTB20180277C44" ref-type="bibr">44</xref>,<xref rid="RSTB20180277C45" ref-type="bibr">45</xref>]).</p><p>Here, we have ignored the additional operational costs of measuring the state space (e.g. surveillance) and of switching among management actions (e.g. overhead costs or costs such as travelling between premises). Additional work to account for these costs is critical to the full evaluation of these methods. Finally, the choice of null strategies against which to assess the performance of state-dependent policies is not always easy. In case study 2, the comparison was against precedents in the modelling literature, but for our case study 1, the choice of null strategy does not have a precedent, and some potential baseline strategies, such as no management, are unrealistic comparisons in an outbreak scenario given there are minimum legal intervention requirements under EU law [<xref rid="RSTB20180277C46" ref-type="bibr">46</xref>].</p><p>RL, coupled with epidemiological models, presents an exciting new avenue to develop optimal control policies. Rather than replacing human decision-makers, we propose applications that augment human decision-making by either using a computer-readable policy to develop practical policy heuristics or directly generating a human-readable policy. Thus, RL has the potential to provide well-supported yet tractable state-dependent policy summaries to facilitate decision-making in times of crisis.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><caption><title>Additional Methods</title></caption><media mimetype="application" mime-subtype="docx" xlink:href="rstb20180277supp1.docx" orientation="portrait" id="d35e1129" position="anchor"/></supplementary-material></sec></body><back><ack><title>Acknowledgements</title><p>For technical computing support, W.J.M.P. thanks Wayne Figurelle and the aci-b team at Penn State University, and Paul Brown at Warwick University. Any use of trade, firm or product names is for descriptive purposes only and does not imply endorsement by the US Government.</p></ack><sec id="s5"><title>Data accessibility</title><p>The datasets and code for case study 2 (<uri xlink:href="https://github.com/p-robot/context_matters">https://github.com/p-robot/context_matters</uri>) supporting this article have been uploaded as part of the electronic supplementary material.</p></sec><sec id="s6"><title>Authors' contributions</title><p>All authors contributed to the design of the study. W.J.M.P., S.L. and C.J.F. developed simulation code and analyses. W.J.M.P., S.L. and M.J.F. prepared figures and wrote the first draft of the manuscript. All authors revised the manuscript.</p></sec><sec id="s7" sec-type="COI-statement"><title>Competing interests</title><p>We declare we have no competing interests.</p></sec><sec id="s8"><title>Funding</title><p>This work was supported by a grant from the Biotechnology and Biological Sciences Research Council (BB/K010972/4; <uri xlink:href="www.bbsrc.ukri.org">www.bbsrc.ukri.org</uri>) and from the Ecology and Evolution of Infectious Disease program of the National Science Foundation (<uri xlink:href="www.nsf.gov">www.nsf.gov</uri>) and the National Institutes of Health (1 R01 GM105247-01; <uri xlink:href="www.nih.gov">www.nih.gov</uri>). M.J.T. and M.J.F. received funding by the Research and Policy for Infectious Disease Dynamics (RAPIDD) program of the Science and Technology Directorate of the Department of Homeland Security.</p></sec><ref-list><title>References</title><ref id="RSTB20180277C1"><label>1</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ong</surname><given-names>JBS</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>MI-C</given-names></name>, <name name-style="western"><surname>Cook</surname><given-names>AR</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>HC</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>VJ</given-names></name>, <name name-style="western"><surname>Lin</surname><given-names>RTP</given-names></name>, <name name-style="western"><surname>Tambyah</surname><given-names>PA</given-names></name>, <name name-style="western"><surname>Goh</surname><given-names>LG</given-names></name></person-group>
<year>2010</year>
<article-title>Real-time epidemic monitoring and forecasting of H1N1&#8211;2009 using influenza-like illness from general practice and family doctor clinics in Singapore</article-title>. <source>PLoS ONE</source>
<volume>5</volume>, <fpage>e10036</fpage> (<pub-id pub-id-type="doi">10.1371/journal.pone.0010036</pub-id>)<pub-id pub-id-type="pmid">20418945</pub-id></mixed-citation></ref><ref id="RSTB20180277C2"><label>2</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Reis</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Shaman</surname><given-names>J</given-names></name></person-group>
<year>2016</year>
<article-title>Retrospective parameter estimation and forecast of respiratory syncytial virus in the United States</article-title>. <source>PLoS Comput. Biol.</source>
<volume>12</volume>, <fpage>e1005133</fpage> (<pub-id pub-id-type="doi">10.1371/journal.pcbi.1005133</pub-id>)<pub-id pub-id-type="pmid">27716828</pub-id></mixed-citation></ref><ref id="RSTB20180277C3"><label>3</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shaman</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Karspeck</surname><given-names>A</given-names></name></person-group>
<year>2012</year>
<article-title>Forecasting seasonal outbreaks of influenza</article-title>. <source>Proc. Natl Acad. Sci. USA</source>
<volume>109</volume>, <fpage>20 425</fpage>&#8211;<lpage>20 430</lpage>. (<pub-id pub-id-type="doi">10.1073/pnas.1208772109</pub-id>)</mixed-citation></ref><ref id="RSTB20180277C4"><label>4</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Drake</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Kaul</surname><given-names>RB</given-names></name>, <name name-style="western"><surname>Alexander</surname><given-names>LW</given-names></name>, <name name-style="western"><surname>O'Regan</surname><given-names>SM</given-names></name>, <name name-style="western"><surname>Kramer</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Pulliam</surname><given-names>JT</given-names></name>, <name name-style="western"><surname>Ferrari</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Park</surname><given-names>AW</given-names></name></person-group>
<year>2015</year>
<article-title>Ebola cases and health system demand in Liberia</article-title>. <source>PLoS Biol.</source>
<volume>13</volume>, <fpage>e1002056</fpage> (<pub-id pub-id-type="doi">10.1371/journal.pbio.1002056</pub-id>)<pub-id pub-id-type="pmid">25585384</pub-id></mixed-citation></ref><ref id="RSTB20180277C5"><label>5</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Keeling</surname><given-names>MJ</given-names></name><etal>et al.</etal></person-group>
<year>2001</year>
<article-title>Dynamics of the 2001 UK foot and mouth epidemic: stochastic dispersal in a heterogeneous landscape</article-title>. <source>Science</source>
<volume>294</volume>, <fpage>813</fpage>&#8211;<lpage>817</lpage>. (<pub-id pub-id-type="doi">10.1126/science.1065973</pub-id>)<pub-id pub-id-type="pmid">11679661</pub-id></mixed-citation></ref><ref id="RSTB20180277C6"><label>6</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ferguson</surname><given-names>NM</given-names></name>, <name name-style="western"><surname>Donnelly</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Anderson</surname><given-names>RM</given-names></name></person-group>
<year>2001</year>
<article-title>The foot-and-mouth epidemic in Great Britain: pattern of spread and impact of interventions</article-title>. <source>Science</source>
<volume>292</volume>, <fpage>1155</fpage>&#8211;<lpage>1160</lpage>. (<pub-id pub-id-type="doi">10.1126/science.1061020</pub-id>)<pub-id pub-id-type="pmid">11303090</pub-id></mixed-citation></ref><ref id="RSTB20180277C7"><label>7</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Burke</surname><given-names>DS</given-names></name>, <name name-style="western"><surname>Epstein</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Cummings</surname><given-names>DAT</given-names></name>, <name name-style="western"><surname>Parker</surname><given-names>JI</given-names></name>, <name name-style="western"><surname>Cline</surname><given-names>KC</given-names></name>, <name name-style="western"><surname>Singa</surname><given-names>RM</given-names></name>, <name name-style="western"><surname>Chakravarty</surname><given-names>S</given-names></name></person-group>
<year>2006</year>
<article-title>Individual-based computational modeling of smallpox epidemic control strategies</article-title>. <source>Acad. Emerg. Med.</source>
<volume>13</volume>, <fpage>1142</fpage>&#8211;<lpage>1149</lpage>. (<pub-id pub-id-type="doi">10.1197/j.aem.2006.07.017</pub-id>)<pub-id pub-id-type="pmid">17085740</pub-id></mixed-citation></ref><ref id="RSTB20180277C8"><label>8</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tildesley</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Savill</surname><given-names>NJ</given-names></name>, <name name-style="western"><surname>Shaw</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Deardon</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Brooks</surname><given-names>SP</given-names></name>, <name name-style="western"><surname>Woolhouse</surname><given-names>MEJ</given-names></name>, <name name-style="western"><surname>Grenfell</surname><given-names>BT</given-names></name>, <name name-style="western"><surname>Keeling</surname><given-names>MJ</given-names></name></person-group>
<year>2006</year>
<article-title>Optimal reactive vaccination strategies for a foot-and-mouth outbreak in the UK</article-title>. <source>Nature</source>
<volume>440</volume>, <fpage>83</fpage>&#8211;<lpage>86</lpage>. (<pub-id pub-id-type="doi">10.1038/nature04324</pub-id>)<pub-id pub-id-type="pmid">16511494</pub-id></mixed-citation></ref><ref id="RSTB20180277C9"><label>9</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Medlock</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Galvani</surname><given-names>AP</given-names></name></person-group>
<year>2009</year>
<article-title>Optimizing influenza vaccine distribution</article-title>. <source>Science</source>
<volume>325</volume>, <fpage>1705</fpage>&#8211;<lpage>1708</lpage>. (<pub-id pub-id-type="doi">10.1126/science.1175570</pub-id>)<pub-id pub-id-type="pmid">19696313</pub-id></mixed-citation></ref><ref id="RSTB20180277C10"><label>10</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bradbury</surname><given-names>NV</given-names></name>, <name name-style="western"><surname>Probert</surname><given-names>WJM</given-names></name>, <name name-style="western"><surname>Shea</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Runge</surname><given-names>MC</given-names></name>, <name name-style="western"><surname>Fonnesbeck</surname><given-names>CJ</given-names></name>, <name name-style="western"><surname>Keeling</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Ferrari</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Tildesley</surname><given-names>MJ</given-names></name></person-group>
<year>2017</year>
<article-title>Quantifying the value of perfect information in emergency vaccination campaigns</article-title>. <source>PLoS Comput. Biol.</source>
<volume>13</volume>, <fpage>e1005318</fpage> (<pub-id pub-id-type="doi">10.1371/journal.pcbi.1005318</pub-id>)<pub-id pub-id-type="pmid">28207777</pub-id></mixed-citation></ref><ref id="RSTB20180277C11"><label>11</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ferguson</surname><given-names>NM</given-names></name>, <name name-style="western"><surname>Cummings</surname><given-names>DAT</given-names></name>, <name name-style="western"><surname>Cauchemez</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Fraser</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Riley</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Meeyai</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Iamsirithaworn</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Burke</surname><given-names>DS</given-names></name></person-group>
<year>2005</year>
<article-title>Strategies for containing an emerging influenza pandemic in Southeast Asia</article-title>. <source>Nature</source>
<volume>437</volume>, <fpage>209</fpage>&#8211;<lpage>214</lpage>. (<pub-id pub-id-type="doi">10.1038/nature04017</pub-id>)<pub-id pub-id-type="pmid">16079797</pub-id></mixed-citation></ref><ref id="RSTB20180277C12"><label>12</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Magori</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Legros</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Puente</surname><given-names>ME</given-names></name>, <name name-style="western"><surname>Focks</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Scott</surname><given-names>TW</given-names></name>, <name name-style="western"><surname>Lloyd</surname><given-names>AL</given-names></name>, <name name-style="western"><surname>Gould</surname><given-names>F</given-names></name></person-group>
<year>2009</year>
<article-title>Skeeter Buster: a stochastic, spatially explicit modeling tool for studying <italic>Aedes aegypti</italic> population replacement and population suppression strategies</article-title>. <source>PLoS Negl. Trop. Dis.</source>
<volume>3</volume>, <fpage>e508</fpage> (<pub-id pub-id-type="doi">10.1371/journal.pntd.0000508</pub-id>)<pub-id pub-id-type="pmid">19721700</pub-id></mixed-citation></ref><ref id="RSTB20180277C13"><label>13</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Roche</surname><given-names>SE</given-names></name><etal>et al</etal></person-group>
<year>2015</year>
<article-title>Evaluating vaccination strategies to control foot-and-mouth disease: a model comparison study</article-title>. <source>Epidemiol. Infect.</source>
<volume>143</volume>, <fpage>1256</fpage>&#8211;<lpage>1275</lpage>. (<pub-id pub-id-type="doi">10.1017/S0950268814001927</pub-id>)<pub-id pub-id-type="pmid">25078780</pub-id></mixed-citation></ref><ref id="RSTB20180277C14"><label>14</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Bellman</surname><given-names>R</given-names></name></person-group>
<year>1957</year>
<source>Dynamic programming</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Princeton University Press</publisher-name>.</mixed-citation></ref><ref id="RSTB20180277C15"><label>15</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Puterman</surname><given-names>ML</given-names></name></person-group>
<year>2014</year>
<source>Markov decision processes: discrete stochastic dynamic programming</source>. <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley &amp; Sons</publisher-name>.</mixed-citation></ref><ref id="RSTB20180277C16"><label>16</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tesauro</surname><given-names>G</given-names></name></person-group>
<year>1995</year>
<article-title>Temporal difference learning and TD-Gammon</article-title>. <source>Commun. ACM</source>
<volume>38</volume>, <fpage>58</fpage>&#8211;<lpage>68</lpage>. (<pub-id pub-id-type="doi">10.1145/203330.203343</pub-id>)</mixed-citation></ref><ref id="RSTB20180277C17"><label>17</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Stone</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Sutton</surname><given-names>RS.</given-names></name></person-group>
<year>2001</year>
<article-title>Scaling reinforcement learning toward RoboCup soccer</article-title>. In <source>Proc. 18th Int. Conf. Machine Learning (ICML-2001)</source>, pp. <fpage>537</fpage>&#8211;<lpage>544</lpage>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Morgan Kaufmann</publisher-name>.</mixed-citation></ref><ref id="RSTB20180277C18"><label>18</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Simonsen</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Gog</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Olson</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Viboud</surname><given-names>C</given-names></name></person-group>
<year>2016</year>
<article-title>Infectious disease surveillance in the big data era: towards faster and locally relevant systems</article-title>. <source>J. Infect. Dis.</source>
<volume>214</volume>, <fpage>S380</fpage>&#8211;<lpage>S385</lpage>. (<pub-id pub-id-type="doi">10.1093/infdis/jiw376</pub-id>)<pub-id pub-id-type="pmid">28830112</pub-id></mixed-citation></ref><ref id="RSTB20180277C19"><label>19</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Leung</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Tran</surname><given-names>LT</given-names></name></person-group>
<year>2000</year>
<article-title>Predicting shrimp disease occurrence: artificial neural networks vs. logistic regression</article-title>. <source>Aquaculture</source>
<volume>187</volume>, <fpage>35</fpage>&#8211;<lpage>49</lpage>. (<pub-id pub-id-type="doi">10.1016/S0044-8486(00)00300-8</pub-id>)</mixed-citation></ref><ref id="RSTB20180277C20"><label>20</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Eames</surname><given-names>KTD</given-names></name>, <name name-style="western"><surname>Keeling</surname><given-names>MJ</given-names></name></person-group>
<year>2003</year>
<article-title>Contact tracing and disease control</article-title>. <source>Proc. R. Soc. Lond. B</source>
<volume>270</volume>, <fpage>2565</fpage>&#8211;<lpage>2571</lpage>. (<pub-id pub-id-type="doi">10.1098/rspb.2003.2554</pub-id>)</mixed-citation></ref><ref id="RSTB20180277C21"><label>21</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fraser</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Riley</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Anderson</surname><given-names>RM</given-names></name>, <name name-style="western"><surname>Ferguson</surname><given-names>NM</given-names></name></person-group>
<year>2004</year>
<article-title>Factors that make an infectious disease outbreak controllable</article-title>. <source>Proc. Natl Acad. Sci. USA</source>
<volume>101</volume>, <fpage>6146</fpage>&#8211;<lpage>6151</lpage>. (<pub-id pub-id-type="doi">10.1073/pnas.0307506101</pub-id>)<pub-id pub-id-type="pmid">15071187</pub-id></mixed-citation></ref><ref id="RSTB20180277C22"><label>22</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kiss</surname><given-names>IZ</given-names></name>, <name name-style="western"><surname>Green</surname><given-names>DM</given-names></name>, <name name-style="western"><surname>Kao</surname><given-names>RR</given-names></name></person-group>
<year>2006</year>
<article-title>Infectious disease control using contact tracing in random and scale-free networks</article-title>. <source>J. R. Soc. Interface</source>
<volume>3</volume>, <fpage>55</fpage>&#8211;<lpage>62</lpage>. (<pub-id pub-id-type="doi">10.1098/rsif.2005.0079</pub-id>)<pub-id pub-id-type="pmid">16849217</pub-id></mixed-citation></ref><ref id="RSTB20180277C23"><label>23</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ludkovski</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Niemi</surname><given-names>J</given-names></name></person-group>
<year>2010</year>
<article-title>Optimal dynamic policies for influenza management</article-title>. <source>Stat. Commun. Infect. Dis.</source>
<volume>2</volume>, <fpage>5</fpage> (<pub-id pub-id-type="doi">10.2202/1948-4690.1020</pub-id>)</mixed-citation></ref><ref id="RSTB20180277C24"><label>24</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>J-M</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>J-B</given-names></name></person-group>
<year>2016</year>
<article-title>Eradication of Ebola based on dynamic programming</article-title>. <source>Comput. Math. Methods Med.</source>
<volume>2016</volume>, <fpage>1</fpage>&#8211;<lpage>9</lpage>. (<pub-id pub-id-type="doi">10.1155/2016/1580917</pub-id>)</mixed-citation></ref><ref id="RSTB20180277C25"><label>25</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Patel</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Longini</surname><given-names>IM</given-names></name>, <name name-style="western"><surname>Halloran</surname><given-names>ME</given-names></name></person-group>
<year>2005</year>
<article-title>Finding optimal vaccination strategies for pandemic influenza using genetic algorithms</article-title>. <source>J. Theor. Biol.</source>
<volume>234</volume>, <fpage>201</fpage>&#8211;<lpage>212</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jtbi.2004.11.032</pub-id>)<pub-id pub-id-type="pmid">15757679</pub-id></mixed-citation></ref><ref id="RSTB20180277C26"><label>26</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Sutton</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Barto</surname><given-names>A</given-names></name></person-group>
<year>1998</year>
<source>Reinforcement learning: an introduction</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref><ref id="RSTB20180277C27"><label>27</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mnih</surname><given-names>V</given-names></name><etal>et al</etal></person-group>
<year>2015</year>
<article-title>Human-level control through deep reinforcement learning</article-title>. <source>Nature</source>
<volume>518</volume>, <fpage>529</fpage>&#8211;<lpage>533</lpage>. (<pub-id pub-id-type="doi">10.1038/nature14236</pub-id>)<pub-id pub-id-type="pmid">25719670</pub-id></mixed-citation></ref><ref id="RSTB20180277C28"><label>28</label><element-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Anschel</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Baram</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Shimkin</surname><given-names>N.</given-names></name></person-group>
<year>2017</year>
<comment>Averaged-DQN: variance reduction and stabilization for deep reinforcement learning. In <italic>Proc. 34th Int. Conf. Machine Learning, 6&#8211;11 August, Sydney, NSW</italic>, vol. 70, pp. 176&#8211;185.</comment></element-citation></ref><ref id="RSTB20180277C29"><label>29</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Silver</surname><given-names>D</given-names></name><etal>et al</etal></person-group>
<year>2016</year>
<article-title>Mastering the game of Go with deep neural networks and tree search</article-title>. <source>Nature</source>
<volume>529</volume>, <fpage>484</fpage>&#8211;<lpage>489</lpage>. (<pub-id pub-id-type="doi">10.1038/nature16961</pub-id>)<pub-id pub-id-type="pmid">26819042</pub-id></mixed-citation></ref><ref id="RSTB20180277C30"><label>30</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>L-J.</given-names></name></person-group>
<year>1992</year>
<article-title>Reinforcement learning for robots using neural networks</article-title>. <comment>PhD Thesis</comment>, <publisher-name>Carnegie Mellon University</publisher-name>, <publisher-loc>Pittsburgh, PA, USA</publisher-loc></mixed-citation></ref><ref id="RSTB20180277C31"><label>31</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Anderson</surname><given-names>I. (chair)</given-names></name></person-group>
<year>2002</year>
<comment><italic>Foot and mouth disease 2001: lessons to be learned inquiry report</italic>. London, UK: Stationery Office</comment>.</mixed-citation></ref><ref id="RSTB20180277C32"><label>32</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Knight-Jones</surname><given-names>TJD</given-names></name>, <name name-style="western"><surname>Rushton</surname><given-names>J</given-names></name></person-group>
<year>2013</year>
<article-title>The economic impacts of foot and mouth disease&#8212;what are they, how big are they and where do they occur?</article-title>
<source>Prev. Vet. Med.</source>
<volume>112</volume>, <fpage>161</fpage>&#8211;<lpage>173</lpage>. (<pub-id pub-id-type="doi">10.1016/j.prevetmed.2013.07.013</pub-id>)<pub-id pub-id-type="pmid">23958457</pub-id></mixed-citation></ref><ref id="RSTB20180277C33"><label>33</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tildesley</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Deardon</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Savill</surname><given-names>NJ</given-names></name>, <name name-style="western"><surname>Bessell</surname><given-names>PR</given-names></name>, <name name-style="western"><surname>Brooks</surname><given-names>SP</given-names></name>, <name name-style="western"><surname>Woolhouse</surname><given-names>MEJ</given-names></name>, <name name-style="western"><surname>Grenfell</surname><given-names>BT</given-names></name>, <name name-style="western"><surname>Keeling</surname><given-names>MJ</given-names></name></person-group>
<year>2008</year>
<article-title>Accuracy of models for the 2001 foot-and-mouth epidemic</article-title>. <source>Proc. R. Soc. Lond. B</source>
<volume>275</volume>, <fpage>1459</fpage>&#8211;<lpage>1468</lpage>. (<pub-id pub-id-type="doi">10.1098/rspb.2008.0006</pub-id>)</mixed-citation></ref><ref id="RSTB20180277C34"><label>34</label><element-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Ong</surname><given-names>HY</given-names></name>, <name name-style="western"><surname>Chavez</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Hong</surname><given-names>A.</given-names></name></person-group>
<year>2015</year>
<comment>Distributed deep Q-learning. <italic>arXiv</italic> 1508.04184. <uri xlink:href="https://arxiv.org/abs/1508.04186">https://arxiv.org/abs/1508.04186</uri>.</comment></element-citation></ref><ref id="RSTB20180277C35"><label>35</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Probert</surname><given-names>WJM</given-names></name><etal>et al</etal></person-group>
<year>2016</year>
<article-title>Decision-making for foot-and-mouth disease control: objectives matter</article-title>. <source>Epidemics</source>
<volume>15</volume>, <fpage>10</fpage>&#8211;<lpage>19</lpage>. (<pub-id pub-id-type="doi">10.1016/j.epidem.2015.11.002</pub-id>)<pub-id pub-id-type="pmid">27266845</pub-id></mixed-citation></ref><ref id="RSTB20180277C36"><label>36</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Probert</surname><given-names>WJM</given-names></name><etal>et al</etal></person-group>
<year>2018</year>
<article-title>Real-time decision-making during emergency disease outbreaks</article-title>. <source>PLoS Comput. Biol.</source>
<volume>14</volume>, <fpage>e1006202</fpage> (<pub-id pub-id-type="doi">10.1371/journal.pcbi.1006202</pub-id>)<pub-id pub-id-type="pmid">30040815</pub-id></mixed-citation></ref><ref id="RSTB20180277C37"><label>37</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>de Klerk</surname><given-names>PF.</given-names></name></person-group>
<year>2002</year>
<article-title>Carcass disposal: lessons from The Netherlands after the foot and mouth disease outbreak of 2001</article-title>. <source>Rev. Off. Int. Epizoot.</source>
<volume>21</volume>, <fpage>789</fpage>&#8211;<lpage>796</lpage>. (<pub-id pub-id-type="doi">10.20506/rst.21.3.1376</pub-id>)<pub-id pub-id-type="pmid">12523715</pub-id></mixed-citation></ref><ref id="RSTB20180277C38"><label>38</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sanson</surname><given-names>RL</given-names></name>, <name name-style="western"><surname>Dub&#233;</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Cork</surname><given-names>SC</given-names></name>, <name name-style="western"><surname>Frederickson</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Morley</surname><given-names>C</given-names></name></person-group>
<year>2014</year>
<article-title>Simulation modelling of a hypothetical introduction of foot-and-mouth disease into Alberta</article-title>. <source>Prev. Vet. Med.</source>
<volume>114</volume>, <fpage>151</fpage>&#8211;<lpage>163</lpage>. (<pub-id pub-id-type="doi">10.1016/j.prevetmed.2014.03.005</pub-id>)<pub-id pub-id-type="pmid">24679716</pub-id></mixed-citation></ref><ref id="RSTB20180277C39"><label>39</label><element-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Samek</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Wiegand</surname><given-names>T</given-names></name>, <name name-style="western"><surname>M&#252;ller</surname><given-names>K-R.</given-names></name></person-group>
<year>2017</year>
<comment>Explainable artificial intelligence: understanding, visualizing and interpreting deep learning models. <italic>arXiv</italic> 1708.0829. <uri xlink:href="https://arxiv.org/abs/1708.08296">https://arxiv.org/abs/1708.08296</uri>.</comment></element-citation></ref><ref id="RSTB20180277C40"><label>40</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tildesley</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>House</surname><given-names>TA</given-names></name>, <name name-style="western"><surname>Bruhn</surname><given-names>MC</given-names></name>, <name name-style="western"><surname>Curry</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>O'Neil</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Allpress</surname><given-names>JLE</given-names></name>, <name name-style="western"><surname>Smith</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Keeling</surname><given-names>MJ</given-names></name></person-group>
<year>2010</year>
<article-title>Impact of spatial clustering on disease transmission and optimal control</article-title>. <source>Proc. Natl Acad. Sci. USA</source>
<volume>107</volume>, <fpage>1041</fpage>&#8211;<lpage>1046</lpage>. (<pub-id pub-id-type="doi">10.1073/pnas.0909047107</pub-id>)<pub-id pub-id-type="pmid">19955428</pub-id></mixed-citation></ref><ref id="RSTB20180277C41"><label>41</label><mixed-citation publication-type="other"><collab>UK Government.</collab>
<year>2018</year>
<comment>Contingency plan for exotic notifiable diseases of animals in England 2018. <uri xlink:href="https://www.gov.uk/government/publications/contingency-plan-for-exotic-notifiable-diseases-of-animals-in-england">https://www.gov.uk/government/publications/contingency-plan-for-exotic-notifiable-diseases-of-animals-in-england</uri></comment>.</mixed-citation></ref><ref id="RSTB20180277C42"><label>42</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Apfelbaum</surname><given-names>JL</given-names></name><etal>et al</etal></person-group>
<year>2013</year>
<article-title>Practice guidelines for management of the difficult airway: an updated report by the American Society of Anesthesiologists Task Force on Management of the Difficult Airway</article-title>. <source>Anesthesiology</source>
<volume>118</volume>, <fpage>251</fpage>&#8211;<lpage>270</lpage>. (<pub-id pub-id-type="doi">10.1097/ALN.0b013e31827773b2</pub-id>)<pub-id pub-id-type="pmid">23364566</pub-id></mixed-citation></ref><ref id="RSTB20180277C43"><label>43</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Teach</surname><given-names>RL</given-names></name>, <name name-style="western"><surname>Shortliffe</surname><given-names>EH</given-names></name></person-group>
<year>1981</year>
<article-title>An analysis of physician attitudes regarding computer-based clinical consultation systems</article-title>. <source>Comput. Biomed. Res.</source>
<volume>14</volume>, <fpage>542</fpage>&#8211;<lpage>558</lpage>. (<pub-id pub-id-type="doi">10.1016/0010-4809(81)90012-4</pub-id>)<pub-id pub-id-type="pmid">7035062</pub-id></mixed-citation></ref><ref id="RSTB20180277C44"><label>44</label><mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Chad&#232;s</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Carwardine</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Martin</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Nicol</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Buffet</surname><given-names>O</given-names></name></person-group>
<year>2012</year>
<article-title>MOMDP: a solution for modelling adaptive management problems</article-title>. In <source>Proc. 26th AAAI Conf. Artificial Intelligence</source>, pp. <fpage>267</fpage>&#8211;<lpage>273</lpage>.</mixed-citation></ref><ref id="RSTB20180277C45"><label>45</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shea</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Tildesley</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Runge</surname><given-names>MC</given-names></name>, <name name-style="western"><surname>Fonnesbeck</surname><given-names>CJ</given-names></name>, <name name-style="western"><surname>Ferrari</surname><given-names>MJ</given-names></name></person-group>
<year>2014</year>
<article-title>Adaptive management and the value of information: learning via intervention in epidemiology</article-title>. <source>PLoS Biol.</source>
<volume>12</volume>, <fpage>e1001970</fpage> (<pub-id pub-id-type="doi">10.1371/journal.pbio.1001970</pub-id>)<pub-id pub-id-type="pmid">25333371</pub-id></mixed-citation></ref><ref id="RSTB20180277C46"><label>46</label><mixed-citation publication-type="other"><collab>European Union</collab>. <year>2003</year>
<comment>Council Directive 2003/85/EC of 29 September 2003 on Community measures for the control of foot-and-mouth disease repealing Directive 85/511/ EEC and Decisions 89/531/EEC and 91/665/EEC and amending Directive 92/46/EEC. <uri xlink:href="https://eur-lex.europa.eu/legal-content/en/ALL/?uri=CELEX:32003L0085">https://eur-lex.europa.eu/legal-content/en/ALL/?uri=CELEX:32003L0085</uri></comment>.</mixed-citation></ref></ref-list></back></article>