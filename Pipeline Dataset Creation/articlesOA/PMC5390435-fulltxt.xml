<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Med Inform Decis Mak</journal-id><journal-id journal-id-type="iso-abbrev">BMC Med Inform Decis Mak</journal-id><journal-title-group><journal-title>BMC Medical Informatics and Decision Making</journal-title></journal-title-group><issn pub-type="epub">1472-6947</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">5390435</article-id><article-id pub-id-type="publisher-id">442</article-id><article-id pub-id-type="doi">10.1186/s12911-017-0442-4</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Development and validation of classifiers and variable subsets for predicting nursing home admission</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7429-3710</contrib-id><name><surname>Nuutinen</surname><given-names>Mikko</given-names></name><address><email>mikko.nuutinen@nhg.fi</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Leskel&#x000e4;</surname><given-names>Riikka-Leena</given-names></name><address><email>riikka-leena.leskela@nhg.fi</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Suojalehto</surname><given-names>Ella</given-names></name><address><email>ella.suojalehto@tampere.fi</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Tirronen</surname><given-names>Anniina</given-names></name><address><email>anniina.tirronen@tampere.fi</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Komssi</surname><given-names>Vesa</given-names></name><address><email>vesa.komssi@nhg.fi</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label>Nordic Healthcare Group, Vattuniemenranta 2, Helsinki, 00210 Finland </aff><aff id="Aff2"><label>2</label>City of Tampere, PL 487, Tampere, 33101 Finland </aff></contrib-group><pub-date pub-type="epub"><day>13</day><month>4</month><year>2017</year></pub-date><pub-date pub-type="pmc-release"><day>13</day><month>4</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>17</volume><elocation-id>39</elocation-id><history><date date-type="received"><day>23</day><month>11</month><year>2016</year></date><date date-type="accepted"><day>7</day><month>4</month><year>2017</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2017</copyright-statement><license license-type="OpenAccess"><license-p>
<bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><abstract id="Abs1"><sec><title>Background</title><p>In previous years a substantial number of studies have identified statistically important predictors of nursing home admission (NHA). However, as far as we know, the analyses have been done at the population-level. No prior research has analysed the prediction accuracy of a NHA model for individuals.</p></sec><sec><title>Methods</title><p>This study is an analysis of 3056 longer-term home care customers in the city of Tampere, Finland. Data were collected from the records of social and health service usage and RAI-HC (Resident Assessment Instrument - Home Care) assessment system during January 2011 and September 2015. The aim was to find out the most efficient variable subsets to predict NHA for individuals and validate the accuracy. The variable subsets of predicting NHA were searched by sequential forward selection (SFS) method, a variable ranking metric and the classifiers of logistic regression (LR), support vector machine (SVM) and Gaussian naive Bayes (GNB). The validation of the results was guaranteed using randomly balanced data sets and cross-validation. The primary performance metrics for the classifiers were the prediction accuracy and AUC (average area under the curve).</p></sec><sec><title>Results</title><p>The LR and GNB classifiers achieved 78% accuracy for predicting NHA. The most important variables were RAI MAPLE (Method for Assigning Priority Levels), functional impairment (RAI IADL, Activities of Daily Living), cognitive impairment (RAI CPS, Cognitive Performance Scale), memory disorders (diagnoses G30-G32 and F00-F03) and the use of community-based health-service and prior hospital use (emergency visits and periods of care).</p></sec><sec><title>Conclusion</title><p>The accuracy of the classifier for individuals was high enough to convince the officials of the city of Tampere to integrate the predictive model based on the findings of this study as a part of home care information system. Further work need to be done to evaluate variables that are modifiable and responsive to interventions.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Nursing home admission</kwd><kwd>Classifier</kwd><kwd>Classification accuracy</kwd><kwd>Variable selection</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2017</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background</title><p>It is a common goal for elderly care services to support and enable living at home as long as possible. Most people would rather live at home in a familiar environment, than move to a nursing home or an assisted living facility. Also, from the view point of the service system, 24 h services are expensive. Thus, supporting functional and cognitive capabilities, which enable living at home, improves the quality of life and is cost saving for the payer. However, interventions aimed at improving or sustaining functional and cognitive capabilities can be expensive. Therefore, the need to be targeted to those individuals, who are at risk of needing 24 h service in the near future, but who are still capable of benefiting from the intervention. Furthermore, the resource planning of 24 h services benefits from the information of upcoming admissions. Identification of reliable predictors and creation of tools that calculate risk for individuals provide an answer to these problems.</p><p>Much research has focused on identifying predictors of nursing home admission (NHA) [<xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR14">14</xref>]. The studies vary according to variables, populations (e.g. with dementia [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR8">8</xref>], without dementia [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR11">11</xref>]), geographical locations (e.g. German [<xref ref-type="bibr" rid="CR1">1</xref>], Singapore [<xref ref-type="bibr" rid="CR12">12</xref>], Norway [<xref ref-type="bibr" rid="CR15">15</xref>]) and sample sizes (e.g. <italic>n</italic> = 210 [<xref ref-type="bibr" rid="CR1">1</xref>] or <italic>n</italic> = 7000 [<xref ref-type="bibr" rid="CR9">9</xref>]) used in the determination of predictors of NHA. The commonly recognized risk factors include advanced age, functional and cognitive impairments, depression, caregiver burden, use of health services, prior hospitalization or nursing home use and dementia. In a literature review [<xref ref-type="bibr" rid="CR5">5</xref>] and meta-analysis [<xref ref-type="bibr" rid="CR16">16</xref>], the strongest predictors of NHA were increased age, low self-rated health status, functional and cognitive impairment, dementia and prior NHA.</p><p>The above research has focused on finding risk factors for NHA. However, as far as we know, no prior research has proved the prediction performance or accuracy of a NHA model for individuals. The prior research articulates the statistically important variables that increase or decrease the risk of NHA at the population level. It is based on the traditional statistical data processing approach in which statistical modeling connects data to a population of interest. It does not answer the question of how accurately the nursing home admission is possible to predict for individuals.</p><p>In this study, we point out the most important variable subsets of different sizes for predicting NHA. Particularly, we measure and validate the performance of our NHA prediction models in terms of classification accuracy. That is, we search the best model and measure how good it is for individuals. The variable subsets were searched by machine learning (ML) methods and the classification accuracy was calculated using the cross-validation principle. The data was consisted of the service records of home care clients in the city of Tampere<sup>1</sup>, Finland. Because our data set is highly unbalanced, we use a random operator to form balanced data sets and the all performance results are reported on those balanced data sets instead of the original unbalanced set.</p><p>We claim, that the knowledge of classification accuracy is highly valuable, when deciding on the adoption of the prediction model in actual service production. It should be noted, that statistically significant variables do not guarantee high classification accuracy. Without adequate accuracy, the cost effectiveness of the targeted interventions is not good enough: interventions are targeted to a significant number of people not at risk (&#x0201c;false positive&#x0201d;), and some of those in need of an intervention do not receive one (&#x0201c;false negatives&#x0201d;). Furthermore, the resource planning of service production benefits from the individual predictions of upcoming admissions. The primary contributions of this paper are summarized below: 
<list list-type="bullet"><list-item><p>As far as we know, no prior research has investigated variable subsets of different sizes for predicting NHA. A few scholars have applied variable selection methods [<xref ref-type="bibr" rid="CR3">3</xref>&#x02013;<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR13">13</xref>]. However, they did not investigate the variable subsets of different sizes (1&#x02212;<italic>n</italic> variables), as we did in this study.</p></list-item><list-item><p>The second contribution relates to the way to use, train and validate classification algorithms for predicting NHA. Compared to prior research work, the present study investigates the NHA prediction models for individuals. Prior research investigated statistical significant population-level risk factors for NHA. The 5% level of significance was a de facto standard for important variables. In this study, we measure classification accuracy for classifiers trained and validated using cross-validation. That is, we study the accuracy of our model for unseen clients of home care according to the risk of NHA.</p></list-item></list>
</p><p>The objective of this study was to gain a better understanding of the accuracy level in which NHA can be predicted in order to support decision making in home care services and allocation of resources between customers. The classification accuracy of our method was 78% that was high enough for the decision to integrate it in the local information system of home caring<sup>2</sup>. The remainder of this paper is divided into three parts. In the first part, we describe how the variables of our prediction models were aggregated and how the variables were selected for the subset selection process. The second part introduces the methods for training and validating classifier algorithms. The third part of the paper presents the performance of the variable selection and discusses the results and practical implications.</p></sec><sec id="Sec2"><title>Methods</title><sec id="Sec3"><title>Data source</title><p>The data consisted of the records of 7259 home care customers between January 2011 and September 2015 in the city of Tampere, Finland. These data were linked to records that contained information regarding all social and health care service usage during the same period. Nursing home admission (model outcome) was indicated by whether the customer admitted to a nursing home or not, and coded as a binary indicator. The data were linked on the customer level using unique encrypted identifiers. We excluded clients with recorded home care episode shorter than 12 months between January 2011 and September 2015 (<italic>n</italic>=3192) and those whose RAI-HC (Resident Assessment Instrument - Home Care [<xref ref-type="bibr" rid="CR17">17</xref>]) values (<italic>n</italic>=981) were missing. In total, we had 3056 customers (539 NHA is &#x0201c;true&#x0201d; and 2544 NHA is &#x0201c;false&#x0201d;) for analysis.</p><p>All the variables were calculated 3&#x02013;12 months before the evaluation day <italic>t</italic>
<sub><italic>ev</italic></sub>. In addition, the variables were calculated 6&#x02013;12 months before the day <italic>t</italic>
<sub><italic>ev</italic></sub> for additional analyses. The main variables are listed in the column &#x0201c;variable&#x0201d; of Table <xref rid="Tab1" ref-type="table">1</xref>. The variables were selected by the experts of elderly care services. Figure <xref rid="Fig1" ref-type="fig">1</xref> shows a time scale in which <italic>t</italic>
<sub><italic>s</italic>,<italic>i</italic></sub> is the starting day and <italic>t</italic>
<sub><italic>e</italic>,<italic>i</italic></sub> the ending day of home care according to the home care service data for customer <italic>i</italic> (<italic>i</italic>=1,&#x02026;,3056). The variables were the numbers of events or boolean value [<italic>t</italic>
<italic>r</italic>
<italic>u</italic>
<italic>e</italic>/<italic>f</italic>
<italic>a</italic>
<italic>l</italic>
<italic>s</italic>
<italic>e</italic>] that an event occurred between times <italic>t</italic>
<sub><italic>k</italic></sub> and <italic>t</italic>
<sub><italic>k</italic>+1</sub> (<italic>k</italic>=1,2,3) or <italic>t</italic>
<sub>1</sub> and <italic>t</italic>
<sub>4</sub>. For example, variable <italic>j</italic> (a blue box in Fig. <xref rid="Fig1" ref-type="fig">1</xref>) for customer <italic>i</italic> was calculated from time period <italic>t</italic>
<sub>2,<italic>i</italic><italic>j</italic></sub>&#x02212;<italic>t</italic>
<sub>3,<italic>i</italic><italic>j</italic></sub>. The interval between times <italic>t</italic>
<sub>1</sub> and <italic>t</italic>
<sub><italic>ev</italic></sub> was set to be 12 months and <italic>t</italic>
<sub>1</sub>&#x0003c;<italic>t</italic>
<sub>2</sub>&#x0003c;<italic>t</italic>
<sub>3</sub>&#x0003c;<italic>t</italic>
<sub>4</sub>&#x0003c;<italic>t</italic>
<sub><italic>ev</italic></sub>. If NHA variable was &#x0201c;true&#x0201d; for the customer <italic>i</italic>, that is the customer <italic>i</italic> was admitted to nursing home at time <italic>t</italic>
<sub><italic>e</italic>,<italic>i</italic></sub>, time <italic>t</italic>
<sub><italic>e</italic><italic>v</italic>,<italic>i</italic></sub> was set to be the admission day (&#x02192; <italic>t</italic>
<sub><italic>e</italic><italic>v</italic>,<italic>i</italic></sub>=<italic>t</italic>
<sub><italic>e</italic>,<italic>i</italic></sub>). If the NHA variable of customer <italic>i</italic> was &#x0201c;false&#x0201d;, then time <italic>t</italic>
<sub><italic>e</italic><italic>v</italic>,<italic>i</italic></sub> was a random day between times <italic>t</italic>
<sub><italic>s</italic>,<italic>i</italic></sub> and <italic>t</italic>
<sub><italic>e</italic>,<italic>i</italic></sub>, st. <italic>t</italic>
<sub><italic>e</italic><italic>v</italic>,<italic>i</italic></sub>&#x0003e;<italic>t</italic>
<sub><italic>s</italic>,<italic>i</italic></sub>+12 months.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Variables were calculated in time periods of <italic>t</italic>
<sub>1</sub> - <italic>t</italic>
<sub>4</sub>, when <italic>t</italic>
<sub><italic>s</italic></sub> - <italic>t</italic>
<sub><italic>e</italic></sub> is the time period of home care for a customer. Figure shows time scale of starting day and ending day of home care for customer i from which the variables of the models were derived</p></caption><graphic xlink:href="12911_2017_442_Fig1_HTML" id="MO1"/></fig>
<table-wrap id="Tab1"><label>Table 1</label><caption><p>The characteristics of the study sample (means / %) with and without a nursing home admission and the results of t-tests of significance difference between the means of continuous values or categorical variables</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Variable</th><th align="left">Time interval / description</th><th align="left">NH admission</th><th align="left">No NH admission</th><th align="left">
<italic>p</italic>-value</th></tr></thead><tbody><tr><td align="justify">Age (mean)</td><td align="left"/><td align="left">84.44</td><td align="left">81.76</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify">Number of Emergency care visits (mean)</td><td align="left">3-6 months</td><td align="left">0.72</td><td align="left">0.35</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">6-9 months</td><td align="left">0.78</td><td align="left">0.38</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">9-12 months</td><td align="left">0.61</td><td align="left">0.37</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify">Number of emergency care visits, change (mean)</td><td align="left">3-6 months vs. 6-9 months</td><td align="left">-0.06</td><td align="left">-0.03</td><td align="left">.6279</td></tr><tr><td align="justify"/><td align="left">6-9 months vs. 9-12 months</td><td align="left">0.18</td><td align="left">0.01</td><td align="left">.0049</td></tr><tr><td align="justify">Number of periods of care (mean)</td><td align="left">3-6 months</td><td align="left">0.98</td><td align="left">0.36</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">6-9 months</td><td align="left">0.86</td><td align="left">0.32</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">9-12 months</td><td align="left">0.53</td><td align="left">0.33</td><td align="left">.0002</td></tr><tr><td align="justify">Number of periods of care, change (mean)</td><td align="left">3-6 months vs. 6-9 months</td><td align="left">0.12</td><td align="left">0.03</td><td align="left">.3121</td></tr><tr><td align="justify"/><td align="left">6-9 months vs. 9-12 months</td><td align="left">0.33</td><td align="left">-0.01</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify">Number of home care visits (mean)</td><td align="left">3-6 months</td><td align="left">131.51</td><td align="left">96.30</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">6-9 months</td><td align="left">142.80</td><td align="left">93.42</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">9-12 months</td><td align="left">130.10</td><td align="left">89.26</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify">Number of home care visits, change (mean)</td><td align="left">3-6 months vs. 6-9 months</td><td align="left">-11.28</td><td align="left">2.88</td><td align="left">.0002</td></tr><tr><td align="justify"/><td align="left">6-9 months vs. 9-12 months</td><td align="left">12.70</td><td align="left">4.16</td><td align="left">.0118</td></tr><tr><td align="justify">Number of outpatient visits in</td><td align="left">3-6 months</td><td align="left">1.89</td><td align="left">1.11</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify">specialised care by appointment (mean)</td><td align="left">6-9 months</td><td align="left">1.77</td><td align="left">1.14</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">9-12 months</td><td align="left">1.58</td><td align="left">1.16</td><td align="left">.0001</td></tr><tr><td align="justify">Number of outpatient visits in specialised</td><td align="left">3-6 months vs. 6-9 months</td><td align="left">0.12</td><td align="left">-0.03</td><td align="left">.1410</td></tr><tr><td align="justify">care by appointment, change (mean)</td><td align="left">6-9 months vs. 9-12 months</td><td align="left">0.19</td><td align="left">-0.02</td><td align="left">.0588</td></tr><tr><td align="justify">Number of physiotherapy visits at home (mean)</td><td align="left">3-12 months</td><td align="left">0.47</td><td align="left">0.49</td><td align="left">.9006</td></tr><tr><td align="justify">Number of outpatient visits in geriatrics (mean)</td><td align="left">3-12 months</td><td align="left">1.56</td><td align="left">.62</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify">Number of physician visits at home (mean)</td><td align="left">3-12 months</td><td align="left">0.57</td><td align="left">0.49</td><td align="left">.1316</td></tr><tr><td align="justify">RAI-HC (mean)<sup>a</sup>
</td><td align="left">CPS</td><td align="left">1.50</td><td align="left">0.92</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">IADL</td><td align="left">13.18</td><td align="left">9.34</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">PAIN</td><td align="left">0.72</td><td align="left">0.77</td><td align="left">.2278</td></tr><tr><td align="justify"/><td align="left">MAPLE</td><td align="left">4.06</td><td align="left">3.14</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify">Customer of support service (%)</td><td align="left">Safety phone</td><td align="left">41%</td><td align="left">25%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">meals-on-wheels</td><td align="left">47%</td><td align="left">28%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">shopping</td><td align="left">38%</td><td align="left">27%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">Cleaning</td><td align="left">10%</td><td align="left">10%</td><td align="left">.9058</td></tr><tr><td align="justify"/><td align="left">transportation</td><td align="left">29%</td><td align="left">15%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">Day center</td><td align="left">30%</td><td align="left">15%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">Support for informal care</td><td align="left">5%</td><td align="left">5%</td><td align="left">.9691</td></tr><tr><td align="justify"/><td align="left">Home rehabilitation</td><td align="left">4%</td><td align="left">3%</td><td align="left">.3683</td></tr><tr><td align="justify">Outpatient visit in specialised care (%)</td><td align="left">Surgery / neurosurgery</td><td align="left">29%</td><td align="left">13%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">Internal medicine</td><td align="left">40%</td><td align="left">22%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">Obstetric</td><td align="left">3%</td><td align="left">2%</td><td align="left">.0367</td></tr><tr><td align="justify"/><td align="left">Neurology</td><td align="left">17%</td><td align="left">9%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">Respiratory medicine</td><td align="left">4%</td><td align="left">4%</td><td align="left">.8502</td></tr><tr><td align="justify"/><td align="left">Ophthalmology</td><td align="left">12%</td><td align="left">7%</td><td align="left">.0004</td></tr><tr><td align="justify"/><td align="left">Phoniatrics</td><td align="left">14%</td><td align="left">5%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">Psychiatry</td><td align="left">14%</td><td align="left">9%</td><td align="left">.0004</td></tr><tr><td align="justify">Period of care in specialised care (%)</td><td align="left">Surgery / neurosurgery</td><td align="left">20%</td><td align="left">8%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">Internal medicine</td><td align="left">30%</td><td align="left">16%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">Obstetric</td><td align="left">0%</td><td align="left">0%</td><td align="left">.5341</td></tr><tr><td align="justify"/><td align="left">Neurology</td><td align="left">10%</td><td align="left">3%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">Respiratory medicine</td><td align="left">2%</td><td align="left">2%</td><td align="left">.5684</td></tr><tr><td align="justify"/><td align="left">Ophthalmology</td><td align="left">1%</td><td align="left">0%</td><td align="left">.0028</td></tr><tr><td align="justify"/><td align="left">Phoniatrics</td><td align="left">1%</td><td align="left">0%</td><td align="left">.1042</td></tr><tr><td align="justify"/><td align="left">Psychiatry</td><td align="left">8%</td><td align="left">2%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">Intensive care unit</td><td align="left">1%</td><td align="left">0%</td><td align="left">.0211</td></tr><tr><td align="justify">Diagnosis (%)</td><td align="left">a00-a09</td><td align="left">4%</td><td align="left">2%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">a30-a49</td><td align="left">4%</td><td align="left">3%</td><td align="left">.1679</td></tr><tr><td align="justify"/><td align="left">e00-e07</td><td align="left">3%</td><td align="left">2%</td><td align="left">0.0723</td></tr><tr><td align="justify"/><td align="left">e10-e14</td><td align="left">9%</td><td align="left">6%</td><td align="left">.0037</td></tr><tr><td align="justify"/><td align="left">e70-e90</td><td align="left">5%</td><td align="left">2%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">f00-f03</td><td align="left">50%</td><td align="left">16%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">f04-f09</td><td align="left">4%</td><td align="left">2%</td><td align="left">.0006</td></tr><tr><td align="justify"/><td align="left">f10-f19</td><td align="left">2%</td><td align="left">1%</td><td align="left">.3515</td></tr><tr><td align="justify"/><td align="left">f20-f29</td><td align="left">2%</td><td align="left">2%</td><td align="left">.7376</td></tr><tr><td align="justify"/><td align="left">f30-f39</td><td align="left">6%</td><td align="left">4%</td><td align="left">.0123</td></tr><tr><td align="justify"/><td align="left">g20-g26</td><td align="left">5%</td><td align="left">2%</td><td align="left">.0001</td></tr><tr><td align="justify"/><td align="left">g30-g32</td><td align="left">32%</td><td align="left">6%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">g40-g47</td><td align="left">3%</td><td align="left">2%</td><td align="left">.0972</td></tr><tr><td align="justify"/><td align="left">i10-i15</td><td align="left">28%</td><td align="left">12%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">i20-i25</td><td align="left">14%</td><td align="left">6%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">i30-i52</td><td align="left">25%</td><td align="left">13%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">i60-i69</td><td align="left">9%</td><td align="left">4%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">i70-i79</td><td align="left">3%</td><td align="left">1%</td><td align="left">.0023</td></tr><tr><td align="justify"/><td align="left">i80-i89</td><td align="left">2%</td><td align="left">1%</td><td align="left">.1083</td></tr><tr><td align="justify"/><td align="left">i95-i99</td><td align="left">5%</td><td align="left">1%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">j09-j18</td><td align="left">6%</td><td align="left">3%</td><td align="left">.0136</td></tr><tr><td align="justify"/><td align="left">j20-j22</td><td align="left">3%</td><td align="left">2%</td><td align="left">.0437</td></tr><tr><td align="justify"/><td align="left">j40-j47</td><td align="left">5%</td><td align="left">3%</td><td align="left">.1991</td></tr><tr><td align="justify"/><td align="left">k55-k63</td><td align="left">5%</td><td align="left">2%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">m05-m14</td><td align="left">2%</td><td align="left">2%</td><td align="left">.7376</td></tr><tr><td align="justify"/><td align="left">m15-m19</td><td align="left">5%</td><td align="left">2%</td><td align="left">.0005</td></tr><tr><td align="justify"/><td align="left">m45-m49</td><td align="left">2%</td><td align="left">1%</td><td align="left">.0008</td></tr><tr><td align="justify"/><td align="left">m50-m54</td><td align="left">4%</td><td align="left">2%</td><td align="left">.0933</td></tr><tr><td align="justify"/><td align="left">m70-m79</td><td align="left">3%</td><td align="left">2%</td><td align="left">.0978</td></tr><tr><td align="justify"/><td align="left">m80-m85</td><td align="left">4%</td><td align="left">2%</td><td align="left">.0008</td></tr><tr><td align="justify"/><td align="left">n10-n16</td><td align="left">4%</td><td align="left">2%</td><td align="left">.0324</td></tr><tr><td align="justify"/><td align="left">n17-n19</td><td align="left">5%</td><td align="left">2%</td><td align="left">.0001</td></tr><tr><td align="justify"/><td align="left">n30-n39</td><td align="left">15%</td><td align="left">5%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">n40-n51</td><td align="left">2%</td><td align="left">1%</td><td align="left">.0813</td></tr><tr><td align="justify"/><td align="left">r00-r09</td><td align="left">2%</td><td align="left">2%</td><td align="left">.9037</td></tr><tr><td align="justify"/><td align="left">r10-r19</td><td align="left">4%</td><td align="left">2%</td><td align="left">.0034</td></tr><tr><td align="justify"/><td align="left">r40-r46</td><td align="left">5%</td><td align="left">2%</td><td align="left">.0012</td></tr><tr><td align="justify"/><td align="left">r50-r69</td><td align="left">10%</td><td align="left">4%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">s00-s09</td><td align="left">8%</td><td align="left">2%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">s30-s39</td><td align="left">2%</td><td align="left">1%</td><td align="left">.0115</td></tr><tr><td align="justify"/><td align="left">s40-s49</td><td align="left">2%</td><td align="left">1%</td><td align="left">.3970</td></tr><tr><td align="justify"/><td align="left">s70-s79</td><td align="left">5%</td><td align="left">2%</td><td align="left">&#x0003c;.0001</td></tr><tr><td align="justify"/><td align="left">s80-s89</td><td align="left">2%</td><td align="left">1%</td><td align="left">.1098</td></tr><tr><td align="justify"/><td align="left">z00-z13</td><td align="left">5%</td><td align="left">3%</td><td align="left">.1123</td></tr></tbody></table><table-wrap-foot><p>
<sup>a</sup>Resident Assessment Instrument for Home Care (RAI-HC). The Cognitive Performance Scale (CPS) uses items on memory and communication skills to create a 7-point scale from 0 (intact) to 6 (very seve re) [<xref ref-type="bibr" rid="CR35">35</xref>]. The Instrumental Activities of Daily Living (IADL) scale [<xref ref-type="bibr" rid="CR36">36</xref>] provides a measure of the customer&#x02019;s self-performance of seven daily tasks: meal preparation, ordinary housework, managing finances, managing medications, phone use, shopping and transportation. The scores are from 0 to 21. The Method for Assigning Priority Levels (MAPLe) differentiates customers into five different groups ranging from low to very high risk of health decline [<xref ref-type="bibr" rid="CR34">34</xref>]. Higher risk group indicates a higher risk to be admitted to a long-term care facility</p></table-wrap-foot></table-wrap>
</p><p>Table <xref rid="Tab1" ref-type="table">1</xref> presents general characteristics of the study sample (<italic>n</italic>=3056), of which 539 (17.6%) were admitted to nursing home. The table includes the results of t-test of significance difference for continuous variables and chi-squared test for categorical variables between the groups of home caring customers and nursing home residents.</p></sec><sec id="Sec4"><title>Variable subset selection</title><p>The aim of this study was to find efficient variable subsets <bold>X</bold>
<sub><italic>sub</italic></sub>={<italic>x</italic>
<sub><italic>i</italic></sub>|<italic>i</italic>=1,&#x02026;<italic>n</italic>} from a large variable set <bold>X</bold>={<italic>x</italic>
<sub><italic>j</italic></sub>|<italic>j</italic>=1,&#x02026;<italic>k</italic>} for predicting the NHA when <italic>n</italic> and <italic>k</italic> are the numbers of variables and <italic>n</italic>&#x0003c;<italic>k</italic>. Let <italic>Y</italic> be the binary vector of NHA variable, <italic>F</italic>(&#x000b7;) is a classifier and <italic>Y</italic>=<italic>F</italic>(<bold>X</bold>
<sub><italic>sub</italic></sub>). That is, we predict the state of <italic>Y</italic>
<sub><italic>i</italic></sub> for customer <italic>i</italic> at time <italic>t</italic>
<sub><italic>e</italic><italic>v</italic>,<italic>i</italic></sub> (Fig. <xref rid="Fig1" ref-type="fig">1</xref>), when the variable vector <italic>X</italic>
<sub><italic>s</italic><italic>u</italic><italic>b</italic>,<italic>i</italic></sub> is calculated from time range <italic>t</italic>
<sub>1,<italic>i</italic></sub>&#x02212;<italic>t</italic>
<sub>4,<italic>i</italic></sub> (3&#x02013;12 months before <italic>t</italic>
<sub><italic>e</italic><italic>v</italic>,<italic>i</italic></sub>) or <italic>t</italic>
<sub>1,<italic>i</italic></sub>&#x02212;<italic>t</italic>
<sub>3,<italic>i</italic></sub> (6&#x02013;12 months before <italic>t</italic>
<sub><italic>e</italic><italic>v</italic>,<italic>i</italic></sub>).</p><p>Variable selection is a mature research topic and has been used for many applications [<xref ref-type="bibr" rid="CR18">18</xref>]. In this study we applied sequential forward selection (SFS) method [<xref ref-type="bibr" rid="CR19">19</xref>] for variable subset generation. SFS starts with an empty set and adds one variable at a time from the original set <bold>X</bold> for classifier by maximizing the performance measure. Our primary performance metric was classification accuracy: 
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ acc=\frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} L\left(y_{pred,i}=y_{i}\right)   $$ \end{document}</tex-math><mml:math id="M2"><mml:mtext mathvariant="italic">acc</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">samples</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">samples</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>L</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">pred</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12911_2017_442_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>
</p><p>where <italic>y</italic>
<sub><italic>p</italic><italic>r</italic><italic>e</italic><italic>d</italic>,<italic>i</italic></sub> is the predicted NHA class of the <italic>i</italic>-th sample, <italic>y</italic>
<sub><italic>i</italic></sub> is the corresponding true NHA class, <italic>n</italic>
<sub><italic>samples</italic></sub> is the number of samples and <italic>L</italic>(&#x000b7;) is the indicator function (<italic>L</italic>=1 if <italic>y</italic>
<sub><italic>pred</italic></sub>=<italic>y</italic>; <italic>L</italic>=0 if <italic>y</italic>
<sub><italic>pred</italic></sub>&#x02260;<italic>y</italic>) [<xref ref-type="bibr" rid="CR20">20</xref>]. We additionally calculated the average area under the curve (AUC) and true-positive rate (recall) values for classifiers. AUC values correlated almost perfectly with <italic>acc</italic> values, but we decided to report them, because in some research areas they are more familiar than the <italic>acc</italic> values. Recall of a classifier is calculated by dividing the correctly classified positives (true positives) by the total positive count (true positives + false negatives) [<xref ref-type="bibr" rid="CR21">21</xref>]. That is, recall is the probability that a risk customer is found.</p><p>The strength of the accuracy metric, compared to the other common metrics, is that the accuracy metric is easy to understand. It should be noted, that our data set is highly unbalanced. We use a random operator to form balanced data sets and the performance results are reported on those balanced data sets instead of the original set. Otherwise, the accuracy metric would be biased and not suitable.</p><p>An alternative of SFS would be sequential backward elimination (SBE). SBE starts with <bold>X</bold> and eliminates one variable at a time by maximizing the performance measure. Our selection of SFS instead of the SBE method is justified by the ratio of relevant (<italic>#</italic>
<italic>r</italic>) and all (<italic>k</italic>) variables. According to Liu et al. [<xref ref-type="bibr" rid="CR18">18</xref>], if <italic>#</italic>
<italic>r</italic> is small, then the SFS strategy should be used, and if the number of irrelevant variables (<italic>k</italic>&#x02212;<italic>#</italic>
<italic>r</italic>) is small, then the SBE strategy should be used. According to the pre-tests, the original variable set <bold>X</bold> includes many irrelevant variables (low univariate prediction power); thus, we prefer the SFS strategy.</p><p>Different classifiers have different performance for different data sets. In this study, we evaluated the performance of three classifiers: logistic regression (LR) [<xref ref-type="bibr" rid="CR22">22</xref>], Gaussian naive Bayes (GNB) [<xref ref-type="bibr" rid="CR23">23</xref>] and support vector classifier (SVC) [<xref ref-type="bibr" rid="CR24">24</xref>]. That is, SFS was run three times using the classifiers of LR, GNB or SVC. Figure <xref rid="Fig2" ref-type="fig">2</xref> shows the components of variable subset selection process. The subset generation component (SFS) feeds candidate variable subset <bold>X</bold>
<sub><italic>sub</italic></sub> to subset evaluation component. Evaluation component trains and validates classifier and calculates the accuracy values for the subset <bold>X</bold>
<sub><italic>sub</italic></sub>.
<fig id="Fig2"><label>Fig. 2</label><caption><p>The framework of variable subset selection used in this study. The subset generation component feeds candidate variable subset to subset evaluation component. Evaluation component trains and validates classifier and calculates the accuracy values for the subset</p></caption><graphic xlink:href="12911_2017_442_Fig2_HTML" id="MO2"/></fig>
</p><p>It should be noted, that we use a random operator to form balanced data sets for the analyses. Let <italic>A</italic>={<bold>X</bold>|<italic>Y</italic>=1} and <italic>B</italic>={<bold>X</bold>|<italic>Y</italic>=0} be the data sets. That is, the set <italic>A</italic> contains the data of customers with the values of &#x0201c;true&#x0201d; of NHA variable and the <italic>B</italic> with &#x0201c;false&#x0201d;. Because the set <italic>A</italic> is smaller (<italic>n</italic>=539) than the set <italic>B</italic> (<italic>n</italic>=2517), the balanced data set <italic>C</italic> was formed, st. <italic>C</italic>=<italic>A</italic>&#x0222a;<italic>R</italic>(<italic>B</italic>) where <italic>R</italic> is a random operator for selecting 539 random samples from <italic>B</italic>, thus setting the level of chance at 50%. To be sure that the selection did not bias the results, data set <italic>C</italic> was formed 100 times.</p><p>Furthermore, classifier algorithms were trained and validated using a ten-fold cross validation method. That is, we formed sample <italic>C</italic>
<sub><italic>i</italic></sub> (<italic>i</italic>=1,&#x02026;,100) from the data sets <italic>A</italic> and <italic>B</italic>, and split it into 10 equal-sized parts <italic>P</italic>
<sub><italic>ik</italic></sub> (<italic>P</italic>
<sub><italic>ik</italic></sub>&#x02208;<italic>C</italic>
<sub><italic>i</italic></sub> and <italic>k</italic>=1,&#x02026;10). The classification accuracy value, <italic>a</italic>
<italic>c</italic>
<italic>c</italic>
<sub><italic>ik</italic></sub>, was calculated by Eq. <xref rid="Equ1" ref-type="">1</xref> for the part <italic>P</italic>
<sub><italic>ik</italic></sub> of the data set <italic>C</italic>
<sub><italic>i</italic></sub> when the parameters of the classifier were trained with the other K-1 parts of the data set <italic>C</italic>
<sub><italic>i</italic></sub>. The process was repeated for <italic>k</italic>=1,2,&#x02026;10. The overall classification accuracy, <italic>CA</italic>, for the subset <bold>X</bold>
<sub><italic>s</italic><italic>u</italic><italic>b</italic>,<italic>n</italic></sub> of size n (<italic>n</italic>=1,&#x02026;15) was calculated as 
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ CA_{\textbf{X}_{sub,n}}=\frac{1}{100*K} \sum_{i=1}^{100} \sum_{K=1}^{10} acc_{ik}   $$ \end{document}</tex-math><mml:math id="M4"><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="bold">X</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">sub</mml:mtext><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>100</mml:mn><mml:mo>&#x02217;</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>100</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:munderover><mml:mtext mathvariant="italic">ac</mml:mtext><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ik</mml:mtext></mml:mrow></mml:msub></mml:math><graphic xlink:href="12911_2017_442_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>
</p><p>SFS calculated the best variable subsets for all balanced data sets <italic>C</italic>
<sub><italic>i</italic></sub>. That is, we have 100 variable subsets of size of 1&#x02013;15 variables. The (average) importance of each variable was measured by a rank metric: 
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ R(j)=\frac{1}{100} \sum_{i=1}^{100} \#F-r(i,j)   $$ \end{document}</tex-math><mml:math id="M6"><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>100</mml:mn></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>100</mml:mn></mml:mrow></mml:munderover><mml:mi>#F</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:math><graphic xlink:href="12911_2017_442_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>
</p><p>where <italic>r</italic>(<italic>i</italic>,<italic>j</italic>) is the rank of variable <italic>j</italic> based on sample <italic>C</italic>
<sub><italic>i</italic></sub> and <italic>#</italic>
<italic>F</italic> is the size of the largest subset that was formed by SFS [<xref ref-type="bibr" rid="CR25">25</xref>&#x02013;<xref ref-type="bibr" rid="CR27">27</xref>]. In this study <italic>#</italic>
<italic>F</italic>=15. Higher <italic>R</italic>(<italic>j</italic>) indicates that variable <italic>j</italic> is more important according to SFS, because it was selected for smaller size variable subsets. That is, variable has higher prediction capability according to SFS and its NHA classification ability is high.</p></sec><sec id="Sec5"><title>Software</title><p>We used four Python packages: <italic>sklearn</italic> [<xref ref-type="bibr" rid="CR20">20</xref>], <italic>mlxtend</italic> [<xref ref-type="bibr" rid="CR28">28</xref>], <italic>numpy</italic> [<xref ref-type="bibr" rid="CR29">29</xref>] and <italic>pandas</italic> [<xref ref-type="bibr" rid="CR30">30</xref>] to implement the classifiers and compute, <italic>acc</italic>, <italic>AUC</italic>, <italic>recall</italic>, <italic>C</italic>
<italic>A</italic>
<sub><italic>Xsub</italic></sub> and <italic>R</italic>(<italic>j</italic>). SFS was computed by the function &#x0201c;SequentialFeatureSelector&#x0201d; in the package <italic>mlxtend</italic>. The classifiers of LR, SVC and GBN were implemented using the functions from the <italic>sklearn.linear_model</italic>, <italic>sklearn.svm</italic> and <italic>sklearn.naive_bayes</italic> packages. The packages of <italic>numpy</italic> and <italic>pandas</italic> were used for data reading and processing.</p></sec></sec><sec id="Sec6" sec-type="results"><title>Results</title><p>Figure <xref rid="Fig3" ref-type="fig">3</xref> shows the performance (average classification accuracy) of the feature subsets found by different classifiers as a function of the subset size when the variables were calculated 3&#x02013;12 months before the evaluation day <italic>t</italic>
<sub><italic>ev</italic></sub>. The average classification accuracy values were determined by Eq. <xref rid="Equ2" ref-type="">2</xref>. The classifiers of LR, SVC and GNB had the average accuracies of 0.776 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0025), 0.762 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0026) and 0.776 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0024), respectively, for the variable subset of 15 variables. According to the student&#x02019;s t-tests [<xref ref-type="bibr" rid="CR31">31</xref>], the average classification accuracy value of LR and GNB methods with 15 variables differed statistically from the SVC method: LR vs. SVC (<italic>p</italic>&#x0003c;.0001) and GNB vs. SVC (<italic>p</italic>&#x0003c;.0001).
<fig id="Fig3"><label>Fig. 3</label><caption><p>Average accuracy as a function of the size of variable subset. Figure shows the classification accuracy of the feature subsets found by different classifiers as a function of the subset size. The average classification accuracy values of LR and GNB methods differ from the SVC method</p></caption><graphic xlink:href="12911_2017_442_Fig3_HTML" id="MO3"/></fig>
</p><p>In addition, we calculated average AUC and recall values for the classifiers. The AUC values were 0.846 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0025), 0.838 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0025) and 0.847 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0024) for the classifiers of LR, SVC and GNB with 15 variables. The recall values were 0.755 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0015), 0.724 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0018) and 0.756 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0018). An AUC of 0.5 indicates no discrimination above chance and an AUC of 1.0 indicates perfect classification. A rough guide for the classification ability is AUC 0.9&#x02212;1.0 excellent, AUC 0.8&#x02212;0.9 good, AUC 0.7&#x02212;0.8 fair and AUC 0.6&#x02212;0.7 poor [<xref ref-type="bibr" rid="CR32">32</xref>]. In general, classification ability is useful if AUC &#x0003e;0.75 [<xref ref-type="bibr" rid="CR33">33</xref>]. That is, the performance of the classifiers with 15 variables was at good level.</p><p>When the variables were calculated 6&#x02013;12 months before the evaluation day <italic>t</italic>
<sub><italic>ev</italic></sub>, the average accuracy of classifiers of LR, SVC and GNB were 0.747 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0030), 0.737 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0029) and 0.734 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0029), respectively, for the variable subset of 15 variables. The AUC values were 0.819 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0027), 0.810 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0028) and 0.813 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0025). The recall values were 0.732 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0017), 0.738 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0025) and 0.732 (<italic>C</italic>
<italic>I</italic>95<italic>%</italic>=.0026). The results of the 6&#x02013;12 months variables show a moderate decrease in performance compared to the 3&#x02013;12 months variables (e.g. LR CA: 0.776 &#x02192; 0.747). The performance of the classifiers with the 6&#x02013;12 months variables, however, is still at good level (<italic>A</italic>
<italic>U</italic>
<italic>C</italic>&#x0003e;0.8).</p><p>Figure <xref rid="Fig4" ref-type="fig">4</xref> shows the <italic>p</italic>-values calculated by the student&#x02019;s t-test when the average classification accuracy values for the subsets of 15 variables of 3&#x02013;12 months were compared to the subsets of <italic>n</italic> variables (<italic>n</italic>=1,&#x02026;15). We defined that if <italic>p</italic>&#x0003c;.05, the difference between the performances of variable subsets is statistically significant. According to the definition, the optimal subset size for LR method was 9 variables. That is, the performance achieved by the subset size of 9 variables did not differ statistically from the subset of 15 variables, when the classifier was LR.
<fig id="Fig4"><label>Fig. 4</label><caption><p>
<italic>P</italic>-value as a function of the size of variable subset compared to the subsets of 15 variables. We defined that if <italic>p</italic>&#x0003c;.05, the difference between the performances of variable subsets is statistically significant. According to the definition, the optimal subset size for LR method is 9 variables. That is, the performance achieved by the subset size of 9 variables did not differ statistically from the subset of 15 variables, when the classifier is LR</p></caption><graphic xlink:href="12911_2017_442_Fig4_HTML" id="MO4"/></fig>
</p><p>Table <xref rid="Tab2" ref-type="table">2</xref> sorts the variables according to ranking score, <italic>R</italic>, described by Eq. <xref rid="Equ3" ref-type="">3</xref>, for the classifiers of LR and GNB. Large <italic>R</italic>(<italic>j</italic>) value means that the variable <italic>j</italic> was selected regularly in small variable subsets for different balanced data sets <italic>C</italic>. That is, the NHA classification ability of the variable <italic>j</italic> is high. According to the results, the most important variables were the diagnoses of G30-G32 and F00-F03 and the RAI metrics of IADL (Activities of Daily Living), MAPLE (Method for Assigning Priority Levels) and CPS (Cognitive Performance Scale). In addition, variables related to the numbes of periods of care were important variables for predicting NHA with the both classifiers. It should be noted, that the RAI variables (IADL, MAPLE and CPS) are not simple measurements or observations, but instead scoring systems developed by researcher and practitioners (e.g., MAPLE [<xref ref-type="bibr" rid="CR34">34</xref>], CPS [<xref ref-type="bibr" rid="CR35">35</xref>] and IADL [<xref ref-type="bibr" rid="CR36">36</xref>]). That is, it is not surprising that these variables have such high performance at predicting NHA.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>The 10 variables of the highest ranking score values calculated for the LR and GNB classifiers (the important variables for the both classifiers are marked as stars)</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left">#</td><td align="left">LR classifier: Variables</td><td align="left">Ranking score</td></tr><tr><td align="left">1</td><td align="left">**Diagnosis F00-F03</td><td align="left">147.9</td></tr><tr><td align="left">2</td><td align="left">**Diagnosis G30-G32</td><td align="left">105.4</td></tr><tr><td align="left">3</td><td align="left">Number of periods of care (6-9 months)</td><td align="left">99.9</td></tr><tr><td align="left">4</td><td align="left">**RAI IADL</td><td align="left">85.5</td></tr><tr><td align="left">5</td><td align="left">**RAI CPS</td><td align="left">73.9</td></tr><tr><td align="left">6</td><td align="left">**RAI MAPLE5</td><td align="left">53.9</td></tr><tr><td align="left">7</td><td align="left">Number of Emergency care visits (3-6 months)</td><td align="left">37.4</td></tr><tr><td align="left">8</td><td align="left">Diagnosis N30-N39</td><td align="left">33.8</td></tr><tr><td align="left">9</td><td align="left">Diagnosis M15-M19</td><td align="left">26.6</td></tr><tr><td align="left">10</td><td align="left">Number of periods of care (3-6 months)</td><td align="left">26.1</td></tr><tr><td align="left">#</td><td align="left">GNB classifier: Variables</td><td align="left">Ranking score</td></tr><tr><td align="left">1</td><td align="left">**Diagnosis F00-F03</td><td align="left">147.2</td></tr><tr><td align="left">2</td><td align="left">**RAI IADL</td><td align="left">106.9</td></tr><tr><td align="left">3</td><td align="left">**Diagnosis G30-G32</td><td align="left">84.2</td></tr><tr><td align="left">4</td><td align="left">Number of home care visits, change (3-6 months vs. 6-9 months)</td><td align="left">84</td></tr><tr><td align="left">5</td><td align="left">Number of periods of care, change (3-6 months vs. 6-9 months)</td><td align="left">78.7</td></tr><tr><td align="left">6</td><td align="left">**RAI CPS</td><td align="left">77.9</td></tr><tr><td align="left">7</td><td align="left">**RAI MAPLE5</td><td align="left">68.8</td></tr><tr><td align="left">8</td><td align="left">RAI PAIN</td><td align="left">57.9</td></tr><tr><td align="left">9</td><td align="left">Specialised care by appointment (6-9 months)</td><td align="left">57</td></tr><tr><td align="left">10</td><td align="left">Specialised care by appointment (3-6 months)</td><td align="left">44.9</td></tr></tbody></table></table-wrap>
</p><p>Figures <xref rid="Fig5" ref-type="fig">5</xref> and <xref rid="Fig6" ref-type="fig">6</xref> plot the normalized ranking score values for the classifiers of SVC and GNB as a function of the values of LR. Ten variables with the highest <italic>R</italic> values of LR classifier are labelled on the figures. The 45&#x000b0; identity line visualizes the differences between the R values of the classifiers. Variables in the lower-right region of the line were more important for the LR than for the SVC (Fig. <xref rid="Fig5" ref-type="fig">5</xref>) or for the GNB (Fig. <xref rid="Fig6" ref-type="fig">6</xref>). Similarly, those in the upper-left region were more important for the SVC (Fig. <xref rid="Fig5" ref-type="fig">5</xref>) or for the GNB (Fig. <xref rid="Fig6" ref-type="fig">6</xref>) than for the LR. For example, the diagnosis N30-N39 was more important for the SVC classifier than for the LR. However, the differences between the most important variables of the classifier were rather small. The variables of the RAI MAPLE, RAI IADL, RAI CPS and diagnoses F00-F03 and G30-G32 were five important variables for the all classifiers.
<fig id="Fig5"><label>Fig. 5</label><caption><p>Normalized ranking score values of the SVC method as a function of the LR method. The variables of the RAI MAPLE, RAI IADL, RAI CPS and diagnoses F00-F03 and G30-G32 were five important variables for the both classifiers</p></caption><graphic xlink:href="12911_2017_442_Fig5_HTML" id="MO5"/></fig>
<fig id="Fig6"><label>Fig. 6</label><caption><p>Normalized ranking score values of the GNB method as a function of the LR method. The variables of the RAI MAPLE, RAI IADL, RAI CPS and diagnoses F00-F03 and G30-G32 were five important variables for the both classifiers</p></caption><graphic xlink:href="12911_2017_442_Fig6_HTML" id="MO6"/></fig>
</p></sec><sec id="Sec7" sec-type="discussion"><title>Discussion</title><p>The aim of the study was to analyse predictors and find out efficient variable subsets to predict NHA in a sample of home caring customers. Particularly, we wanted to find and report the level of accuracy in which NHA can be predicted for individuals. Our results show that the admission of nursing home can be predicted at an accuracy level of 78% / 74% when the variables were calculated 3&#x02013;12 months / 6&#x02013;12 months before the evaluation day. Thus, on average, our model predicts four out of five or three out of four home care customers in the right class in terms of nursing home admission. This is crucial information for decision makers for two reasons. Firstly, the model has to be accurate enough so that investments in preventive interventions can be made. If the accuracy of the model is too low, there are too many false positives and the cost effectiveness of the interventions is low. Secondly, the model needs to predict the individuals with high risk well in advance of the admission. Otherwise, it is too late to implement any interventions. Therefore, the fact that the accuracy of our model with variables 6&#x02013;12 months before the evaluation day is as high as 74%, is important.</p><p>As far as we know, no prior research has published the classification accuracy of the NHA model for individuals. It should be noted, that the classification accuracy is a very common metric in machine learning and other fields. However, prior research has done the analyses at the population level. The important variables have been detected using the 5% level of significance. That is, the values of the parameters of a model (e.g. linear regression (e.g. [<xref ref-type="bibr" rid="CR12">12</xref>]), logistic regression (e.g. [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR14">14</xref>]) or Cox model (e.g. [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>])) are estimated from whole data (without the split of train and test sets) and the significance levels for coefficients are derived. Nothing else has done to see if the model generalizes on the data and individuals that played no role in estimating the parameters for models. Few scholars of NHA (e.g. [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR12">12</xref>]) have applied goodness-of-fit tests (e.g. AIC, <italic>R</italic>
<sup>2</sup>) for the model, but the test results were often more close to zero than one (&#x02248;.20&#x02212;.25).</p><p>We see that the above lacks in NHA research are related to the public health science and data modelling cultures, in which model validation is omitted or calculated only on training data [<xref ref-type="bibr" rid="CR37">37</xref>]. In this study we searched the important variables by averaging the results of variable selection that was executed for many random split of the whole data set. The importance of variables was measured by the ranking metric. The level of classification accuracy of model for different variable subsets was tested by cross-validation. The variable selection from many random data samples and cross-validation warrants the generalization of our variables and models.</p><p>The variables of RAI MAPLE, functional impairment (RAI IADL), cognitive impairment (RAI CPS), memory disorders (G30-G32 and F00-F03) and the use of community-based health services and prior hospital use (emergency visits and periods of care) were the most important. The ICD10 (International Classification of Diseases) group of G30-G32 contains the codes for other degenerative diseases of the nervous system (e.g. Alzheimer) and F00-F03 for dementia. A comparison of our results with the findings of the other investigations revealed that especially, functional [<xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR5">5</xref>&#x02013;<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR14">14</xref>] and cognitive [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR14">14</xref>] impairment, dementia [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR14">14</xref>] and use of community-based health services [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR4">4</xref>] or prior hospitalization [<xref ref-type="bibr" rid="CR9">9</xref>] were also strong predictors of NHA. In contrast to our findings, [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR4">4</xref>&#x02013;<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR14">14</xref>] found that increased age lead to increased risk of NHA. In our study, the importance of variable of age was rather low according to the ranking score.</p><p>The major strengths of this study include its detailed assessment of important variables and model validation and availability of a range of important variables for nursing home admission. The accuracy of the model was high enough to convince the officials of the city of Tampere to integrate the predictive model as a part of home care information system. However, there are some limitations to the present study. We were unable to investigate the associations of social relationships with nursing home admission. Some studies have shown that caregiver characteristics [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR38">38</xref>, <xref ref-type="bibr" rid="CR39">39</xref>], having children [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref>] and marital status [<xref ref-type="bibr" rid="CR6">6</xref>] can be important factors for NHA. Second, this was a study of home caring clients living in a defined geographical location, which may limit the generalizability to older adults living in other areas. Also the finding of this study may not be applicable to population without home caring services. In addition, many of the evaluated risk variables found in this study, are not modifiable. Further work need to be done to evaluate variables that are modifiable and responsive to interventions.</p></sec><sec id="Sec8"><title>Practical implications</title><p>It is clear, that applying ML methods will progress and reform the work of the gerontology researchers and practitioners. The benefits can be viewed from the two aspects: 1) ML methods can be used to construct practical computer software for predicting NHA to aid the decision-making of practitioners, 2) large variable groups can be studied and the most important variables can be found.</p><p>The aspect (1) contributes most to the work of practitioners, e.g. home care case managers. The problem the case managers face is equivalent to that in any preventive care: it is difficult to achieve cost-efficiency if you cannot target a specific subgroup. You usually provide a small intervention for everyone, which is not enough for those at high risk. In order to be effective, the preventive measure needs to be substantial (e.g. in the case of home care customers, 2000&#x020ac;) but becomes too expensive, if offered for many customers. With limited resources one needs to know which customers are most in need of a rehabilitation intervention and target those individuals to maximize cost-effectiveness.</p><p>In the case of home care in Tampere, about 17% of customers are admitted to a nursing home within a year, which is the a priori risk for everyone. The algorithm produced with ML techniques gives a much more accurate risk value enabling the targeting off interventions. Without an accurate prediction algorithm, it is difficult to identify the high risk individuals. It is not enough to identify variables that have a statistically significant relationship with NHA, because this does not provide guidelines that can be applied in practice. For example, we know that a diagnosis indicating dementia or Alzheimer&#x02019;s disease increases NHA risk, but this information is not specific enough to identify the individuals in need of an intervention (unless we target everyone with that particular diagnosis). The ML algorithm provides a risk classification and also allows for the estimation of the accuracy of the prediction. Also, in many cases the case managers need to convince their superiors of the need of investing in rehabilitation interventions for a particular customer. The risk estimate from a validated prediction algorithm can be used as a means of communication between the case manager and her superior.</p><p>Furthermore, the prediction model can also be used to estimate resource requirements for 24 h services by summing up the individual predictions. The predictions provide an upper limit estimation for capacity requirement. With time, when data is gathered on by how much targeted rehabilitation interventions can reduce NHA, the capacity estimates become more accurate.</p><p>In this study, the city of Tampere integrated a computer software containing the prediction algorithm in their data warehouse. The computer software aggregates and processes the variables from different databases and calculates the customer specific NHA risk value. If the risk is high, the case managers consider customer specific interventions, e.g. a new service level assessment, more home care visits, a particular therapy or revised medication. Prior to the implementation of the prediction algorithm, the rehabilitation interventions were not targeted systematically. Most often interventions were used when a care taker or nurse or next of kin noticed a change in functional ability and notified the case manager. When using the prediction algorithm, interventions are targeted based on more objective evaluations and customers are screened regularly. This way it is possible to identify customers at risk earlier than before. Also, after the implementation of the prediction algorithm, the selection of different rehabilitation interventions available for home care customers has been increased.</p><p>The next step in the study and implementation project is to gather data from the interventions and their effects, and build another ML model to predict the effectiveness of each intervention for each type of customer. Also, the model can be used to predict, who is no longer capable of benefitting from an intervention. This added information will further improve the cost-effectiveness of home care.</p><p>The aspect (2) contributes both the gerontology research and practical work. Variable selection can be used to identify which of the available variables are closely related to the prediction of the NHA and to discard those unrelated to it, reducing the dimensionality of the dataset. For the researcher of gerontology, the process of variable selection may indicate new variables that had not been previously considered as relevant to NHA. For example, in this study, we found about 10 important variables for predicting NHA. Furthermore, the model validity is easier to evaluate after variable selection is used to reduce the dimensionality of the model. After dimension reduction, the researchers know the variables for which they should focus in their research [<xref ref-type="bibr" rid="CR40">40</xref>]. For the NHA research, this may mean that the variables for which the interventions should be focused can be found.</p><p>The second benefit, because of the variable selection, is that the number of variables, integrated in the software tool, can be minimized. This is important, because each new added variable requires resources for the processes of data aggregation and validation and requirements for data integration from different databases.</p></sec><sec id="Sec9" sec-type="conclusion"><title>Conclusion</title><p>Most elderly people prefer to live at home in a familiar environment than move to a nursing home. The findings of our study indicate important variable subsets for predicting NHA of community dwelling home care customers, and offer potential to find those individuals at the level of 78%, who are at risk of NHA. The most important variables were RAI MAPLE, functional impairment (RAI IADL), cognitive impairment (RAI CPS), memory disorders (diagnoses G30-G32 and F00-F03) and the use of community-based health-service and prior hospital use.</p></sec><sec id="Sec10"><title>Endnotes</title><p>
<sup>1</sup> Tampere is the third largest city in Finland. The percent of population over 65 years is 18.0% that is approximately same as in the other big cities in Finland (<ext-link ext-link-type="uri" xlink:href="http://www.stat.fi">http://www.stat.fi</ext-link>). Also, the scope or services offered for the elderly as well as eligibility criteria for home care and nursing home care are fairly similar in all areas in Finland.</p><p>
<sup>2</sup> Kotitori <ext-link ext-link-type="uri" xlink:href="http://www.tampereenkotitori.fi/">http://www.tampereenkotitori.fi/</ext-link>
</p></sec></body><back><glossary><title>Abbreviations</title><def-list><def-item><term>AUC</term><def><p>Area under the curve</p></def></def-item><def-item><term>CA</term><def><p>Classification accuracy</p></def></def-item><def-item><term>CPS</term><def><p>Cognitive performance scale</p></def></def-item><def-item><term>GNB</term><def><p>Gaussian naive Bayes</p></def></def-item><def-item><term>IADL</term><def><p>Activities of daily living</p></def></def-item><def-item><term>LR</term><def><p>Logistic regression</p></def></def-item><def-item><term>MAPLE</term><def><p>Method for assigning priority levels</p></def></def-item><def-item><term>ML</term><def><p>Machine learning</p></def></def-item><def-item><term>NHS</term><def><p>Nursing home admission</p></def></def-item><def-item><term>RAI-HC</term><def><p>Resident assessment instrument - home care</p></def></def-item><def-item><term>SBE</term><def><p>Sequential backward elimination</p></def></def-item><def-item><term>SFS</term><def><p>Sequential forward selection</p></def></def-item><def-item><term>SVM</term><def><p>Support vector machine</p></def></def-item></def-list></glossary><ack><title>Acknowledgements</title><p>The authors would like to thanks Mr. Mikko Mulari for his contribution for data acquisition and interpretation.</p><sec id="d29e3873"><title>Funding</title><p>The study was independent of external funding sources.</p></sec><sec id="d29e3878"><title>Availability of data and materials</title><p>The dataset which we have acquired will not be shared as a supplementary file. All Python codes for data analysis and variable subset selection are available upon request.</p></sec><sec id="d29e3883"><title>Authors&#x02019; contributions</title><p>MN participated in the data collection, study design, performed the literature review, programming all analyses and wrote the first draft of the paper. RLL contributed to the design of the study, interpretation of the results and was involved in writing of the first draft of the paper. ES and AT participated to the design of the study and revised the paper. VK managed and supervised the study, participated to the design of the study and revised the paper. All authors read and approved the final manuscript.</p></sec><sec id="d29e3888"><title>Competing interests</title><p>MN: employment (Nordic Healthcare Group), RLL and VK: employment and stockholder (Nordic Healthcare Group). Nordic Healthcare Group (NHG) is a Finnish company specialised in planning and developing health and social services especially in Finland, Sweden and Russia. ES and AT declare that they have no competing interests.</p></sec><sec id="d29e3893"><title>Consent for publication</title><p>Not applicable.</p></sec><sec id="d29e3898"><title>Ethics approval and consent to participate</title><p>The data were provided by the city of Tampere and Pirkanmaa Hospital District who granted us the research promises and the privilege to use the data. Data were aggregated and anonymized by the data administration of the city of Tampere before they were provided to the authors. Ethics approval was not required. In Finland, ethics approval is not required for retrospective registry studies with anonymized data. Since this was a retrospective study, consent to participate was not required.</p></sec><sec id="d29e3903"><title>Publisher&#x02019;s Note</title><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></sec></ack><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hajek</surname><given-names>A</given-names></name><name><surname>Brettschneider</surname><given-names>C</given-names></name><name><surname>Lange</surname><given-names>C</given-names></name><name><surname>Posselt</surname><given-names>T</given-names></name><name><surname>Wiese</surname><given-names>B</given-names></name><name><surname>Steinmann</surname><given-names>S</given-names></name><name><surname>Weyerer</surname><given-names>S</given-names></name><name><surname>Werle</surname><given-names>J</given-names></name><name><surname>Pentzek</surname><given-names>M</given-names></name><name><surname>Fuchs</surname><given-names>A</given-names></name><name><surname>Stein</surname><given-names>J</given-names></name><name><surname>Luck</surname><given-names>T</given-names></name><name><surname>Bickel</surname><given-names>H</given-names></name><name><surname>M&#x000f6;sch</surname><given-names>E</given-names></name><name><surname>Wagner</surname><given-names>M</given-names></name><name><surname>Jessen</surname><given-names>F</given-names></name><name><surname>Maier</surname><given-names>W</given-names></name><name><surname>Scherer</surname><given-names>M</given-names></name><name><surname>Riedel-Heller</surname><given-names>SG</given-names></name><name><surname>K&#x000f6;nig</surname><given-names>HH</given-names></name><name><surname>Group</surname><given-names>AS</given-names></name></person-group><article-title>Longitudinal predictors of institutionalization in old age</article-title><source>PLoS ONE</source><year>2015</year><volume>10</volume><issue>12</issue><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1371/journal.pone.0144203</pub-id></element-citation></ref><ref id="CR2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>S&#x000f8;rbye</surname><given-names>L</given-names></name><name><surname>Hamran</surname><given-names>T</given-names></name><name><surname>Henriksen</surname><given-names>N</given-names></name><name><surname>Norberg</surname><given-names>A</given-names></name></person-group><article-title>Home care patients in four nordic capitals &#x02014; predictors of nursing home admission during one-year followup</article-title><source>J Multidiscip Healthc</source><year>2010</year><volume>3</volume><fpage>11</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.2147/JMDH.S8979</pub-id><?supplied-pmid 21197351?><pub-id pub-id-type="pmid">21197351</pub-id></element-citation></ref><ref id="CR3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gnjidic</surname><given-names>D</given-names></name><name><surname>Stanaway</surname><given-names>F</given-names></name><name><surname>Cumming</surname><given-names>R</given-names></name><name><surname>Waite</surname><given-names>L</given-names></name><name><surname>Blyth</surname><given-names>F</given-names></name><name><surname>Naganathan</surname><given-names>V</given-names></name><name><surname>Handelsman</surname><given-names>DJ</given-names></name><name><surname>Le Couteur</surname><given-names>DG</given-names></name></person-group><article-title>Mild cognitive impairment predicts institutionalization among older men: A population-based cohort study</article-title><source>PLoS ONE</source><year>2012</year><volume>7</volume><issue>9</issue><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1371/journal.pone.0046061</pub-id></element-citation></ref><ref id="CR4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eska</surname><given-names>K</given-names></name><name><surname>Graessel</surname><given-names>E</given-names></name><name><surname>Donath</surname><given-names>C</given-names></name><name><surname>Schwarzkopf</surname><given-names>L</given-names></name><name><surname>Lauterberg</surname><given-names>J</given-names></name><name><surname>Holle</surname><given-names>R</given-names></name></person-group><article-title>Predictors of institutionalization of dementia patients in mild and moderate stages: A 4-year prospective analysis</article-title><source>Dement Geriatr Cogn Disord Extra</source><year>2013</year><volume>3</volume><issue>1</issue><fpage>426</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1159/000355079</pub-id></element-citation></ref><ref id="CR5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luppa</surname><given-names>M</given-names></name><name><surname>Luck</surname><given-names>T</given-names></name><name><surname>Matschinger</surname><given-names>H</given-names></name><name><surname>K&#x000f6;nig</surname><given-names>HH</given-names></name><name><surname>Riedel-Heller</surname><given-names>SG</given-names></name></person-group><article-title>Predictors of nursing home admission of individuals without a dementia diagnosis before admission - results from the leipzig longitudinal study of the aged (leila 75+)</article-title><source>BMC Health Serv Res</source><year>2010</year><volume>10</volume><issue>1</issue><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1186/1472-6963-10-186</pub-id><pub-id pub-id-type="pmid">20044945</pub-id></element-citation></ref><ref id="CR6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andel</surname><given-names>R</given-names></name><name><surname>Hyer</surname><given-names>K</given-names></name><name><surname>Slack</surname><given-names>A</given-names></name></person-group><article-title>Risk factors for nursing home placement in older adults with and without dementia</article-title><source>J Aging Health</source><year>2007</year><volume>19</volume><issue>2</issue><fpage>213</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1177/0898264307299359</pub-id><?supplied-pmid 17413132?><pub-id pub-id-type="pmid">17413132</pub-id></element-citation></ref><ref id="CR7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiska</surname><given-names>C</given-names></name><name><surname>Philip</surname><given-names>W</given-names></name></person-group><article-title>Predictors of entry to the nursing home: Does length of follow-up matter?</article-title><source>Arch Gerontol Geriatr</source><year>2011</year><volume>53</volume><issue>3</issue><fpage>309</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1016/j.archger.2010.12.009</pub-id><pub-id pub-id-type="pmid">21251719</pub-id></element-citation></ref><ref id="CR8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dram&#x000e9;</surname><given-names>M</given-names></name><name><surname>Lang</surname><given-names>P</given-names></name><name><surname>Jolly</surname><given-names>D</given-names></name><name><surname>Narbey</surname><given-names>D</given-names></name><name><surname>Mahmoudi</surname><given-names>R</given-names></name><name><surname>Lani&#x000e8;ce</surname><given-names>I</given-names></name><name><surname>Somme</surname><given-names>D</given-names></name><name><surname>Gauvain</surname><given-names>J</given-names></name><name><surname>Heitz</surname><given-names>D</given-names></name><name><surname>Voisin</surname><given-names>T</given-names></name><name><surname>de Wazi&#x000e8;res</surname><given-names>B</given-names></name><name><surname>Gonthier</surname><given-names>R</given-names></name><name><surname>Ankri</surname><given-names>J</given-names></name><name><surname>Saint-Jean</surname><given-names>O</given-names></name><name><surname>Jeandel</surname><given-names>C</given-names></name><name><surname>Couturier</surname><given-names>P</given-names></name><name><surname>Blanchard</surname><given-names>F</given-names></name><name><surname>Novella</surname><given-names>J</given-names></name></person-group><article-title>Nursing home admission in elderly subjects with dementia: predictive factors and future challenges</article-title><source>J Am Med Dir Assoc</source><year>2013</year><volume>13</volume><fpage>17</fpage><lpage>20</lpage></element-citation></ref><ref id="CR9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akamigbo</surname><given-names>A</given-names></name><name><surname>Wolinsky</surname><given-names>F</given-names></name></person-group><article-title>Reported expectations for nursing home placement among older adults and their role as risk factors for nursing home admissions</article-title><source>Gerontologist</source><year>2006</year><volume>46</volume><fpage>464</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1093/geront/46.4.464</pub-id><?supplied-pmid 16921000?><pub-id pub-id-type="pmid">16921000</pub-id></element-citation></ref><ref id="CR10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheppard</surname><given-names>K</given-names></name><name><surname>Brown</surname><given-names>C</given-names></name><name><surname>Hearld</surname><given-names>K</given-names></name><name><surname>Roth</surname><given-names>D</given-names></name><name><surname>Sawyer</surname><given-names>P</given-names></name><name><surname>Locher</surname><given-names>J</given-names></name><name><surname>Allman</surname><given-names>R</given-names></name><name><surname>Ritchie</surname><given-names>CS</given-names></name></person-group><article-title>Symptom burden predicts nursing home admissions among older adults</article-title><source>J Pain Symptom Manage</source><year>2013</year><volume>46</volume><fpage>591</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1016/j.jpainsymman.2012.10.228</pub-id><?supplied-pmid 23218806?><pub-id pub-id-type="pmid">23218806</pub-id></element-citation></ref><ref id="CR11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Bonsdorff</surname><given-names>M</given-names></name><name><surname>Rantanen</surname><given-names>T</given-names></name><name><surname>Laukkanen</surname><given-names>P</given-names></name><name><surname>Suutama</surname><given-names>T</given-names></name><name><surname>Heikkinen</surname><given-names>E</given-names></name></person-group><article-title>Mobility limitations and cognitive deficits as predictors of institutionalization among community-dwelling older people</article-title><source>Gerontology</source><year>2006</year><volume>52</volume><issue>6</issue><fpage>359</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1159/000094985</pub-id><?supplied-pmid 16905887?><pub-id pub-id-type="pmid">16905887</pub-id></element-citation></ref><ref id="CR12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Naidoo</surname><given-names>N</given-names></name><name><surname>Er</surname><given-names>B</given-names></name><name><surname>Cheong</surname><given-names>A</given-names></name><name><surname>Fong</surname><given-names>NP</given-names></name><name><surname>Tay</surname><given-names>CY</given-names></name><name><surname>Chan</surname><given-names>KM</given-names></name><name><surname>Tan</surname><given-names>BY</given-names></name><name><surname>Menon</surname><given-names>E</given-names></name><name><surname>Ee</surname><given-names>CH</given-names></name><name><surname>Lee</surname><given-names>KK</given-names></name><name><surname>Ng</surname><given-names>YS</given-names></name><name><surname>Teo</surname><given-names>YY</given-names></name><name><surname>Koh</surname><given-names>GCH</given-names></name></person-group><article-title>Factors associated with nursing home placement of all patients admitted for inpatient rehabilitation in singapore community hospitals from 1996 to 2005: A disease stratified analysis</article-title><source>PLoS ONE</source><year>2013</year><volume>8</volume><issue>12</issue><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1371/annotation/dd945f7c-c50b-461d-ab38-15e8b0966458</pub-id></element-citation></ref><ref id="CR13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wergeland</surname><given-names>J</given-names></name><name><surname>Selb&#x000e6;k</surname><given-names>G</given-names></name><name><surname>Bergh</surname><given-names>S</given-names></name><name><surname>Soederhamn</surname><given-names>U</given-names></name><name><surname>Kirkevold</surname><given-names>.</given-names></name></person-group><article-title>Predictors for nursing home admission and death among community-dwelling people 70 years and older who receive domiciliary care</article-title><source>Dement Geriatr Cogn Disord Extra</source><year>2015</year><volume>5</volume><fpage>320</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1159/000437382</pub-id></element-citation></ref><ref id="CR14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheppard</surname><given-names>K</given-names></name><name><surname>Sawyer</surname><given-names>P</given-names></name><name><surname>Ritchie</surname><given-names>C</given-names></name><name><surname>Allman</surname><given-names>R</given-names></name><name><surname>Brown</surname><given-names>C</given-names></name></person-group><article-title>Life-space mobility predicts nursing home admission over 6 years</article-title><source>J Aging Health</source><year>2013</year><volume>25</volume><fpage>907</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1177/0898264313497507</pub-id><?supplied-pmid 23965310?><pub-id pub-id-type="pmid">23965310</pub-id></element-citation></ref><ref id="CR15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helvik</surname><given-names>AS</given-names></name><name><surname>Skancke</surname><given-names>RH</given-names></name><name><surname>Selb&#x000e6;k</surname><given-names>G</given-names></name><name><surname>Engedal</surname><given-names>K</given-names></name></person-group><article-title>Nursing home admission during the first year after hospitalization? the contribution of cognitive impairment</article-title><source>PLoS ONE</source><year>2014</year><volume>9</volume><issue>1</issue><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1371/journal.pone.0086116</pub-id></element-citation></ref><ref id="CR16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaugler</surname><given-names>J</given-names></name><name><surname>Duval</surname><given-names>S</given-names></name><name><surname>Anderson</surname><given-names>K</given-names></name><name><surname>Kane</surname><given-names>R</given-names></name></person-group><article-title>Predicting nursing home admission in the u.s: a meta-analysis</article-title><source>BMC Geriatr</source><year>2007</year><volume>13</volume><fpage>1</fpage><lpage>14</lpage></element-citation></ref><ref id="CR17"><label>17</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>J</given-names></name><name><surname>Fries</surname><given-names>B</given-names></name><name><surname>Bernabei</surname><given-names>R</given-names></name><name><surname>Steel</surname><given-names>K</given-names></name><name><surname>Ikegami</surname><given-names>N</given-names></name><name><surname>Carpenter</surname><given-names>I</given-names></name><name><surname>Gilgen</surname><given-names>R</given-names></name><name><surname>DuPasquier</surname><given-names>J</given-names></name><name><surname>Frijters</surname><given-names>D</given-names></name><name><surname>Henrard</surname><given-names>J</given-names></name><name><surname>Hirdes</surname><given-names>J</given-names></name><name><surname>Belleville-Taylor</surname><given-names>P</given-names></name><name><surname>Berg</surname><given-names>K</given-names></name><name><surname>Bj&#x000f6;rkgren</surname><given-names>M</given-names></name><name><surname>Gray</surname><given-names>I</given-names></name><name><surname>Hawes</surname><given-names>C</given-names></name><name><surname>Ljunggren</surname><given-names>G</given-names></name><name><surname>Nonemaker</surname><given-names>S</given-names></name><name><surname>Phillips</surname><given-names>C</given-names></name><name><surname>Zimmerman</surname><given-names>D</given-names></name></person-group><source>interRAI Home Care (HC) Assessment Form and User&#x02019;s Manual</source><year>2009</year><publisher-loc>USA</publisher-loc><publisher-name>interRAI</publisher-name></element-citation></ref><ref id="CR18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Yu</surname><given-names>L</given-names></name></person-group><article-title>Toward integrating feature selection algorithms for classification and clustering</article-title><source>IEEE Trans Knowl Data Eng</source><year>2005</year><volume>17</volume><issue>4</issue><fpage>491</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.1109/TKDE.2005.66</pub-id></element-citation></ref><ref id="CR19"><label>19</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Friedman</surname><given-names>J</given-names></name></person-group><source>The Elements of Statistical Learning</source><year>2009</year><publisher-loc>New York</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="CR20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name><name><surname>Vanderplas</surname><given-names>J</given-names></name><name><surname>Passos</surname><given-names>A</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Brucher</surname><given-names>M</given-names></name><name><surname>Perrot</surname><given-names>M</given-names></name><name><surname>Duchesnay</surname><given-names>E</given-names></name></person-group><article-title>Scikit-learn: Machine Learning in Python</article-title><source>J Mach Learn Res</source><year>2011</year><volume>12</volume><fpage>2825</fpage><lpage>830</lpage></element-citation></ref><ref id="CR21"><label>21</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Olson</surname><given-names>DL</given-names></name><name><surname>Delen</surname><given-names>D</given-names></name></person-group><source>Advanced Data Mining Techniques</source><year>2008</year><publisher-loc>Germany</publisher-loc><publisher-name>Springer-Verlag Berlin Heidelberg</publisher-name></element-citation></ref><ref id="CR22"><label>22</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McCullagh</surname><given-names>P</given-names></name><name><surname>Nelder</surname><given-names>JA</given-names></name></person-group><source>Generalized Linear Models</source><year>1989</year><publisher-loc>London</publisher-loc><publisher-name>London: Chapman &#x00026; Hall</publisher-name></element-citation></ref><ref id="CR23"><label>23</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Barr</surname><given-names>V</given-names></name><name><surname>Markov</surname><given-names>Z</given-names></name></person-group><article-title>The optimality of naive bayes</article-title><source>Proceedings of the Seventeenth International Florida Artificial Intelligence Research Society Conference (FLAIRS 2004)</source><year>2004</year><publisher-loc>Palo Alto</publisher-loc><publisher-name>AAAI Press</publisher-name></element-citation></ref><ref id="CR24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cortes</surname><given-names>C</given-names></name><name><surname>Vapnik</surname><given-names>V</given-names></name></person-group><article-title>Support-vector networks</article-title><source>Mach Learn</source><year>1995</year><volume>20</volume><issue>3</issue><fpage>273</fpage><lpage>97</lpage></element-citation></ref><ref id="CR25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xin</surname><given-names>L</given-names></name><name><surname>Zhu</surname><given-names>M</given-names></name></person-group><article-title>Stochastic stepwise ensembles for variable selection</article-title><source>J Comput Graph Stat</source><year>2012</year><volume>21</volume><issue>2</issue><fpage>275</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1080/10618600.2012.679223</pub-id></element-citation></ref><ref id="CR26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>L</given-names></name><name><surname>Zhu</surname><given-names>M</given-names></name><name><surname>Poss</surname><given-names>JW</given-names></name><name><surname>Hirdes</surname><given-names>JP</given-names></name><name><surname>Glenny</surname><given-names>C</given-names></name><name><surname>Stolee</surname><given-names>P</given-names></name></person-group><article-title>Opinion versus practice regarding the use of rehabilitation services in home care: an investigation using machine learning algorithms</article-title><source>BMC Med Inform Decis Mak</source><year>2015</year><volume>15</volume><issue>1</issue><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1186/s12911-015-0203-1</pub-id><pub-id pub-id-type="pmid">25889846</pub-id></element-citation></ref><ref id="CR27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>N</given-names></name><name><surname>Koh</surname><given-names>ZX</given-names></name><name><surname>Goh</surname><given-names>J</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Haaland</surname><given-names>B</given-names></name><name><surname>Ting</surname><given-names>BP</given-names></name><name><surname>Ong</surname><given-names>MEH</given-names></name></person-group><article-title>Prediction of adverse cardiac events in emergency department patients with chest pain using machine learning for variable selection</article-title><source>BMC Med Inform Decis Mak</source><year>2014</year><volume>14</volume><issue>1</issue><fpage>75</fpage><pub-id pub-id-type="doi">10.1186/1472-6947-14-75</pub-id><?supplied-pmid 25150702?><pub-id pub-id-type="pmid">25150702</pub-id></element-citation></ref><ref id="CR28"><label>28</label><mixed-citation publication-type="other">Raschka S. Mlxtend. 2016. doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5281/zenodo.49235">10.5281/zenodo.49235</ext-link>. <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5281/zenodo.49235">http://dx.doi.org/10.5281/zenodo.49235</ext-link>. Accessed 8 Feb 2016.</mixed-citation></ref><ref id="CR29"><label>29</label><mixed-citation publication-type="other">Jones E, Oliphant T, Peterson P, et al.SciPy: Open source scientific tools for Python. 2001. <ext-link ext-link-type="uri" xlink:href="http://www.scipy.org/">http://www.scipy.org/</ext-link>. Accessed 15 Aug 2016.</mixed-citation></ref><ref id="CR30"><label>30</label><mixed-citation publication-type="other">McKinney W. Data structures for statistical computing in python. In: Proceedings of the 9th Python in Science Conference, SciPy.org. van der Walt, S., Millman, J. (eds.).2010. p. 51&#x02013;56. <ext-link ext-link-type="uri" xlink:href="http://conference.scipy.org/proceedings/">http://conference.scipy.org/proceedings/</ext-link>.</mixed-citation></ref><ref id="CR31"><label>31</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wilcox</surname><given-names>R</given-names></name></person-group><source>Basic Statistics: Understanding Conventional Methods and Modern Insights</source><year>2009</year><publisher-loc>New York</publisher-loc><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="CR32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roelen</surname><given-names>CA</given-names></name><name><surname>B&#x000fc;ltmann</surname><given-names>U</given-names></name><name><surname>van Rhenen</surname><given-names>W</given-names></name><name><surname>van der Klink</surname><given-names>JJ</given-names></name><name><surname>Twisk</surname><given-names>JW</given-names></name><name><surname>Heymans</surname><given-names>MW</given-names></name></person-group><article-title>External validation of two prediction models identifying employees at risk of high sickness absence: cohort study with 1-year follow-up</article-title><source>BMC Public Health</source><year>2013</year><volume>13</volume><issue>1</issue><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1186/1471-2458-13-105</pub-id><pub-id pub-id-type="pmid">23280303</pub-id></element-citation></ref><ref id="CR33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>J</given-names></name><name><surname>Upadhye</surname><given-names>S</given-names></name><name><surname>Worster</surname><given-names>A</given-names></name></person-group><article-title>Understanding receiver operating characteristic (roc) curves</article-title><source>Can J Emerg Med</source><year>2006</year><volume>8</volume><issue>1</issue><fpage>19</fpage><lpage>20</lpage></element-citation></ref><ref id="CR34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hirdes</surname><given-names>J</given-names></name><name><surname>Poss</surname><given-names>J</given-names></name><name><surname>Curtin-Telegdi</surname><given-names>N</given-names></name></person-group><article-title>The method for assigning priority levels (maple): A new decision-support system for allocating home care resources</article-title><source>BMC Med</source><year>2008</year><volume>6</volume><issue>1</issue><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1186/1741-7015-6-9</pub-id><pub-id pub-id-type="pmid">18234075</pub-id></element-citation></ref><ref id="CR35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>J</given-names></name><name><surname>Fries</surname><given-names>B</given-names></name><name><surname>Mehr</surname><given-names>D</given-names></name><name><surname>Hawes</surname><given-names>C</given-names></name><name><surname>Philips</surname><given-names>C</given-names></name><name><surname>Mor</surname><given-names>V</given-names></name><name><surname>Lipsitz</surname><given-names>L</given-names></name></person-group><article-title>Mds cognitive performance scale</article-title><source>J Gerontol Med Sci</source><year>1994</year><volume>49</volume><issue>4</issue><fpage>174</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1093/geronj/49.4.M174</pub-id></element-citation></ref><ref id="CR36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spector</surname><given-names>W</given-names></name><name><surname>Fleishman</surname><given-names>J</given-names></name></person-group><article-title>Combining activities of daily living with instrumental activities of daily living to measure functional disability</article-title><source>J Gerontol B Psychol Sci Soc Sci.</source><year>1998</year><volume>53</volume><issue>1</issue><fpage>46</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1093/geronb/53B.1.S46</pub-id></element-citation></ref><ref id="CR37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title>Statistical modeling: The two cultures (with comments and a rejoinder by the author)</article-title><source>Statist Sci</source><year>2001</year><volume>16</volume><issue>3</issue><fpage>199</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1214/ss/1009213726</pub-id></element-citation></ref><ref id="CR38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaugler</surname><given-names>J</given-names></name><name><surname>Yu</surname><given-names>F</given-names></name><name><surname>Krichbaum</surname><given-names>K</given-names></name><name><surname>Wyman</surname><given-names>J</given-names></name></person-group><article-title>Predictors of nursing home admission for persons with dementia</article-title><source>Med Care</source><year>2009</year><volume>47</volume><issue>2</issue><fpage>191</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1097/MLR.0b013e31818457ce</pub-id><?supplied-pmid 19169120?><pub-id pub-id-type="pmid">19169120</pub-id></element-citation></ref><ref id="CR39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donnelly</surname><given-names>NA</given-names></name><name><surname>Hickey</surname><given-names>A</given-names></name><name><surname>Burns</surname><given-names>A</given-names></name><name><surname>Murphy</surname><given-names>P</given-names></name><name><surname>Doyle</surname><given-names>F</given-names></name></person-group><article-title>Systematic review and meta-analysis of the impact of carer stress on subsequent institutionalisation of community-dwelling older people</article-title><source>PLoS ONE</source><year>2015</year><volume>10</volume><issue>6</issue><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1371/journal.pone.0128213</pub-id></element-citation></ref><ref id="CR40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giabbanelli</surname><given-names>PJ</given-names></name><name><surname>Adams</surname><given-names>J</given-names></name></person-group><article-title>Identifying small groups of foods that can predict achievement of key dietary recommendations: data mining of the uk national diet and nutrition survey, 2008-12</article-title><source>Public Health Nutr</source><year>2016</year><volume>19</volume><issue>9</issue><fpage>1543</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1017/S1368980016000185</pub-id><?supplied-pmid 26879185?><pub-id pub-id-type="pmid">26879185</pub-id></element-citation></ref></ref-list></back></article>