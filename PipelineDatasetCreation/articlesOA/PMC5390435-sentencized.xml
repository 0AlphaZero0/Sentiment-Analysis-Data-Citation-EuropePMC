<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Med Inform Decis Mak</journal-id><journal-id journal-id-type="iso-abbrev">BMC Med Inform Decis Mak</journal-id><journal-title-group><journal-title>BMC Medical Informatics and Decision Making</journal-title></journal-title-group><issn pub-type="epub">1472-6947</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">5390435</article-id><article-id pub-id-type="publisher-id">442</article-id><article-id pub-id-type="doi">10.1186/s12911-017-0442-4</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title><SecTag type="TITLE"><text><SENT sid="0" pm="."><plain>Development and validation of classifiers and variable subsets for predicting nursing home admission </plain></SENT>
</text></SecTag></article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7429-3710</contrib-id><name><surname>Nuutinen</surname><given-names>Mikko</given-names></name><address><email>mikko.nuutinen@nhg.fi</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Leskelä</surname><given-names>Riikka-Leena</given-names></name><address><email>riikka-leena.leskela@nhg.fi</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Suojalehto</surname><given-names>Ella</given-names></name><address><email>ella.suojalehto@tampere.fi</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Tirronen</surname><given-names>Anniina</given-names></name><address><email>anniina.tirronen@tampere.fi</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Komssi</surname><given-names>Vesa</given-names></name><address><email>vesa.komssi@nhg.fi</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label>Nordic Healthcare Group, Vattuniemenranta 2, Helsinki, 00210 Finland </aff><aff id="Aff2"><label>2</label>City of Tampere, PL 487, Tampere, 33101 Finland </aff></contrib-group><pub-date pub-type="epub"><day>13</day><month>4</month><year>2017</year></pub-date><pub-date pub-type="pmc-release"><day>13</day><month>4</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>17</volume><elocation-id>39</elocation-id><history><date date-type="received"><day>23</day><month>11</month><year>2016</year></date><date date-type="accepted"><day>7</day><month>4</month><year>2017</year></date></history><permissions><copyright-statement>© The Author(s) 2017</copyright-statement><license license-type="OpenAccess"><license-p>
<bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><abstract id="Abs1"><sec><title><text><SENT sid="1" pm="."><plain>Background </plain></SENT>
</text></title><p><SecTag type="ABS"><text><SENT sid="2" pm="."><plain>In previous years a substantial number of studies have identified statistically important predictors of nursing home admission (NHA). </plain></SENT>
<SENT sid="3" pm="."><plain>However, as far as we know, the analyses have been done at the population-level. </plain></SENT>
<SENT sid="4" pm="."><plain>No prior research has analysed the prediction accuracy of a NHA model for individuals. </plain></SENT>
</text></SecTag></p></sec><sec><title><text><SENT sid="5" pm="."><plain>Methods </plain></SENT>
</text></title><p><SecTag type="ABS"><text><SENT sid="6" pm="."><plain>This study is an analysis of 3056 longer-term home care customers in the city of Tampere, Finland. </plain></SENT>
<SENT sid="7" pm="."><plain>Data were collected from the records of social and health service usage and RAI-HC (Resident Assessment Instrument - Home Care) assessment system during January 2011 and September 2015. </plain></SENT>
<SENT sid="8" pm="."><plain>The aim was to find out the most efficient variable subsets to predict NHA for individuals and validate the accuracy. </plain></SENT>
<SENT sid="9" pm="."><plain>The variable subsets of predicting NHA were searched by sequential forward selection (SFS) method, a variable ranking metric and the classifiers of logistic regression (LR), support vector machine (SVM) and Gaussian naive Bayes (GNB). </plain></SENT>
<SENT sid="10" pm="."><plain>The validation of the results was guaranteed using randomly balanced data sets and cross-validation. </plain></SENT>
<SENT sid="11" pm="."><plain>The primary performance metrics for the classifiers were the prediction accuracy and AUC (average area under the curve). </plain></SENT>
</text></SecTag></p></sec><sec><title><text><SENT sid="12" pm="."><plain>Results </plain></SENT>
</text></title><p><SecTag type="ABS"><text><SENT sid="13" pm="."><plain>The LR and GNB classifiers achieved 78% accuracy for predicting NHA. </plain></SENT>
<SENT sid="14" pm="."><plain>The most important variables were RAI MAPLE (Method for Assigning Priority Levels), functional impairment (RAI IADL, Activities of Daily Living), cognitive impairment (RAI CPS, Cognitive Performance Scale), memory disorders (diagnoses G30-G32 and F00-F03) and the use of community-based health-service and prior hospital use (emergency visits and periods of care). </plain></SENT>
</text></SecTag></p></sec><sec><title><text><SENT sid="15" pm="."><plain>Conclusion </plain></SENT>
</text></title><p><SecTag type="ABS"><text><SENT sid="16" pm="."><plain>The accuracy of the classifier for individuals was high enough to convince the officials of the city of Tampere to integrate the predictive model based on the findings of this study as a part of home care information system. </plain></SENT>
<SENT sid="17" pm="."><plain>Further work need to be done to evaluate variables that are modifiable and responsive to interventions. </plain></SENT>
</text></SecTag></p></sec></abstract><SecTag type="KEYWORD"><kwd-group xml:lang="en"><title>Keywords</title><kwd>Nursing home admission</kwd><kwd>Classifier</kwd><kwd>Classification accuracy</kwd><kwd>Variable selection</kwd></kwd-group></SecTag><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>© The Author(s) 2017</meta-value></custom-meta></custom-meta-group></article-meta></front><body><SecTag type="INTRO"><sec id="Sec1"><title><text><SENT sid="18" pm="."><plain>Background </plain></SENT>
</text></title><p><text><SENT sid="19" pm="."><plain>It is a common goal for elderly care services to support and enable living at home as long as possible. </plain></SENT>
<SENT sid="20" pm="."><plain>Most people would rather live at home in a familiar environment, than move to a nursing home or an assisted living facility. </plain></SENT>
<SENT sid="21" pm="."><plain>Also, from the view point of the service system, 24 h services are expensive. </plain></SENT>
<SENT sid="22" pm="."><plain>Thus, supporting functional and cognitive capabilities, which enable living at home, improves the quality of life and is cost saving for the payer. </plain></SENT>
<SENT sid="23" pm="."><plain>However, interventions aimed at improving or sustaining functional and cognitive capabilities can be expensive. </plain></SENT>
<SENT sid="24" pm="."><plain>Therefore, the need to be targeted to those individuals, who are at risk of needing 24 h service in the near future, but who are still capable of benefiting from the intervention. </plain></SENT>
<SENT sid="25" pm="."><plain>Furthermore, the resource planning of 24 h services benefits from the information of upcoming admissions. </plain></SENT>
<SENT sid="26" pm="."><plain>Identification of reliable predictors and creation of tools that calculate risk for individuals provide an answer to these problems. </plain></SENT>
</text></p><p><text><SENT sid="27" pm="."><plain>Much research has focused on identifying predictors of nursing home admission (NHA) [1–14]. </plain></SENT>
<SENT sid="28" pm="."><plain>The studies vary according to variables, populations (e.g. with dementia [4, 8], without dementia [5, 11]), geographical locations (e.g. German [1], Singapore [12], Norway [15]) and sample sizes (e.g. n = 210 [1] or n = 7000 [9]) used in the determination of predictors of NHA. </plain></SENT>
<SENT sid="29" pm="."><plain>The commonly recognized risk factors include advanced age, functional and cognitive impairments, depression, caregiver burden, use of health services, prior hospitalization or nursing home use and dementia. </plain></SENT>
<SENT sid="30" pm="."><plain>In a literature review [5] and meta-analysis [16], the strongest predictors of NHA were increased age, low self-rated health status, functional and cognitive impairment, dementia and prior NHA. </plain></SENT>
</text></p><p><text><SENT sid="31" pm="."><plain>The above research has focused on finding risk factors for NHA. </plain></SENT>
<SENT sid="32" pm="."><plain>However, as far as we know, no prior research has proved the prediction performance or accuracy of a NHA model for individuals. </plain></SENT>
<SENT sid="33" pm="."><plain>The prior research articulates the statistically important variables that increase or decrease the risk of NHA at the population level. </plain></SENT>
<SENT sid="34" pm="."><plain>It is based on the traditional statistical data processing approach in which statistical modeling connects data to a population of interest. </plain></SENT>
<SENT sid="35" pm="."><plain>It does not answer the question of how accurately the nursing home admission is possible to predict for individuals. </plain></SENT>
</text></p><p><text><SENT sid="36" pm="."><plain>In this study, we point out the most important variable subsets of different sizes for predicting NHA. </plain></SENT>
<SENT sid="37" pm="."><plain>Particularly, we measure and validate the performance of our NHA prediction models in terms of classification accuracy. </plain></SENT>
<SENT sid="38" pm="."><plain>That is, we search the best model and measure how good it is for individuals. </plain></SENT>
<SENT sid="39" pm="."><plain>The variable subsets were searched by machine learning (ML) methods and the classification accuracy was calculated using the cross-validation principle. </plain></SENT>
<SENT sid="40" pm="."><plain>The data was consisted of the service records of home care clients in the city of Tampere1, Finland. </plain></SENT>
<SENT sid="41" pm="."><plain>Because our data set is highly unbalanced, we use a random operator to form balanced data sets and the all performance results are reported on those balanced data sets instead of the original unbalanced set. </plain></SENT>
</text></p><p><text><SENT sid="42" pm="."><plain>We claim, that the knowledge of classification accuracy is highly valuable, when deciding on the adoption of the prediction model in actual service production. </plain></SENT>
<SENT sid="43" pm="."><plain>It should be noted, that statistically significant variables do not guarantee high classification accuracy. </plain></SENT>
<SENT sid="44" pm="."><plain>Without adequate accuracy, the cost effectiveness of the targeted interventions is not good enough: interventions are targeted to a significant number of people not at risk (“false positive”), and some of those in need of an intervention do not receive one (“false negatives”). </plain></SENT>
<SENT sid="45" pm="."><plain>Furthermore, the resource planning of service production benefits from the individual predictions of upcoming admissions. </plain></SENT>
<SENT sid="46" pm="."><plain>The primary contributions of this paper are summarized below: As far as we know, no prior research has investigated variable subsets of different sizes for predicting NHA. </plain></SENT>
<SENT sid="47" pm="."><plain>A few scholars have applied variable selection methods [3–5, 13]. </plain></SENT>
<SENT sid="48" pm="."><plain>However, they did not investigate the variable subsets of different sizes (1−n variables), as we did in this study.The second contribution relates to the way to use, train and validate classification algorithms for predicting NHA. </plain></SENT>
<SENT sid="49" pm="."><plain>Compared to prior research work, the present study investigates the NHA prediction models for individuals. </plain></SENT>
<SENT sid="50" pm="."><plain>Prior research investigated statistical significant population-level risk factors for NHA. </plain></SENT>
<SENT sid="51" pm="."><plain>The 5% level of significance was a de facto standard for important variables. </plain></SENT>
<SENT sid="52" pm="."><plain>In this study, we measure classification accuracy for classifiers trained and validated using cross-validation. </plain></SENT>
<SENT sid="53" pm="."><plain>That is, we study the accuracy of our model for unseen clients of home care according to the risk of NHA. </plain></SENT>
</text></p><p><text><SENT sid="54" pm="."><plain>The objective of this study was to gain a better understanding of the accuracy level in which NHA can be predicted in order to support decision making in home care services and allocation of resources between customers. </plain></SENT>
<SENT sid="55" pm="."><plain>The classification accuracy of our method was 78% that was high enough for the decision to integrate it in the local information system of home caring2. </plain></SENT>
<SENT sid="56" pm="."><plain>The remainder of this paper is divided into three parts. </plain></SENT>
<SENT sid="57" pm="."><plain>In the first part, we describe how the variables of our prediction models were aggregated and how the variables were selected for the subset selection process. </plain></SENT>
<SENT sid="58" pm="."><plain>The second part introduces the methods for training and validating classifier algorithms. </plain></SENT>
<SENT sid="59" pm="."><plain>The third part of the paper presents the performance of the variable selection and discusses the results and practical implications. </plain></SENT>
</text></p></sec></SecTag><SecTag type="METHODS"><sec id="Sec2"><title><text><SENT sid="60" pm="."><plain>Methods </plain></SENT>
</text></title><sec id="Sec3"><title><text><SENT sid="61" pm="."><plain>Data source </plain></SENT>
</text></title><p><text><SENT sid="62" pm="."><plain>The data consisted of the records of 7259 home care customers between January 2011 and September 2015 in the city of Tampere, Finland. </plain></SENT>
<SENT sid="63" pm="."><plain>These data were linked to records that contained information regarding all social and health care service usage during the same period. </plain></SENT>
<SENT sid="64" pm="."><plain>Nursing home admission (model outcome) was indicated by whether the customer admitted to a nursing home or not, and coded as a binary indicator. </plain></SENT>
<SENT sid="65" pm="."><plain>The data were linked on the customer level using unique encrypted identifiers. </plain></SENT>
<SENT sid="66" pm="."><plain>We excluded clients with recorded home care episode shorter than 12 months between January 2011 and September 2015 (n=3192) and those whose RAI-HC (Resident Assessment Instrument - Home Care [17]) values (n=981) were missing. </plain></SENT>
<SENT sid="67" pm="."><plain>In total, we had 3056 customers (539 NHA is “true” and 2544 NHA is “false”) for analysis. </plain></SENT>
</text></p><p><text><SENT sid="68" pm="."><plain>All the variables were calculated 3–12 months before the evaluation day t ev. </plain></SENT>
<SENT sid="69" pm="."><plain>In addition, the variables were calculated 6–12 months before the day t ev for additional analyses. </plain></SENT>
<SENT sid="70" pm="."><plain>The main variables are listed in the column “variable” of Table 1. </plain></SENT>
<SENT sid="71" pm="."><plain>The variables were selected by the experts of elderly care services. </plain></SENT>
<SENT sid="72" pm="."><plain>Figure 1 shows a time scale in which t s,i is the starting day and t e,i the ending day of home care according to the home care service data for customer i (i=1,…,3056). </plain></SENT>
<SENT sid="73" pm="."><plain>The variables were the numbers of events or boolean value [t r u e/f a l s e] that an event occurred between times t k and t k+1 (k=1,2,3) or t 1 and t 4. </plain></SENT>
<SENT sid="74" pm="."><plain>For example, variable j (a blue box in Fig. 1) for customer i was calculated from time period t 2,ij−t 3,ij. </plain></SENT>
<SENT sid="75" pm="."><plain>The interval between times t 1 and t ev was set to be 12 months and t 1&lt;t 2&lt;t 3&lt;t 4&lt;t ev. </plain></SENT>
<SENT sid="76" pm="."><plain>If NHA variable was “true” for the customer i, that is the customer i was admitted to nursing home at time t e,i, time t ev,i was set to be the admission day (→ t ev,i=t e,i). </plain></SENT>
<SENT sid="77" pm="."><plain>If the NHA variable of customer i was “false”, then time t ev,i was a random day between times t s,i and t e,i, st. t ev,i&gt;t s,i+12 months. Fig. 1Variables were calculated in time periods of t 1 - t 4, when t s - t e is the time period of home care for a customer. </plain></SENT>
<SENT sid="78" pm="."><plain>Figure shows time scale of starting day and ending day of home care for customer i from which the variables of the models were derived Table 1The characteristics of the study sample (means / %) with and without a nursing home admission and the results of t-tests of significance difference between the means of continuous values or categorical variablesVariableTime interval / descriptionNH admissionNo NH admission p-valueAge (mean)84.4481.76&lt;.0001Number of Emergency care visits (mean)3-6 months0.720.35&lt;.00016-9 months0.780.38&lt;.00019-12 months0.610.37&lt;.0001Number of emergency care visits, change (mean)3-6 months vs. 6-9 months-0.06-0.03.62796-9 months vs. 9-12 months0.180.01.0049Number of periods of care (mean)3-6 months0.980.36&lt;.00016-9 months0.860.32&lt;.00019-12 months0.530.33.0002Number of periods of care, change (mean)3-6 months vs. 6-9 months0.120.03.31216-9 months vs. 9-12 months0.33-0.01&lt;.0001Number of home care visits (mean)3-6 months131.5196.30&lt;.00016-9 months142.8093.42&lt;.00019-12 months130.1089.26&lt;.0001Number of home care visits, change (mean)3-6 months vs. 6-9 months-11.282.88.00026-9 months vs. 9-12 months12.704.16.0118Number of outpatient visits in3-6 months1.891.11&lt;.0001specialised care by appointment (mean)6-9 months1.771.14&lt;.00019-12 months1.581.16.0001Number of outpatient visits in specialised3-6 months vs. 6-9 months0.12-0.03.1410care by appointment, change (mean)6-9 months vs. 9-12 months0.19-0.02.0588Number of physiotherapy visits at home (mean)3-12 months0.470.49.9006Number of outpatient visits in geriatrics (mean)3-12 months1.56.62&lt;.0001Number of physician visits at home (mean)3-12 months0.570.49.1316RAI-HC (mean)a CPS1.500.92&lt;.0001IADL13.189.34&lt;.0001PAIN0.720.77.2278MAPLE4.063.14&lt;.0001Customer of support service (%)Safety phone41%25%&lt;.0001meals-on-wheels47%28%&lt;.0001shopping38%27%&lt;.0001Cleaning10%10%.9058transportation29%15%&lt;.0001Day center30%15%&lt;.0001Support for informal care5%5%.9691Home rehabilitation4%3%.3683Outpatient visit in specialised care (%)Surgery / neurosurgery29%13%&lt;.0001Internal medicine40%22%&lt;.0001Obstetric3%2%.0367Neurology17%9%&lt;.0001Respiratory medicine4%4%.8502Ophthalmology12%7%.0004Phoniatrics14%5%&lt;.0001Psychiatry14%9%.0004Period of care in specialised care (%)Surgery / neurosurgery20%8%&lt;.0001Internal medicine30%16%&lt;.0001Obstetric0%0%.5341Neurology10%3%&lt;.0001Respiratory medicine2%2%.5684Ophthalmology1%0%.0028Phoniatrics1%0%.1042Psychiatry8%2%&lt;.0001Intensive care unit1%0%.0211Diagnosis (%)a00-a094%2%&lt;.0001a30-a494%3%.1679e00-e073%2%0.0723e10-e149%6%.0037e70-e905%2%&lt;.0001f00-f0350%16%&lt;.0001f04-f094%2%.0006f10-f192%1%.3515f20-f292%2%.7376f30-f396%4%.0123g20-g265%2%.0001g30-g3232%6%&lt;.0001g40-g473%2%.0972i10-i1528%12%&lt;.0001i20-i2514%6%&lt;.0001i30-i5225%13%&lt;.0001i60-i699%4%&lt;.0001i70-i793%1%.0023i80-i892%1%.1083i95-i995%1%&lt;.0001j09-j186%3%.0136j20-j223%2%.0437j40-j475%3%.1991k55-k635%2%&lt;.0001m05-m142%2%.7376m15-m195%2%.0005m45-m492%1%.0008m50-m544%2%.0933m70-m793%2%.0978m80-m854%2%.0008n10-n164%2%.0324n17-n195%2%.0001n30-n3915%5%&lt;.0001n40-n512%1%.0813r00-r092%2%.9037r10-r194%2%.0034r40-r465%2%.0012r50-r6910%4%&lt;.0001s00-s098%2%&lt;.0001s30-s392%1%.0115s40-s492%1%.3970s70-s795%2%&lt;.0001s80-s892%1%.1098z00-z135%3%.1123 aResident Assessment Instrument for Home Care (RAI-HC). </plain></SENT>
<SENT sid="79" pm="."><plain>The Cognitive Performance Scale (CPS) uses items on memory and communication skills to create a 7-point scale from 0 (intact) to 6 (very seve re) [35]. </plain></SENT>
<SENT sid="80" pm="."><plain>The Instrumental Activities of Daily Living (IADL) scale [36] provides a measure of the customer’s self-performance of seven daily tasks: meal preparation, ordinary housework, managing finances, managing medications, phone use, shopping and transportation. </plain></SENT>
<SENT sid="81" pm="."><plain>The scores are from 0 to 21. </plain></SENT>
<SENT sid="82" pm="."><plain>The Method for Assigning Priority Levels (MAPLe) differentiates customers into five different groups ranging from low to very high risk of health decline [34]. </plain></SENT>
<SENT sid="83" pm="."><plain>Higher risk group indicates a higher risk to be admitted to a long-term care facility </plain></SENT>
</text></p><p><text><SENT sid="84" pm="."><plain>Table 1 presents general characteristics of the study sample (n=3056), of which 539 (17.6%) were admitted to nursing home. </plain></SENT>
<SENT sid="85" pm="."><plain>The table includes the results of t-test of significance difference for continuous variables and chi-squared test for categorical variables between the groups of home caring customers and nursing home residents. </plain></SENT>
</text></p></sec><sec id="Sec4"><title><text><SENT sid="86" pm="."><plain>Variable subset selection </plain></SENT>
</text></title><p><text><SENT sid="87" pm="."><plain>The aim of this study was to find efficient variable subsets X sub={x i|i=1,…n} from a large variable set X={x j|j=1,…k} for predicting the NHA when n and k are the numbers of variables and n&lt;k. </plain></SENT>
<SENT sid="88" pm="."><plain>Let Y be the binary vector of NHA variable, F(·) is a classifier and Y=F(X sub). </plain></SENT>
<SENT sid="89" pm="."><plain>That is, we predict the state of Y i for customer i at time t ev,i (Fig. 1), when the variable vector X sub,i is calculated from time range t 1,i−t 4,i (3–12 months before t ev,i) or t 1,i−t 3,i (6–12 months before t ev,i). </plain></SENT>
</text></p><p><text><SENT sid="90" pm="."><plain>Variable selection is a mature research topic and has been used for many applications [18]. </plain></SENT>
<SENT sid="91" pm="."><plain>In this study we applied sequential forward selection (SFS) method [19] for variable subset generation. </plain></SENT>
<SENT sid="92" pm="."><plain>SFS starts with an empty set and adds one variable at a time from the original set X for classifier by maximizing the performance measure. </plain></SENT>
<SENT sid="93" pm="."><plain>Our primary performance metric was classification accuracy: 1\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document} $$ acc=\frac{1}{n_{samples}} \sum_{i=1}^{n_{samples}} L\left(y_{pred,i}=y_{i}\right) $$ \end{document}acc=1nsamples∑i=1nsamplesLypred,i=yi </plain></SENT>
</text></p><p><text><SENT sid="94" pm="."><plain>where y pred,i is the predicted NHA class of the i-th sample, y i is the corresponding true NHA class, n samples is the number of samples and L(·) is the indicator function (L=1 if y pred=y; L=0 if y pred≠y) [20]. </plain></SENT>
<SENT sid="95" pm="."><plain>We additionally calculated the average area under the curve (AUC) and true-positive rate (recall) values for classifiers. </plain></SENT>
<SENT sid="96" pm="."><plain>AUC values correlated almost perfectly with acc values, but we decided to report them, because in some research areas they are more familiar than the acc values. </plain></SENT>
<SENT sid="97" pm="."><plain>Recall of a classifier is calculated by dividing the correctly classified positives (true positives) by the total positive count (true positives + false negatives) [21]. </plain></SENT>
<SENT sid="98" pm="."><plain>That is, recall is the probability that a risk customer is found. </plain></SENT>
</text></p><p><text><SENT sid="99" pm="."><plain>The strength of the accuracy metric, compared to the other common metrics, is that the accuracy metric is easy to understand. </plain></SENT>
<SENT sid="100" pm="."><plain>It should be noted, that our data set is highly unbalanced. </plain></SENT>
<SENT sid="101" pm="."><plain>We use a random operator to form balanced data sets and the performance results are reported on those balanced data sets instead of the original set. </plain></SENT>
<SENT sid="102" pm="."><plain>Otherwise, the accuracy metric would be biased and not suitable. </plain></SENT>
</text></p><p><text><SENT sid="103" pm="."><plain>An alternative of SFS would be sequential backward elimination (SBE). </plain></SENT>
<SENT sid="104" pm="."><plain>SBE starts with X and eliminates one variable at a time by maximizing the performance measure. </plain></SENT>
<SENT sid="105" pm="."><plain>Our selection of SFS instead of the SBE method is justified by the ratio of relevant (# r) and all (k) variables. </plain></SENT>
<SENT sid="106" pm="."><plain>According to Liu et al. [18], if # r is small, then the SFS strategy should be used, and if the number of irrelevant variables (k−# r) is small, then the SBE strategy should be used. </plain></SENT>
<SENT sid="107" pm="."><plain>According to the pre-tests, the original variable set X includes many irrelevant variables (low univariate prediction power); thus, we prefer the SFS strategy. </plain></SENT>
</text></p><p><text><SENT sid="108" pm="."><plain>Different classifiers have different performance for different data sets. </plain></SENT>
<SENT sid="109" pm="."><plain>In this study, we evaluated the performance of three classifiers: logistic regression (LR) [22], Gaussian naive Bayes (GNB) [23] and support vector classifier (SVC) [24]. </plain></SENT>
<SENT sid="110" pm="."><plain>That is, SFS was run three times using the classifiers of LR, GNB or SVC. </plain></SENT>
<SENT sid="111" pm="."><plain>Figure 2 shows the components of variable subset selection process. </plain></SENT>
<SENT sid="112" pm="."><plain>The subset generation component (SFS) feeds candidate variable subset X sub to subset evaluation component. </plain></SENT>
<SENT sid="113" pm="."><plain>Evaluation component trains and validates classifier and calculates the accuracy values for the subset X sub. Fig. 2The framework of variable subset selection used in this study. </plain></SENT>
<SENT sid="114" pm="."><plain>The subset generation component feeds candidate variable subset to subset evaluation component. </plain></SENT>
<SENT sid="115" pm="."><plain>Evaluation component trains and validates classifier and calculates the accuracy values for the subset </plain></SENT>
</text></p><p><text><SENT sid="116" pm="."><plain>It should be noted, that we use a random operator to form balanced data sets for the analyses. </plain></SENT>
<SENT sid="117" pm="."><plain>Let A={X|Y=1} and B={X|Y=0} be the data sets. </plain></SENT>
<SENT sid="118" pm="."><plain>That is, the set A contains the data of customers with the values of “true” of NHA variable and the B with “false”. </plain></SENT>
<SENT sid="119" pm="."><plain>Because the set A is smaller (n=539) than the set B (n=2517), the balanced data set C was formed, st. C=A∪R(B) where R is a random operator for selecting 539 random samples from B, thus setting the level of chance at 50%. </plain></SENT>
<SENT sid="120" pm="."><plain>To be sure that the selection did not bias the results, data set C was formed 100 times. </plain></SENT>
</text></p><p><text><SENT sid="121" pm="."><plain>Furthermore, classifier algorithms were trained and validated using a ten-fold cross validation method. </plain></SENT>
<SENT sid="122" pm="."><plain>That is, we formed sample C i (i=1,…,100) from the data sets A and B, and split it into 10 equal-sized parts P ik (P ik∈C i and k=1,…10). </plain></SENT>
<SENT sid="123" pm="."><plain>The classification accuracy value, a c c ik, was calculated by Eq. 1 for the part P ik of the data set C i when the parameters of the classifier were trained with the other K-1 parts of the data set C i. </plain></SENT>
<SENT sid="124" pm="."><plain>The process was repeated for k=1,2,…10. </plain></SENT>
<SENT sid="125" pm="."><plain>The overall classification accuracy, CA, for the subset X sub,n of size n (n=1,…15) was calculated as 2\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document} $$ CA_{\textbf{X}_{sub,n}}=\frac{1}{100*K} \sum_{i=1}^{100} \sum_{K=1}^{10} acc_{ik} $$ \end{document}CAXsub,n=1100∗K∑i=1100∑K=110accik </plain></SENT>
</text></p><p><text><SENT sid="126" pm="."><plain>SFS calculated the best variable subsets for all balanced data sets C i. </plain></SENT>
<SENT sid="127" pm="."><plain>That is, we have 100 variable subsets of size of 1–15 variables. </plain></SENT>
<SENT sid="128" pm="."><plain>The (average) importance of each variable was measured by a rank metric: 3\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document} $$ R(j)=\frac{1}{100} \sum_{i=1}^{100} \#F-r(i,j) $$ \end{document}R(j)=1100∑i=1100#F−r(i,j) </plain></SENT>
</text></p><p><text><SENT sid="129" pm="."><plain>where r(i,j) is the rank of variable j based on sample C i and # F is the size of the largest subset that was formed by SFS [25–27]. </plain></SENT>
<SENT sid="130" pm="."><plain>In this study # F=15. </plain></SENT>
<SENT sid="131" pm="."><plain>Higher R(j) indicates that variable j is more important according to SFS, because it was selected for smaller size variable subsets. </plain></SENT>
<SENT sid="132" pm="."><plain>That is, variable has higher prediction capability according to SFS and its NHA classification ability is high. </plain></SENT>
</text></p></sec><sec id="Sec5"><title><text><SENT sid="133" pm="."><plain>Software </plain></SENT>
</text></title><p><text><SENT sid="134" pm="."><plain>We used four Python packages: sklearn [20], mlxtend [28], numpy [29] and pandas [30] to implement the classifiers and compute, acc, AUC, recall, C A Xsub and R(j). </plain></SENT>
<SENT sid="135" pm="."><plain>SFS was computed by the function “SequentialFeatureSelector” in the package mlxtend. </plain></SENT>
<SENT sid="136" pm="."><plain>The classifiers of LR, SVC and GBN were implemented using the functions from the sklearn.linear_model, sklearn.svm and sklearn.naive_bayes packages. </plain></SENT>
<SENT sid="137" pm="."><plain>The packages of numpy and pandas were used for data reading and processing. </plain></SENT>
</text></p></sec></sec></SecTag><SecTag type="RESULTS"><sec id="Sec6" sec-type="results"><title><text><SENT sid="138" pm="."><plain>Results </plain></SENT>
</text></title><p><text><SENT sid="139" pm="."><plain>Figure 3 shows the performance (average classification accuracy) of the feature subsets found by different classifiers as a function of the subset size when the variables were calculated 3–12 months before the evaluation day t ev. </plain></SENT>
<SENT sid="140" pm="."><plain>The average classification accuracy values were determined by Eq. 2. </plain></SENT>
<SENT sid="141" pm="."><plain>The classifiers of LR, SVC and GNB had the average accuracies of 0.776 (C I95%=.0025), 0.762 (C I95%=.0026) and 0.776 (C I95%=.0024), respectively, for the variable subset of 15 variables. </plain></SENT>
<SENT sid="142" pm="."><plain>According to the student’s t-tests [31], the average classification accuracy value of LR and GNB methods with 15 variables differed statistically from the SVC method: LR vs. SVC (p&lt;.0001) and GNB vs. SVC (p&lt;.0001). Fig. 3Average accuracy as a function of the size of variable subset. </plain></SENT>
<SENT sid="143" pm="."><plain>Figure shows the classification accuracy of the feature subsets found by different classifiers as a function of the subset size. </plain></SENT>
<SENT sid="144" pm="."><plain>The average classification accuracy values of LR and GNB methods differ from the SVC method </plain></SENT>
</text></p><p><text><SENT sid="145" pm="."><plain>In addition, we calculated average AUC and recall values for the classifiers. </plain></SENT>
<SENT sid="146" pm="."><plain>The AUC values were 0.846 (C I95%=.0025), 0.838 (C I95%=.0025) and 0.847 (C I95%=.0024) for the classifiers of LR, SVC and GNB with 15 variables. </plain></SENT>
<SENT sid="147" pm="."><plain>The recall values were 0.755 (C I95%=.0015), 0.724 (C I95%=.0018) and 0.756 (C I95%=.0018). </plain></SENT>
<SENT sid="148" pm="."><plain>An AUC of 0.5 indicates no discrimination above chance and an AUC of 1.0 indicates perfect classification. </plain></SENT>
<SENT sid="149" pm="."><plain>A rough guide for the classification ability is AUC 0.9−1.0 excellent, AUC 0.8−0.9 good, AUC 0.7−0.8 fair and AUC 0.6−0.7 poor [32]. </plain></SENT>
<SENT sid="150" pm="."><plain>In general, classification ability is useful if AUC &gt;0.75 [33]. </plain></SENT>
<SENT sid="151" pm="."><plain>That is, the performance of the classifiers with 15 variables was at good level. </plain></SENT>
</text></p><p><text><SENT sid="152" pm="."><plain>When the variables were calculated 6–12 months before the evaluation day t ev, the average accuracy of classifiers of LR, SVC and GNB were 0.747 (C I95%=.0030), 0.737 (C I95%=.0029) and 0.734 (C I95%=.0029), respectively, for the variable subset of 15 variables. </plain></SENT>
<SENT sid="153" pm="."><plain>The AUC values were 0.819 (C I95%=.0027), 0.810 (C I95%=.0028) and 0.813 (C I95%=.0025). </plain></SENT>
<SENT sid="154" pm="."><plain>The recall values were 0.732 (C I95%=.0017), 0.738 (C I95%=.0025) and 0.732 (C I95%=.0026). </plain></SENT>
<SENT sid="155" pm="."><plain>The results of the 6–12 months variables show a moderate decrease in performance compared to the 3–12 months variables (e.g. LR CA: 0.776 → 0.747). </plain></SENT>
<SENT sid="156" pm="."><plain>The performance of the classifiers with the 6–12 months variables, however, is still at good level (A U C&gt;0.8). </plain></SENT>
</text></p><p><text><SENT sid="157" pm="."><plain>Figure 4 shows the p-values calculated by the student’s t-test when the average classification accuracy values for the subsets of 15 variables of 3–12 months were compared to the subsets of n variables (n=1,…15). </plain></SENT>
<SENT sid="158" pm="."><plain>We defined that if p&lt;.05, the difference between the performances of variable subsets is statistically significant. </plain></SENT>
<SENT sid="159" pm="."><plain>According to the definition, the optimal subset size for LR method was 9 variables. </plain></SENT>
<SENT sid="160" pm="."><plain>That is, the performance achieved by the subset size of 9 variables did not differ statistically from the subset of 15 variables, when the classifier was LR. Fig. 4 P-value as a function of the size of variable subset compared to the subsets of 15 variables. </plain></SENT>
<SENT sid="161" pm="."><plain>We defined that if p&lt;.05, the difference between the performances of variable subsets is statistically significant. </plain></SENT>
<SENT sid="162" pm="."><plain>According to the definition, the optimal subset size for LR method is 9 variables. </plain></SENT>
<SENT sid="163" pm="."><plain>That is, the performance achieved by the subset size of 9 variables did not differ statistically from the subset of 15 variables, when the classifier is LR </plain></SENT>
</text></p><p><text><SENT sid="164" pm="."><plain>Table 2 sorts the variables according to ranking score, R, described by Eq. 3, for the classifiers of LR and GNB. </plain></SENT>
<SENT sid="165" pm="."><plain>Large R(j) value means that the variable j was selected regularly in small variable subsets for different balanced data sets C. </plain></SENT>
<SENT sid="166" pm="."><plain>That is, the NHA classification ability of the variable j is high. </plain></SENT>
<SENT sid="167" pm="."><plain>According to the results, the most important variables were the diagnoses of G30-G32 and F00-F03 and the RAI metrics of IADL (Activities of Daily Living), MAPLE (Method for Assigning Priority Levels) and CPS (Cognitive Performance Scale). </plain></SENT>
<SENT sid="168" pm="."><plain>In addition, variables related to the numbes of periods of care were important variables for predicting NHA with the both classifiers. </plain></SENT>
<SENT sid="169" pm="."><plain>It should be noted, that the RAI variables (IADL, MAPLE and CPS) are not simple measurements or observations, but instead scoring systems developed by researcher and practitioners (e.g., MAPLE [34], CPS [35] and IADL [36]). </plain></SENT>
<SENT sid="170" pm="."><plain>That is, it is not surprising that these variables have such high performance at predicting NHA. Table 2The 10 variables of the highest ranking score values calculated for the LR and GNB classifiers (the important variables for the both classifiers are marked as stars)#LR classifier: VariablesRanking score1**Diagnosis F00-F03147.92**Diagnosis G30-G32105.43Number of periods of care (6-9 months)99.94**RAI IADL85.55**RAI CPS73.96**RAI MAPLE553.97Number of Emergency care visits (3-6 months)37.48Diagnosis N30-N3933.89Diagnosis M15-M1926.610Number of periods of care (3-6 months)26.1#GNB classifier: VariablesRanking score1**Diagnosis F00-F03147.22**RAI IADL106.93**Diagnosis G30-G3284.24Number of home care visits, change (3-6 months vs. 6-9 months)845Number of periods of care, change (3-6 months vs. 6-9 months)78.76**RAI CPS77.97**RAI MAPLE568.88RAI PAIN57.99Specialised care by appointment (6-9 months)5710Specialised care by appointment (3-6 months)44.9 </plain></SENT>
</text></p><p><text><SENT sid="171" pm="."><plain>Figures 5 and 6 plot the normalized ranking score values for the classifiers of SVC and GNB as a function of the values of LR. </plain></SENT>
<SENT sid="172" pm="."><plain>Ten variables with the highest R values of LR classifier are labelled on the figures. </plain></SENT>
<SENT sid="173" pm="."><plain>The 45° identity line visualizes the differences between the R values of the classifiers. </plain></SENT>
<SENT sid="174" pm="."><plain>Variables in the lower-right region of the line were more important for the LR than for the SVC (Fig. 5) or for the GNB (Fig. 6). </plain></SENT>
<SENT sid="175" pm="."><plain>Similarly, those in the upper-left region were more important for the SVC (Fig. 5) or for the GNB (Fig. 6) than for the LR. </plain></SENT>
<SENT sid="176" pm="."><plain>For example, the diagnosis N30-N39 was more important for the SVC classifier than for the LR. </plain></SENT>
<SENT sid="177" pm="."><plain>However, the differences between the most important variables of the classifier were rather small. </plain></SENT>
<SENT sid="178" pm="."><plain>The variables of the RAI MAPLE, RAI IADL, RAI CPS and diagnoses F00-F03 and G30-G32 were five important variables for the all classifiers. Fig. 5Normalized ranking score values of the SVC method as a function of the LR method. </plain></SENT>
<SENT sid="179" pm="."><plain>The variables of the RAI MAPLE, RAI IADL, RAI CPS and diagnoses F00-F03 and G30-G32 were five important variables for the both classifiers Fig. 6Normalized ranking score values of the GNB method as a function of the LR method. </plain></SENT>
<SENT sid="180" pm="."><plain>The variables of the RAI MAPLE, RAI IADL, RAI CPS and diagnoses F00-F03 and G30-G32 were five important variables for the both classifiers </plain></SENT>
</text></p></sec></SecTag><SecTag type="DISCUSS"><sec id="Sec7" sec-type="discussion"><title><text><SENT sid="181" pm="."><plain>Discussion </plain></SENT>
</text></title><p><text><SENT sid="182" pm="."><plain>The aim of the study was to analyse predictors and find out efficient variable subsets to predict NHA in a sample of home caring customers. </plain></SENT>
<SENT sid="183" pm="."><plain>Particularly, we wanted to find and report the level of accuracy in which NHA can be predicted for individuals. </plain></SENT>
<SENT sid="184" pm="."><plain>Our results show that the admission of nursing home can be predicted at an accuracy level of 78% / 74% when the variables were calculated 3–12 months / 6–12 months before the evaluation day. </plain></SENT>
<SENT sid="185" pm="."><plain>Thus, on average, our model predicts four out of five or three out of four home care customers in the right class in terms of nursing home admission. </plain></SENT>
<SENT sid="186" pm="."><plain>This is crucial information for decision makers for two reasons. </plain></SENT>
<SENT sid="187" pm="."><plain>Firstly, the model has to be accurate enough so that investments in preventive interventions can be made. </plain></SENT>
<SENT sid="188" pm="."><plain>If the accuracy of the model is too low, there are too many false positives and the cost effectiveness of the interventions is low. </plain></SENT>
<SENT sid="189" pm="."><plain>Secondly, the model needs to predict the individuals with high risk well in advance of the admission. </plain></SENT>
<SENT sid="190" pm="."><plain>Otherwise, it is too late to implement any interventions. </plain></SENT>
<SENT sid="191" pm="."><plain>Therefore, the fact that the accuracy of our model with variables 6–12 months before the evaluation day is as high as 74%, is important. </plain></SENT>
</text></p><p><text><SENT sid="192" pm="."><plain>As far as we know, no prior research has published the classification accuracy of the NHA model for individuals. </plain></SENT>
<SENT sid="193" pm="."><plain>It should be noted, that the classification accuracy is a very common metric in machine learning and other fields. </plain></SENT>
<SENT sid="194" pm="."><plain>However, prior research has done the analyses at the population level. </plain></SENT>
<SENT sid="195" pm="."><plain>The important variables have been detected using the 5% level of significance. </plain></SENT>
<SENT sid="196" pm="."><plain>That is, the values of the parameters of a model (e.g. linear regression (e.g. [12]), logistic regression (e.g. [9, 14]) or Cox model (e.g. [4, 5])) are estimated from whole data (without the split of train and test sets) and the significance levels for coefficients are derived. </plain></SENT>
<SENT sid="197" pm="."><plain>Nothing else has done to see if the model generalizes on the data and individuals that played no role in estimating the parameters for models. </plain></SENT>
<SENT sid="198" pm="."><plain>Few scholars of NHA (e.g. [2, 12]) have applied goodness-of-fit tests (e.g. AIC, R 2) for the model, but the test results were often more close to zero than one (≈.20−.25). </plain></SENT>
</text></p><p><text><SENT sid="199" pm="."><plain>We see that the above lacks in NHA research are related to the public health science and data modelling cultures, in which model validation is omitted or calculated only on training data [37]. </plain></SENT>
<SENT sid="200" pm="."><plain>In this study we searched the important variables by averaging the results of variable selection that was executed for many random split of the whole data set. </plain></SENT>
<SENT sid="201" pm="."><plain>The importance of variables was measured by the ranking metric. </plain></SENT>
<SENT sid="202" pm="."><plain>The level of classification accuracy of model for different variable subsets was tested by cross-validation. </plain></SENT>
<SENT sid="203" pm="."><plain>The variable selection from many random data samples and cross-validation warrants the generalization of our variables and models. </plain></SENT>
</text></p><p><text><SENT sid="204" pm="."><plain>The variables of RAI MAPLE, functional impairment (RAI IADL), cognitive impairment (RAI CPS), memory disorders (G30-G32 and F00-F03) and the use of community-based health services and prior hospital use (emergency visits and periods of care) were the most important. </plain></SENT>
<SENT sid="205" pm="."><plain>The ICD10 (International Classification of Diseases) group of G30-G32 contains the codes for other degenerative diseases of the nervous system (e.g. Alzheimer) and F00-F03 for dementia. </plain></SENT>
<SENT sid="206" pm="."><plain>A comparison of our results with the findings of the other investigations revealed that especially, functional [1–3, 5–9, 11, 13, 14] and cognitive [2, 5, 8, 9, 11, 14] impairment, dementia [1, 3, 13, 14] and use of community-based health services [2, 4] or prior hospitalization [9] were also strong predictors of NHA. </plain></SENT>
<SENT sid="207" pm="."><plain>In contrast to our findings, [2, 4–6, 9, 13, 14] found that increased age lead to increased risk of NHA. </plain></SENT>
<SENT sid="208" pm="."><plain>In our study, the importance of variable of age was rather low according to the ranking score. </plain></SENT>
</text></p><p><text><SENT sid="209" pm="."><plain>The major strengths of this study include its detailed assessment of important variables and model validation and availability of a range of important variables for nursing home admission. </plain></SENT>
<SENT sid="210" pm="."><plain>The accuracy of the model was high enough to convince the officials of the city of Tampere to integrate the predictive model as a part of home care information system. </plain></SENT>
<SENT sid="211" pm="."><plain>However, there are some limitations to the present study. </plain></SENT>
<SENT sid="212" pm="."><plain>We were unable to investigate the associations of social relationships with nursing home admission. </plain></SENT>
<SENT sid="213" pm="."><plain>Some studies have shown that caregiver characteristics [4, 7, 14, 38, 39], having children [8, 9] and marital status [6] can be important factors for NHA. </plain></SENT>
<SENT sid="214" pm="."><plain>Second, this was a study of home caring clients living in a defined geographical location, which may limit the generalizability to older adults living in other areas. </plain></SENT>
<SENT sid="215" pm="."><plain>Also the finding of this study may not be applicable to population without home caring services. </plain></SENT>
<SENT sid="216" pm="."><plain>In addition, many of the evaluated risk variables found in this study, are not modifiable. </plain></SENT>
<SENT sid="217" pm="."><plain>Further work need to be done to evaluate variables that are modifiable and responsive to interventions. </plain></SENT>
</text></p></sec></SecTag><sec id="Sec8"><title><text><SENT sid="218" pm="."><plain>Practical implications </plain></SENT>
</text></title><p><text><SENT sid="219" pm="."><plain>It is clear, that applying ML methods will progress and reform the work of the gerontology researchers and practitioners. </plain></SENT>
<SENT sid="220" pm="."><plain>The benefits can be viewed from the two aspects: 1) ML methods can be used to construct practical computer software for predicting NHA to aid the decision-making of practitioners, 2) large variable groups can be studied and the most important variables can be found. </plain></SENT>
</text></p><p><text><SENT sid="221" pm="."><plain>The aspect (1) contributes most to the work of practitioners, e.g. home care case managers. </plain></SENT>
<SENT sid="222" pm="."><plain>The problem the case managers face is equivalent to that in any preventive care: it is difficult to achieve cost-efficiency if you cannot target a specific subgroup. </plain></SENT>
<SENT sid="223" pm="."><plain>You usually provide a small intervention for everyone, which is not enough for those at high risk. </plain></SENT>
<SENT sid="224" pm="."><plain>In order to be effective, the preventive measure needs to be substantial (e.g. in the case of home care customers, 2000€) but becomes too expensive, if offered for many customers. </plain></SENT>
<SENT sid="225" pm="."><plain>With limited resources one needs to know which customers are most in need of a rehabilitation intervention and target those individuals to maximize cost-effectiveness. </plain></SENT>
</text></p><p><text><SENT sid="226" pm="."><plain>In the case of home care in Tampere, about 17% of customers are admitted to a nursing home within a year, which is the a priori risk for everyone. </plain></SENT>
<SENT sid="227" pm="."><plain>The algorithm produced with ML techniques gives a much more accurate risk value enabling the targeting off interventions. </plain></SENT>
<SENT sid="228" pm="."><plain>Without an accurate prediction algorithm, it is difficult to identify the high risk individuals. </plain></SENT>
<SENT sid="229" pm="."><plain>It is not enough to identify variables that have a statistically significant relationship with NHA, because this does not provide guidelines that can be applied in practice. </plain></SENT>
<SENT sid="230" pm="."><plain>For example, we know that a diagnosis indicating dementia or Alzheimer’s disease increases NHA risk, but this information is not specific enough to identify the individuals in need of an intervention (unless we target everyone with that particular diagnosis). </plain></SENT>
<SENT sid="231" pm="."><plain>The ML algorithm provides a risk classification and also allows for the estimation of the accuracy of the prediction. </plain></SENT>
<SENT sid="232" pm="."><plain>Also, in many cases the case managers need to convince their superiors of the need of investing in rehabilitation interventions for a particular customer. </plain></SENT>
<SENT sid="233" pm="."><plain>The risk estimate from a validated prediction algorithm can be used as a means of communication between the case manager and her superior. </plain></SENT>
</text></p><p><text><SENT sid="234" pm="."><plain>Furthermore, the prediction model can also be used to estimate resource requirements for 24 h services by summing up the individual predictions. </plain></SENT>
<SENT sid="235" pm="."><plain>The predictions provide an upper limit estimation for capacity requirement. </plain></SENT>
<SENT sid="236" pm="."><plain>With time, when data is gathered on by how much targeted rehabilitation interventions can reduce NHA, the capacity estimates become more accurate. </plain></SENT>
</text></p><p><text><SENT sid="237" pm="."><plain>In this study, the city of Tampere integrated a computer software containing the prediction algorithm in their data warehouse. </plain></SENT>
<SENT sid="238" pm="."><plain>The computer software aggregates and processes the variables from different databases and calculates the customer specific NHA risk value. </plain></SENT>
<SENT sid="239" pm="."><plain>If the risk is high, the case managers consider customer specific interventions, e.g. a new service level assessment, more home care visits, a particular therapy or revised medication. </plain></SENT>
<SENT sid="240" pm="."><plain>Prior to the implementation of the prediction algorithm, the rehabilitation interventions were not targeted systematically. </plain></SENT>
<SENT sid="241" pm="."><plain>Most often interventions were used when a care taker or nurse or next of kin noticed a change in functional ability and notified the case manager. </plain></SENT>
<SENT sid="242" pm="."><plain>When using the prediction algorithm, interventions are targeted based on more objective evaluations and customers are screened regularly. </plain></SENT>
<SENT sid="243" pm="."><plain>This way it is possible to identify customers at risk earlier than before. </plain></SENT>
<SENT sid="244" pm="."><plain>Also, after the implementation of the prediction algorithm, the selection of different rehabilitation interventions available for home care customers has been increased. </plain></SENT>
</text></p><p><text><SENT sid="245" pm="."><plain>The next step in the study and implementation project is to gather data from the interventions and their effects, and build another ML model to predict the effectiveness of each intervention for each type of customer. </plain></SENT>
<SENT sid="246" pm="."><plain>Also, the model can be used to predict, who is no longer capable of benefitting from an intervention. </plain></SENT>
<SENT sid="247" pm="."><plain>This added information will further improve the cost-effectiveness of home care. </plain></SENT>
</text></p><p><text><SENT sid="248" pm="."><plain>The aspect (2) contributes both the gerontology research and practical work. </plain></SENT>
<SENT sid="249" pm="."><plain>Variable selection can be used to identify which of the available variables are closely related to the prediction of the NHA and to discard those unrelated to it, reducing the dimensionality of the dataset. </plain></SENT>
<SENT sid="250" pm="."><plain>For the researcher of gerontology, the process of variable selection may indicate new variables that had not been previously considered as relevant to NHA. </plain></SENT>
<SENT sid="251" pm="."><plain>For example, in this study, we found about 10 important variables for predicting NHA. </plain></SENT>
<SENT sid="252" pm="."><plain>Furthermore, the model validity is easier to evaluate after variable selection is used to reduce the dimensionality of the model. </plain></SENT>
<SENT sid="253" pm="."><plain>After dimension reduction, the researchers know the variables for which they should focus in their research [40]. </plain></SENT>
<SENT sid="254" pm="."><plain>For the NHA research, this may mean that the variables for which the interventions should be focused can be found. </plain></SENT>
</text></p><p><text><SENT sid="255" pm="."><plain>The second benefit, because of the variable selection, is that the number of variables, integrated in the software tool, can be minimized. </plain></SENT>
<SENT sid="256" pm="."><plain>This is important, because each new added variable requires resources for the processes of data aggregation and validation and requirements for data integration from different databases. </plain></SENT>
</text></p></sec><SecTag type="CONCL"><sec id="Sec9" sec-type="conclusion"><title><text><SENT sid="257" pm="."><plain>Conclusion </plain></SENT>
</text></title><p><text><SENT sid="258" pm="."><plain>Most elderly people prefer to live at home in a familiar environment than move to a nursing home. </plain></SENT>
<SENT sid="259" pm="."><plain>The findings of our study indicate important variable subsets for predicting NHA of community dwelling home care customers, and offer potential to find those individuals at the level of 78%, who are at risk of NHA. </plain></SENT>
<SENT sid="260" pm="."><plain>The most important variables were RAI MAPLE, functional impairment (RAI IADL), cognitive impairment (RAI CPS), memory disorders (diagnoses G30-G32 and F00-F03) and the use of community-based health-service and prior hospital use. </plain></SENT>
</text></p></sec></SecTag><sec id="Sec10"><title><text><SENT sid="261" pm="."><plain>Endnotes </plain></SENT>
</text></title><p><text><SENT sid="262" pm="."><plain>1 Tampere is the third largest city in Finland. </plain></SENT>
<SENT sid="263" pm="."><plain>The percent of population over 65 years is 18.0% that is approximately same as in the other big cities in Finland (<ext-link ext-link-type="uri" xlink:href="http://www.stat.fi">http://www.stat.fi</ext-link>). </plain></SENT>
<SENT sid="264" pm="."><plain>Also, the scope or services offered for the elderly as well as eligibility criteria for home care and nursing home care are fairly similar in all areas in Finland. </plain></SENT>
</text></p><p><text><SENT sid="265" pm="."><plain>2 Kotitori <ext-link ext-link-type="uri" xlink:href="http://www.tampereenkotitori.fi/">http://www.tampereenkotitori.fi/</ext-link> </plain></SENT>
</text></p></sec></body><back><SecTag type="ABBR"><glossary><title>Abbreviations</title><def-list><def-item><term>AUC</term><def><p>Area under the curve</p></def></def-item><def-item><term>CA</term><def><p>Classification accuracy</p></def></def-item><def-item><term>CPS</term><def><p>Cognitive performance scale</p></def></def-item><def-item><term>GNB</term><def><p>Gaussian naive Bayes</p></def></def-item><def-item><term>IADL</term><def><p>Activities of daily living</p></def></def-item><def-item><term>LR</term><def><p>Logistic regression</p></def></def-item><def-item><term>MAPLE</term><def><p>Method for assigning priority levels</p></def></def-item><def-item><term>ML</term><def><p>Machine learning</p></def></def-item><def-item><term>NHS</term><def><p>Nursing home admission</p></def></def-item><def-item><term>RAI-HC</term><def><p>Resident assessment instrument - home care</p></def></def-item><def-item><term>SBE</term><def><p>Sequential backward elimination</p></def></def-item><def-item><term>SFS</term><def><p>Sequential forward selection</p></def></def-item><def-item><term>SVM</term><def><p>Support vector machine</p></def></def-item></def-list></glossary></SecTag><SecTag type="AUTH_CONT"><ack><title>Acknowledgements</title><p>The authors would like to thanks Mr. Mikko Mulari for his contribution for data acquisition and interpretation.</p><sec id="d29e3873"><title>Funding</title><p>The study was independent of external funding sources.</p></sec><sec id="d29e3878"><title>Availability of data and materials</title><p>The dataset which we have acquired will not be shared as a supplementary file. All Python codes for data analysis and variable subset selection are available upon request.</p></sec><sec id="d29e3883"><title>Authors’ contributions</title><p>MN participated in the data collection, study design, performed the literature review, programming all analyses and wrote the first draft of the paper. RLL contributed to the design of the study, interpretation of the results and was involved in writing of the first draft of the paper. ES and AT participated to the design of the study and revised the paper. VK managed and supervised the study, participated to the design of the study and revised the paper. All authors read and approved the final manuscript.</p></sec><sec id="d29e3888"><title>Competing interests</title><p>MN: employment (Nordic Healthcare Group), RLL and VK: employment and stockholder (Nordic Healthcare Group). Nordic Healthcare Group (NHG) is a Finnish company specialised in planning and developing health and social services especially in Finland, Sweden and Russia. ES and AT declare that they have no competing interests.</p></sec><sec id="d29e3893"><title>Consent for publication</title><p>Not applicable.</p></sec><sec id="d29e3898"><title>Ethics approval and consent to participate</title><p>The data were provided by the city of Tampere and Pirkanmaa Hospital District who granted us the research promises and the privilege to use the data. Data were aggregated and anonymized by the data administration of the city of Tampere before they were provided to the authors. Ethics approval was not required. In Finland, ethics approval is not required for retrospective registry studies with anonymized data. Since this was a retrospective study, consent to participate was not required.</p></sec><sec id="d29e3903"><title>Publisher’s Note</title><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></sec></ack></SecTag><SecTag type="REF"><ref-list id="Bib1"><title>References</title><ref id="CR1"><text><SENT sid="266" pm="."><plain>1HajekABrettschneiderCLangeCPosseltTWieseBSteinmannSWeyererSWerleJPentzekMFuchsASteinJLuckTBickelHMöschEWagnerMJessenFMaierWSchererMRiedel-HellerSGKönigHHGroupASLongitudinal predictors of institutionalization in old agePLoS ONE2015101211110.1371/journal.pone.0144203 </plain></SENT>
</text></ref><ref id="CR2"><text><SENT sid="267" pm="."><plain>2SørbyeLHamranTHenriksenNNorbergAHome care patients in four nordic capitals — predictors of nursing home admission during one-year followupJ Multidiscip Healthc20103111810.2147/JMDH.S8979<?supplied-pmid 21197351?>21197351 </plain></SENT>
</text></ref><ref id="CR3"><text><SENT sid="268" pm="."><plain>3GnjidicDStanawayFCummingRWaiteLBlythFNaganathanVHandelsmanDJLe CouteurDGMild cognitive impairment predicts institutionalization among older men: A population-based cohort studyPLoS ONE2012791810.1371/journal.pone.0046061 </plain></SENT>
</text></ref><ref id="CR4"><text><SENT sid="269" pm="."><plain>4EskaKGraesselEDonathCSchwarzkopfLLauterbergJHolleRPredictors of institutionalization of dementia patients in mild and moderate stages: A 4-year prospective analysisDement Geriatr Cogn Disord Extra2013314264510.1159/000355079 </plain></SENT>
</text></ref><ref id="CR5"><text><SENT sid="270" pm="."><plain>5LuppaMLuckTMatschingerHKönigHHRiedel-HellerSGPredictors of nursing home admission of individuals without a dementia diagnosis before admission - results from the leipzig longitudinal study of the aged (leila 75+)BMC Health Serv Res20101011810.1186/1472-6963-10-18620044945 </plain></SENT>
</text></ref><ref id="CR6"><text><SENT sid="271" pm="."><plain>6AndelRHyerKSlackARisk factors for nursing home placement in older adults with and without dementiaJ Aging Health2007192213810.1177/0898264307299359<?supplied-pmid 17413132?>17413132 </plain></SENT>
</text></ref><ref id="CR7"><text><SENT sid="272" pm="."><plain>7JiskaCPhilipWPredictors of entry to the nursing home: Does length of follow-up matter?Arch Gerontol Geriatr20115333091510.1016/j.archger.2010.12.00921251719 </plain></SENT>
</text></ref><ref id="CR8"><text><SENT sid="273" pm="."><plain>8DraméMLangPJollyDNarbeyDMahmoudiRLanièceISommeDGauvainJHeitzDVoisinTde WazièresBGonthierRAnkriJSaint-JeanOJeandelCCouturierPBlanchardFNovellaJNursing home admission in elderly subjects with dementia: predictive factors and future challengesJ Am Med Dir Assoc2013131720 </plain></SENT>
</text></ref><ref id="CR9"><text><SENT sid="274" pm="."><plain>9AkamigboAWolinskyFReported expectations for nursing home placement among older adults and their role as risk factors for nursing home admissionsGerontologist2006464647310.1093/geront/46.4.464<?supplied-pmid 16921000?>16921000 </plain></SENT>
</text></ref><ref id="CR10"><text><SENT sid="275" pm="."><plain>10SheppardKBrownCHearldKRothDSawyerPLocherJAllmanRRitchieCSSymptom burden predicts nursing home admissions among older adultsJ Pain Symptom Manage201346591710.1016/j.jpainsymman.2012.10.228<?supplied-pmid 23218806?>23218806 </plain></SENT>
</text></ref><ref id="CR11"><text><SENT sid="276" pm="."><plain>11von BonsdorffMRantanenTLaukkanenPSuutamaTHeikkinenEMobility limitations and cognitive deficits as predictors of institutionalization among community-dwelling older peopleGerontology20065263596510.1159/000094985<?supplied-pmid 16905887?>16905887 </plain></SENT>
</text></ref><ref id="CR12"><text><SENT sid="277" pm="."><plain>12ChenCNaidooNErBCheongAFongNPTayCYChanKMTanBYMenonEEeCHLeeKKNgYSTeoYYKohGCHFactors associated with nursing home placement of all patients admitted for inpatient rehabilitation in singapore community hospitals from 1996 to 2005: A disease stratified analysisPLoS ONE201381211110.1371/annotation/dd945f7c-c50b-461d-ab38-15e8b0966458 </plain></SENT>
</text></ref><ref id="CR13"><text><SENT sid="278" pm="."><plain>13WergelandJSelbækGBerghSSoederhamnUKirkevold.Predictors for nursing home admission and death among community-dwelling people 70 years and older who receive domiciliary careDement Geriatr Cogn Disord Extra20155320910.1159/000437382 </plain></SENT>
</text></ref><ref id="CR14"><text><SENT sid="279" pm="."><plain>14SheppardKSawyerPRitchieCAllmanRBrownCLife-space mobility predicts nursing home admission over 6 yearsJ Aging Health2013259072010.1177/0898264313497507<?supplied-pmid 23965310?>23965310 </plain></SENT>
</text></ref><ref id="CR15"><text><SENT sid="280" pm="."><plain>15HelvikASSkanckeRHSelbækGEngedalKNursing home admission during the first year after hospitalization? the contribution of cognitive impairmentPLoS ONE2014911710.1371/journal.pone.0086116 </plain></SENT>
</text></ref><ref id="CR16"><text><SENT sid="281" pm="."><plain>16GauglerJDuvalSAndersonKKaneRPredicting nursing home admission in the u.s: a meta-analysisBMC Geriatr200713114 </plain></SENT>
</text></ref><ref id="CR17"><text><SENT sid="282" pm="."><plain>17MorrisJFriesBBernabeiRSteelKIkegamiNCarpenterIGilgenRDuPasquierJFrijtersDHenrardJHirdesJBelleville-TaylorPBergKBjörkgrenMGrayIHawesCLjunggrenGNonemakerSPhillipsCZimmermanDinterRAI Home Care (HC) Assessment Form and User’s Manual2009USAinterRAI </plain></SENT>
</text></ref><ref id="CR18"><text><SENT sid="283" pm="."><plain>18LiuHYuLToward integrating feature selection algorithms for classification and clusteringIEEE Trans Knowl Data Eng200517449150210.1109/TKDE.2005.66 </plain></SENT>
</text></ref><ref id="CR19"><text><SENT sid="284" pm="."><plain>19HastieTTibshiraniRFriedmanJThe Elements of Statistical Learning2009New YorkSpringer </plain></SENT>
</text></ref><ref id="CR20"><text><SENT sid="285" pm="."><plain>20PedregosaFVaroquauxGGramfortAMichelVThirionBGriselOBlondelMPrettenhoferPWeissRDubourgVVanderplasJPassosACournapeauDBrucherMPerrotMDuchesnayEScikit-learn: Machine Learning in PythonJ Mach Learn Res2011122825830 </plain></SENT>
</text></ref><ref id="CR21"><text><SENT sid="286" pm="."><plain>21OlsonDLDelenDAdvanced Data Mining Techniques2008GermanySpringer-Verlag Berlin Heidelberg </plain></SENT>
</text></ref><ref id="CR22"><text><SENT sid="287" pm="."><plain>22McCullaghPNelderJAGeneralized Linear Models1989LondonLondon: Chapman &amp; Hall </plain></SENT>
</text></ref><ref id="CR23"><text><SENT sid="288" pm="."><plain>23ZhangHBarrVMarkovZThe optimality of naive bayesProceedings of the Seventeenth International Florida Artificial Intelligence Research Society Conference (FLAIRS 2004)2004Palo AltoAAAI Press </plain></SENT>
</text></ref><ref id="CR24"><text><SENT sid="289" pm="."><plain>24CortesCVapnikVSupport-vector networksMach Learn199520327397 </plain></SENT>
</text></ref><ref id="CR25"><text><SENT sid="290" pm="."><plain>25XinLZhuMStochastic stepwise ensembles for variable selectionJ Comput Graph Stat20122122759410.1080/10618600.2012.679223 </plain></SENT>
</text></ref><ref id="CR26"><text><SENT sid="291" pm="."><plain>26ChengLZhuMPossJWHirdesJPGlennyCStoleePOpinion versus practice regarding the use of rehabilitation services in home care: an investigation using machine learning algorithmsBMC Med Inform Decis Mak201515111110.1186/s12911-015-0203-125889846 </plain></SENT>
</text></ref><ref id="CR27"><text><SENT sid="292" pm="."><plain>27LiuNKohZXGohJLinZHaalandBTingBPOngMEHPrediction of adverse cardiac events in emergency department patients with chest pain using machine learning for variable selectionBMC Med Inform Decis Mak20141417510.1186/1472-6947-14-75<?supplied-pmid 25150702?>25150702 </plain></SENT>
</text></ref><ref id="CR28"><text><SENT sid="293" pm="."><plain>28Raschka S. </plain></SENT>
<SENT sid="294" pm="."><plain>Mlxtend. </plain></SENT>
<SENT sid="295" pm="."><plain>2016. doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5281/zenodo.49235">10.5281/zenodo.49235</ext-link>. <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5281/zenodo.49235">http://dx.doi.org/10.5281/zenodo.49235</ext-link>. </plain></SENT>
<SENT sid="296" pm="."><plain>Accessed 8 Feb 2016. </plain></SENT>
</text></ref><ref id="CR29"><text><SENT sid="297" pm="."><plain>29Jones E, Oliphant T, Peterson P, et al.SciPy: Open source scientific tools for Python. </plain></SENT>
<SENT sid="298" pm="."><plain>2001. <ext-link ext-link-type="uri" xlink:href="http://www.scipy.org/">http://www.scipy.org/</ext-link>. </plain></SENT>
<SENT sid="299" pm="."><plain>Accessed 15 Aug 2016. </plain></SENT>
</text></ref><ref id="CR30"><text><SENT sid="300" pm="."><plain>30McKinney W. </plain></SENT>
<SENT sid="301" pm="."><plain>Data structures for statistical computing in python. </plain></SENT>
<SENT sid="302" pm="."><plain>In: Proceedings of the 9th Python in Science Conference, SciPy.org. van der Walt, S., Millman, J. </plain></SENT>
<SENT sid="303" pm="."><plain>(eds.).2010. p. </plain></SENT>
<SENT sid="304" pm="."><plain>51–56. <ext-link ext-link-type="uri" xlink:href="http://conference.scipy.org/proceedings/">http://conference.scipy.org/proceedings/</ext-link>. </plain></SENT>
</text></ref><ref id="CR31"><text><SENT sid="305" pm="."><plain>31WilcoxRBasic Statistics: Understanding Conventional Methods and Modern Insights2009New YorkOxford University Press </plain></SENT>
</text></ref><ref id="CR32"><text><SENT sid="306" pm="."><plain>32RoelenCABültmannUvan RhenenWvan der KlinkJJTwiskJWHeymansMWExternal validation of two prediction models identifying employees at risk of high sickness absence: cohort study with 1-year follow-upBMC Public Health20131311810.1186/1471-2458-13-10523280303 </plain></SENT>
</text></ref><ref id="CR33"><text><SENT sid="307" pm="."><plain>33FanJUpadhyeSWorsterAUnderstanding receiver operating characteristic (roc) curvesCan J Emerg Med2006811920 </plain></SENT>
</text></ref><ref id="CR34"><text><SENT sid="308" pm="."><plain>34HirdesJPossJCurtin-TelegdiNThe method for assigning priority levels (maple): A new decision-support system for allocating home care resourcesBMC Med20086111110.1186/1741-7015-6-918234075 </plain></SENT>
</text></ref><ref id="CR35"><text><SENT sid="309" pm="."><plain>35MorrisJFriesBMehrDHawesCPhilipsCMorVLipsitzLMds cognitive performance scaleJ Gerontol Med Sci19944941748210.1093/geronj/49.4.M174 </plain></SENT>
</text></ref><ref id="CR36"><text><SENT sid="310" pm="."><plain>36SpectorWFleishmanJCombining activities of daily living with instrumental activities of daily living to measure functional disabilityJ Gerontol B Psychol Sci Soc Sci.1998531465710.1093/geronb/53B.1.S46 </plain></SENT>
</text></ref><ref id="CR37"><text><SENT sid="311" pm="."><plain>37BreimanLStatistical modeling: The two cultures (with comments and a rejoinder by the author)Statist Sci200116319923110.1214/ss/1009213726 </plain></SENT>
</text></ref><ref id="CR38"><text><SENT sid="312" pm="."><plain>38GauglerJYuFKrichbaumKWymanJPredictors of nursing home admission for persons with dementiaMed Care2009472191810.1097/MLR.0b013e31818457ce<?supplied-pmid 19169120?>19169120 </plain></SENT>
</text></ref><ref id="CR39"><text><SENT sid="313" pm="."><plain>39DonnellyNAHickeyABurnsAMurphyPDoyleFSystematic review and meta-analysis of the impact of carer stress on subsequent institutionalisation of community-dwelling older peoplePLoS ONE201510611910.1371/journal.pone.0128213 </plain></SENT>
</text></ref><ref id="CR40"><text><SENT sid="314" pm="."><plain>40GiabbanelliPJAdamsJIdentifying small groups of foods that can predict achievement of key dietary recommendations: data mining of the uk national diet and nutrition survey, 2008-12Public Health Nutr201619915435110.1017/S1368980016000185<?supplied-pmid 26879185?>26879185 </plain></SENT>
</text></ref></ref-list></SecTag></back></article>
