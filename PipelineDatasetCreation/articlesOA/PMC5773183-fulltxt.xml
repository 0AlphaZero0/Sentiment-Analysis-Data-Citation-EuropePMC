<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 39.96?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">5773183</article-id><article-id pub-id-type="publisher-id">PONE-D-17-17368</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0191175</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Plants</subject><subj-group><subject>Flowering Plants</subject><subj-group><subject>Vanilla</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Database and Informatics Methods</subject><subj-group><subject>Information Retrieval</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Recall (Memory)</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and Memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Recall (Memory)</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Software Engineering</subject><subj-group><subject>Preprocessing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Software Engineering</subject><subj-group><subject>Preprocessing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Database and Informatics Methods</subject><subj-group><subject>Database Searching</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Information Technology</subject><subj-group><subject>Natural Language Processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and Memory</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>An evaluation of multi-probe locality sensitive hashing for computing similarities over web-scale query logs</article-title><alt-title alt-title-type="running-head">An evaluation of multi-probe locality sensitive hashing for computing similarities over web-scale query logs</alt-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0698-0922</contrib-id><name><surname>Cormode</surname><given-names>Graham</given-names></name><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="corresp" rid="cor001">*</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Dasgupta</surname><given-names>Anirban</given-names></name><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff002"><sup>2</sup></xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Goyal</surname><given-names>Amit</given-names></name><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Lee</surname><given-names>Chi Hoon</given-names></name><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib></contrib-group><aff id="aff001">
<label>1</label>
<addr-line>Department of Computer Science, University of Warwick, Coventry, United Kingdom</addr-line>
</aff><aff id="aff002">
<label>2</label>
<addr-line>Computer Science and Engineering, IIT Gandhinagar, Gandhinagar, India</addr-line>
</aff><aff id="aff003">
<label>3</label>
<addr-line>Yahoo Research, Sunnyvale CA, United States of America</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Wang</surname><given-names>Yeng-Tseng</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">
<addr-line>Kaohsiung Medical University, TAIWAN</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>This work of GC is supported in part by European Research Council grant ERC-2014-CoG 647557, the Yahoo Faculty Research and Engagement Program and a Royal Society Wolfson Research Merit Award. These funders did not have any role in the development and execution of this work. The work of AD, AG, CHL was carried out while they were employed by Yahoo Research. The funder provided support in the form of salaries for authors AD, AG, CHL, but did not have any additional role in the study design, data analysis, decision to publish, or preparation of the manuscript. The specific roles of these authors are articulated in the &#x02018;author contributions&#x02019; section. Some of the data used in the method evaluation study was provided by Yahoo and cannot be shared further by the authors. Please see the Data Availability Statement for more information about data access. This restriction notwithstanding, these interests do not alter our adherence to PLOS ONE policies on sharing data and materials.</p></fn><corresp id="cor001">* E-mail: <email>G.Cormode@warwick.ac.uk</email></corresp></author-notes><pub-date pub-type="collection"><year>2018</year></pub-date><pub-date pub-type="epub"><day>18</day><month>1</month><year>2018</year></pub-date><volume>13</volume><issue>1</issue><elocation-id>e0191175</elocation-id><history><date date-type="received"><day>5</day><month>5</month><year>2017</year></date><date date-type="accepted"><day>3</day><month>12</month><year>2017</year></date></history><permissions><copyright-statement>&#x000a9; 2018 Cormode et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Cormode et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0191175.pdf"/><abstract><p>Many modern applications of AI such as web search, mobile browsing, image processing, and natural language processing rely on finding similar items from a large database of complex objects. Due to the very large scale of data involved (e.g., users&#x02019; queries from commercial search engines), computing such near or nearest neighbors is a non-trivial task, as the computational cost grows significantly with the number of items. To address this challenge, we adopt Locality Sensitive Hashing (a.k.a, <sc>LSH</sc>) methods and evaluate four variants in a distributed computing environment (specifically, Hadoop). We identify several optimizations which improve performance, suitable for deployment in very large scale settings. The experimental results demonstrate our variants of <sc>LSH</sc> achieve the robust performance with better recall compared with &#x0201c;vanilla&#x0201d; <sc>LSH</sc>, even when using the same amount of space.</p></abstract><funding-group><award-group id="award001"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>647557</award-id><principal-award-recipient><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0698-0922</contrib-id><name><surname>Cormode</surname><given-names>Graham</given-names></name></principal-award-recipient></award-group><award-group id="award002"><funding-source><institution>Yahoo Faculty Research Engagement Program</institution></funding-source><principal-award-recipient><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0698-0922</contrib-id><name><surname>Cormode</surname><given-names>Graham</given-names></name></principal-award-recipient></award-group><award-group id="award003"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000288</institution-id><institution>Royal Society</institution></institution-wrap></funding-source><principal-award-recipient><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0698-0922</contrib-id><name><surname>Cormode</surname><given-names>Graham</given-names></name></principal-award-recipient></award-group><funding-statement>This work of GC is supported in part by European Research Council grant ERC-2014-CoG 647557, the Yahoo Faculty Research and Engagement Program and a Royal Society Wolfson Research Merit Award. These funders did not have any role in the development and execution of this work. The work of AD, AG, CHL was carried out while they were employed by Yahoo Research. Yahoo provided the query log data used to evaluate the compared methods. The funder provided support in the form of salaries for authors AD, AG, CHL, and computing resources, but did not have any additional role in the study design, data analysis, decision to publish, or preparation of the manuscript. The specific roles of these authors are articulated in the &#x02018;author contributions&#x02019; section.</funding-statement></funding-group><counts><fig-count count="0"/><table-count count="11"/><page-count count="14"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>* Deidentified AOL data is available from the figshare repository <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.5527231.v1">https://doi.org/10.6084/m9.figshare.5527231.v1</ext-link>* QLogs data is proprietary and cannot be released by us. Requests to access this data can be addressed to Yahoo&#x02019;s academic relations manager, <email>kimcapps@oath.com</email>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>* Deidentified AOL data is available from the figshare repository <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.5527231.v1">https://doi.org/10.6084/m9.figshare.5527231.v1</ext-link>* QLogs data is proprietary and cannot be released by us. Requests to access this data can be addressed to Yahoo&#x02019;s academic relations manager, <email>kimcapps@oath.com</email>.</p></notes></front><body><sec id="sec001"><title>1 Introduction</title><p>Every day, hundreds of millions of people visit web sites and commercial search engines to pose queries on topics of their interest. Such queries are typically just a few key words intended to specify the topic that the user has in mind. To provide users with a high quality service, search engines such as Bing, Google, and Yahoo require intelligent analysis to realize users&#x02019; implicit intents. The key resource that they have to help tease out the intent is their large history of requests, in the form of large scale query logs, as well as the log of user actions on the corresponding result pages. A key primitive in learning users&#x02019; intents is finding the nearest neighbors for a user-given query. Computing nearest neighbors is useful for many search-related problems on the Web and Mobile such as finding related queries [<xref rid="pone.0191175.ref001" ref-type="bibr">1</xref>&#x02013;<xref rid="pone.0191175.ref003" ref-type="bibr">3</xref>], finding near-duplicate queries [<xref rid="pone.0191175.ref004" ref-type="bibr">4</xref>], spelling correction [<xref rid="pone.0191175.ref005" ref-type="bibr">5</xref>, <xref rid="pone.0191175.ref006" ref-type="bibr">6</xref>], and diversifying search results [<xref rid="pone.0191175.ref007" ref-type="bibr">7</xref>]; and Natural Language Processing (NLP) tasks such as paraphrasing [<xref rid="pone.0191175.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0191175.ref009" ref-type="bibr">9</xref>], calculating distributional similarity [<xref rid="pone.0191175.ref010" ref-type="bibr">10</xref>&#x02013;<xref rid="pone.0191175.ref012" ref-type="bibr">12</xref>], and creating sentiment lexicons from large-scale Web data [<xref rid="pone.0191175.ref013" ref-type="bibr">13</xref>].</p><p>In this paper, we focus on the problem of finding nearest neighbors over very large data sets, and ground our study with the application of searching for the best match of a given query from very large scale query logs from a large search engine. In order to understand the implicit users&#x02019; intent, each query is initially represented in a high dimensional feature space, where each dimension corresponds to a clicked url. Given the importance of this question, it is critical to design algorithms that can scale to many queries over huge logs, and allow online and offline computation. However, computing nearest neighbors of a query can be very costly. Naive solutions that involve a linear search of the set of possibilities are simply infeasible in these settings due to the computational cost of processing hundreds of millions of queries. Even though distributed computing environments such as Hadoop make it feasible to store and search large data sets in parallel, the naive pairwise computation is still infeasible. The reason is that the total amount of work performed is still huge, and simply throwing more resources at the problem is not effective. Given a log of hundreds of millions queries, most are &#x0201c;far&#x0201d; from a query of interest, and we should aim to avoid doing many &#x0201c;useless&#x0201d; comparisons that only confirm that other queries are indeed far from it.</p><p>In order to address the computational challenge, this paper aims to find nearest neighbors by doing a <italic>small</italic> number of comparisons&#x02014;that is, sublinear in the dataset size&#x02014;instead of brute force linear search. In addition to minimizing the number of comparisons, we aim to retrieve neighboring candidates with 100% precision and high recall. It is important that the false positive rate (ratio of &#x0201c;incorrectly&#x0201d; identifying queries as close) is penalized more severely than the false negative rate (ratio of missing &#x0201c;true&#x0201d; neighbors).</p><p>When seeking exact matches for queries, effective solutions are based on storing values in a hash table and mapping in via hash functions. The generalization of this approach to approximate matches is the framework of Locality Sensitive Hashing, where queries are more likely to collide under the hash function if they are more alike, and less likely to collide if they are less alike. The methods we propose in this paper meet our criteria by extending Locality Sensitive Hashing [<xref rid="pone.0191175.ref014" ref-type="bibr">14</xref>&#x02013;<xref rid="pone.0191175.ref016" ref-type="bibr">16</xref>]. In particular, we apply the framework within a distributed system, Hadoop, and take advantage of its distributed computing power.</p><p>Our work makes the following contributions:
<list list-type="order"><list-item><p>We describe four variants of vanilla LSH motivated by the research on Multi-Probe LSH [<xref rid="pone.0191175.ref017" ref-type="bibr">17</xref>]. We show that two of these achieve much better recall than vanilla <sc>LSH</sc> using the same number of hash tables. The main idea behind these variants is to intelligently probe multiple &#x0201c;nearby&#x0201d; buckets within a table that have high probability of containing near neighbors of a query.</p></list-item><list-item><p>We present a framework on Hadoop that efficiently finds nearest neighbors for a given query from commercial large-scale query logs in sublinear time.</p></list-item><list-item><p>We discuss the applicability of our framework on two real-world applications: finding related queries and removing (near) duplicate queries. The algorithms presented in this paper are currently being implemented for production use within a large search provider.</p></list-item></list></p></sec><sec id="sec002"><title>2 Problem statement</title><p>We start with user query logs <italic>C</italic> having query vectors collected from a commercial search engine over some domain (e.g. URLs); closeness of queries is measured via cosine similarity on the corresponding vectors. Given a set of queries <italic>Q</italic> and similarity threshold <italic>&#x003c4;</italic>, the problem is to develop a batch process to return a <italic>small</italic> set <italic>T</italic> of candidate neighbors from <italic>C</italic> for each query <italic>q</italic> &#x02208; <italic>Q</italic> such that:
<list list-type="order"><list-item><p><italic>T</italic> = {<italic>l</italic> &#x02223; <italic>s</italic>(<italic>l</italic>, <italic>q</italic>) &#x02265; <italic>&#x003c4;</italic>, <italic>l</italic> &#x02208; <italic>C</italic>}, where <italic>s</italic>(<italic>q</italic><sub>1</sub>, <italic>q</italic><sub>2</sub>) is a function to compute a similarity score between query feature vector <italic>q</italic><sub>1</sub> and <italic>q</italic><sub>2</sub>;</p></list-item><list-item><p><italic>T</italic> achieves 100% precision with &#x0201c;large&#x0201d; recall. That is, our aim is to achieve high recall, while using a scalable efficient algorithm.</p></list-item></list></p><p>The exact brute force algorithm to solve the above problem would be to compute <italic>s</italic>(<italic>l</italic>, <italic>q</italic>) for all <italic>q</italic> &#x02208; <italic>Q</italic> and all <italic>l</italic> &#x02208; <italic>C</italic> and return those (<italic>l</italic>, <italic>q</italic>) where <italic>s</italic>(<italic>l</italic>, <italic>q</italic>) &#x0003e; <italic>&#x003c4;</italic>. This approach is computationally infeasible on a single machine, even if the size of <italic>Q</italic> is of the order of few thousands when the size of <italic>C</italic> is hundreds of millions. Even in a distributed setting such as Hadoop, the resulting communication needed between machines makes this strategy impractical.</p><p>Our aim is to study locality sensitive hashing techniques that enable us to return a set of candidate neighbors while performing a much smaller (<italic>sublinear</italic> in |<italic>Q</italic>| &#x000d7; |<italic>C</italic>|) set of comparisons. In order to tackle this scalability problem, we explore the combination of distributed computation using a map-reduce platform (Hadoop) as well as locality sensitive hashing (<sc>LSH</sc>) algorithms. We explore a few commonly known variants of <sc>LSH</sc> and suggest several variants that are suitable to the map-reduce platform. The methods that we propose meet the practical requirements of a real life search engine backend, and demonstrates how to use locality sensitive hashing on a distributed platform.</p></sec><sec id="sec003"><title>3 Proposed approach</title><p>We describe a distributed Locality Sensitive Hashing framework based on map-reduce. First, we present the &#x0201c;vanilla&#x0201d; <sc>LSH</sc> algorithm due to Andoni and Indyk [<xref rid="pone.0191175.ref016" ref-type="bibr">16</xref>]. This algorithm builds on prior work on <sc>LSH</sc> and Point Location in Equal Balls (PLEB) [<xref rid="pone.0191175.ref014" ref-type="bibr">14</xref>, <xref rid="pone.0191175.ref015" ref-type="bibr">15</xref>]. Subsequent prior work on new variants of PLEB [<xref rid="pone.0191175.ref018" ref-type="bibr">18</xref>] for distributional similarity can be seen as implementing a special case of Andoni and Indyk&#x02019;s <sc>LSH</sc> algorithm. We next present four variants of vanilla <sc>LSH</sc> motivated by the technique of Multi-Probe LSH [<xref rid="pone.0191175.ref017" ref-type="bibr">17</xref>]. A significant drawback of vanilla <sc>LSH</sc> is that it requires a large number of hash tables in order to achieve good recall in finding nearest neighbors, making the algorithm memory intensive. The goal of Multi-probe LSH is to get significantly better recall than the vanilla <sc>LSH</sc> with the same number of hash tables. The main idea behind Multi-probe LSH is to look up multiple buckets within a table that have a high probability of containing the nearest neighbors of a query. We present the high-level ideas behind the Multi-probe LSH algorithm; for more details, the reader is referred to [<xref rid="pone.0191175.ref017" ref-type="bibr">17</xref>].</p><sec id="sec004"><title>3.1 Vanilla <sc>LSH</sc></title><p>The <sc>LSH</sc> algorithm relies on the existence of a family of locality sensitive hash functions. Let <italic>H</italic> be a family of hash functions mapping <inline-formula id="pone.0191175.e001"><alternatives><graphic xlink:href="pone.0191175.e001.jpg" id="pone.0191175.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>D</mml:mi></mml:msup></mml:math></alternatives></inline-formula> to some universe S. For any two query terms <italic>p</italic>, <italic>q</italic>, we choose <italic>h</italic> &#x02208; <italic>H</italic> uniformly at random and analyze the probability that <italic>h</italic>(<italic>p</italic>) = <italic>h</italic>(<italic>q</italic>). Suppose <italic>d</italic> is a distance function (e.g. cosine distance), <italic>R</italic> &#x0003e; 0 is a distance threshold, and <italic>c</italic> &#x0003e; 1 an approximation factor. Let <italic>P</italic><sub>1</sub>, <italic>P</italic><sub>2</sub> &#x02208; (0, 1) be two probability thresholds. The family <italic>H</italic> of hash functions is called a (<italic>R</italic>, <italic>cR</italic>, <italic>P</italic><sub>1</sub>, <italic>P</italic><sub>2</sub>) locality sensitive family if it satisfies the following conditions:
<list list-type="order"><list-item><p>If <italic>d</italic>(<italic>p</italic>, <italic>q</italic>) &#x02264; <italic>R</italic>, then Pr[<italic>h</italic>(<italic>p</italic>) = <italic>h</italic>(<italic>q</italic>)] &#x02265; <italic>P</italic><sub>1</sub>,</p></list-item><list-item><p>If <italic>d</italic>(<italic>p</italic>, <italic>q</italic>) &#x02265; <italic>cR</italic>, then Pr[<italic>h</italic>(<italic>p</italic>) = <italic>h</italic>(<italic>q</italic>)] &#x02264; <italic>P</italic><sub>2</sub></p></list-item></list></p><p>An <sc>LSH</sc> family is generally interesting when <italic>P</italic><sub>1</sub> &#x0003e; <italic>P</italic><sub>2</sub>. However, the difference between <italic>P</italic><sub>1</sub> and <italic>P</italic><sub>2</sub> can be very small. Given a family <italic>H</italic> of hash functions with parameters (<italic>R</italic>, <italic>cR</italic>, <italic>P</italic><sub>1</sub>, <italic>P</italic><sub>2</sub>), the <sc>LSH</sc> algorithm amplifies the gap between the two probabilities <italic>P</italic><sub>1</sub> and <italic>P</italic><sub>2</sub> by concatenating <italic>K</italic> hash functions to create <italic>g</italic>(&#x022c5;) as: <italic>g</italic>(<italic>q</italic>) = (<italic>h</italic><sub>1</sub>(<italic>q</italic>), <italic>h</italic><sub>2</sub>(<italic>q</italic>), &#x02026;, <italic>h</italic><sub><italic>K</italic></sub>(<italic>q</italic>)). A larger value of <italic>K</italic> leads to a larger gap between probabilities of collision for close neighbors (i.e. distance less than <italic>R</italic>) and those for neighbors that are far (i.e. distance more than <italic>cR</italic>); the corresponding probabilities are <inline-formula id="pone.0191175.e002"><alternatives><graphic xlink:href="pone.0191175.e002.jpg" id="pone.0191175.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:msubsup><mml:mi>P</mml:mi><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0191175.e003"><alternatives><graphic xlink:href="pone.0191175.e003.jpg" id="pone.0191175.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:msubsup><mml:mi>P</mml:mi><mml:mn>2</mml:mn><mml:mi>K</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> respectively. This amplification ensures high <italic>precision</italic> by reducing the probability of dissimilar queries having the same hash value. To increase the <italic>recall</italic> of the <sc>LSH</sc> algorithm, Andoni et al. use <italic>L</italic> hash tables, each constructed using a different <italic>g</italic><sub><italic>j</italic></sub>(&#x022c5;) function, where each <italic>g</italic><sub><italic>j</italic></sub>(&#x022c5;) is defined as <italic>g</italic><sub><italic>j</italic></sub>(<italic>q</italic>) = (<italic>h</italic><sub>1,<italic>j</italic></sub>(<italic>q</italic>), <italic>h</italic><sub>2,<italic>j</italic></sub>(<italic>q</italic>), &#x02026;, <italic>h</italic><sub><italic>K</italic>,<italic>j</italic></sub>(<italic>q</italic>))); &#x02200;1 &#x02264; <italic>j</italic> &#x02264; <italic>L</italic>.</p><p><bold>Algorithm 1</bold> Locality Sensitive Hashing Algorithm</p><p specific-use="line"><bold>Preprocessing:</bold> Input is <italic>N</italic> queries with their respective feature vectors.
<list list-type="bullet"><list-item><p>Select <italic>L</italic> functions <italic>g</italic><sub><italic>j</italic></sub>, <italic>j</italic> = 1, 2, &#x02026;, <italic>L</italic>, setting <italic>g</italic><sub><italic>j</italic></sub>(<italic>q</italic>) = (<italic>h</italic><sub>1,<italic>j</italic></sub>(<italic>q</italic>), <italic>h</italic><sub>2,<italic>j</italic></sub>(<italic>q</italic>), &#x02026;, <italic>h</italic><sub><italic>K</italic>,<italic>j</italic></sub>(<italic>q</italic>)), where {<italic>h</italic><sub><italic>i</italic>,<italic>j</italic></sub>, <italic>i</italic> &#x02208; [1, <italic>K</italic>], <italic>j</italic> &#x02208; [1, <italic>L</italic>]} are chosen at random from the <sc>LSH</sc> family.</p></list-item><list-item><p>Construct <italic>L</italic> hash tables, &#x02200;1 &#x02264; <italic>j</italic> &#x02264; <italic>L</italic>. All queries with the same <italic>g</italic><sub><italic>j</italic></sub> value (&#x02200;1 &#x02264; <italic>j</italic> &#x02264; <italic>L</italic>) are placed in the same bucket.</p></list-item></list></p><p specific-use="line"><bold>Query:</bold> Set of <italic>M</italic> test queries. Let <italic>q</italic> denote a test query.
<list list-type="bullet"><list-item><p>For each <italic>j</italic> = 1, 2, &#x02026;, <italic>L</italic>
<list list-type="bullet"><list-item><p>Retrieve all the queries from bucket <italic>g</italic><sub><italic>j</italic></sub>(<italic>q</italic>)</p></list-item><list-item><p>Compute cosine similarity between query <italic>q</italic> and all retrieved queries. Return all the queries within threshold <italic>&#x003c4;</italic>.</p></list-item></list></p></list-item></list></p></sec><sec id="sec005"><title>3.2 <sc>LSH</sc> for cosine similarity</title><p>For cosine similarity we adapt the <sc>LSH</sc> family defined by Charikar [<xref rid="pone.0191175.ref015" ref-type="bibr">15</xref>]. The cosine similarity between two queries <inline-formula id="pone.0191175.e004"><alternatives><graphic xlink:href="pone.0191175.e004.jpg" id="pone.0191175.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is <inline-formula id="pone.0191175.e005"><alternatives><graphic xlink:href="pone.0191175.e005.jpg" id="pone.0191175.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>.</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x02225;</mml:mo><mml:mo>&#x02225;</mml:mo><mml:mi>q</mml:mi><mml:mo>&#x02225;</mml:mo></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. The <sc>LSH</sc> functions for cosine similarity use a random vector <inline-formula id="pone.0191175.e006"><alternatives><graphic xlink:href="pone.0191175.e006.jpg" id="pone.0191175.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mi>D</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> to define a hash function as <italic>h</italic><sub><italic>&#x003b1;</italic></sub>(<italic>p</italic>) = sign(<italic>&#x003b1;</italic> &#x022c5; <italic>p</italic>). A negative sign is interpreted as 0 and positive sign as 1 to generate indices of buckets in the hash tables (i.e. the range of each <italic>g</italic><sub><italic>j</italic></sub>) as <italic>K</italic> bit vectors. To create <italic>&#x003b1;</italic>, we exploit the intuition in [<xref rid="pone.0191175.ref019" ref-type="bibr">19</xref>] and sample each coordinate of <italic>&#x003b1;</italic> from {&#x02212;1, +1} with equal probability. In practice, these are generated by hash functions that maps that index to {&#x02212;1, +1} (a.k.a. the &#x0201c;hashing trick&#x0201d; of [<xref rid="pone.0191175.ref020" ref-type="bibr">20</xref>]). This lets us avoid explicitly storing a (huge) <italic>D</italic> &#x000d7; <italic>K</italic> &#x000d7; <italic>L</italic> random projection matrix.</p><p>Algorithm 1 gives the algorithm for creating and querying the data structure. In a preprocessing step, the algorithm takes as input <italic>N</italic> queries along with the associated feature vectors. In our application, each query is represented using an extremely sparse and high dimensional feature vector constructed as follows: for query <italic>q</italic>, we take all the webpages (urls) that any user has clicked on when querying for <italic>q</italic>. Using this representation, we generate the <italic>L</italic> different hash values for each query <italic>q</italic>, where each such hash value is again the concatenation of <italic>K</italic> hash functions. These <italic>L</italic> hash values per query are then used to create <italic>L</italic> hash tables. Since the width of the index of each bucket is <italic>K</italic> and each coordinate is one bit, each hash table contains 2<sup><italic>K</italic></sup> buckets. Each query term is placed in its respective buckets in each of the <italic>L</italic> hash tables.</p><p>To retrieve near neighbors, we first find all query terms appearing in the buckets associated with each of the <italic>M</italic> test queries. We compute cosine similarity between each of the retrieved terms and the input test queries and return all those queries as neighbors which are within a similarity threshold (<italic>&#x003c4;</italic>).</p><p>The above algorithm fits the Map-Reduce setting quite naturally. We describe a batch setting which performs the LSH on all queries together to perform an all-pairs comparison; other variations are possible depending on the setting. Our implementation performs two map-reduce iterations: in the first phase, the map jobs read in all the queries and their vector representation and outputs key-value pairs that contain the hash-function id (&#x02208;[1, <italic>L</italic>]) and the bucket id (&#x02208;[0, 2<sup><italic>K</italic></sup> &#x02212; 1]) as the keys and the query as the value. The reduce jobs then aggregate all queries belonging to a single bucket for a particular hash function, and output candidate pairs. A second map-reduce job then joins these candidate query pairs with their respective feature vectors, computes the exact cosine similarity, and outputs the pairs that have similarity larger than <italic>&#x003c4;</italic>, ensuring that our precision is 100%. To only consider matches between the <italic>M</italic> test queries and the <italic>N</italic> stored queries, we simply tag each query with its type (test or stored), and only consider candidate pairs that have one of each type. Our experiments show that this map-reduce implementation scales to hundreds of millions of queries.</p></sec><sec id="sec006"><title>3.3 Reusing hash functions</title><p>Directly implementing vanilla <sc>LSH</sc> requires <italic>L</italic> &#x000d7; <italic>K</italic> hash functions. But generating hash functions is computationally expensive as it takes time to read all features and evaluate hash functions over all those features to generate a single bit. To reduce the number of hash functions evaluations, we use a trick from Andoni and Indyk [<xref rid="pone.0191175.ref016" ref-type="bibr">16</xref>] in which hash functions are reused to generate <italic>L</italic> tables. <italic>K</italic> is assumed to be even and <inline-formula id="pone.0191175.e007"><alternatives><graphic xlink:href="pone.0191175.e007.jpg" id="pone.0191175.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:mrow><mml:mi>R</mml:mi><mml:mo>&#x02248;</mml:mo><mml:msqrt><mml:mi>L</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>. We generate <italic>f</italic><sub><italic>j</italic></sub>(<italic>q</italic>) = (<italic>h</italic><sub>1,<italic>j</italic></sub>(<italic>q</italic>), <italic>h</italic><sub>2,<italic>j</italic></sub>(<italic>q</italic>), &#x02026;, <italic>h</italic><sub><italic>K</italic>/2,<italic>j</italic></sub>(<italic>q</italic>))) of length <italic>k</italic>/2. Next, we define <italic>g</italic>(<italic>q</italic>) = (<italic>f</italic><sub><italic>a</italic></sub>, <italic>f</italic><sub><italic>b</italic></sub>), where 1 &#x02264; <italic>a</italic> &#x0003c; <italic>b</italic> &#x02264; <italic>R</italic>. Using such pairings, we can thus generate <inline-formula id="pone.0191175.e008"><alternatives><graphic xlink:href="pone.0191175.e008.jpg" id="pone.0191175.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> hash indices. This scheme requires <inline-formula id="pone.0191175.e009"><alternatives><graphic xlink:href="pone.0191175.e009.jpg" id="pone.0191175.e009g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M9"><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mi>K</mml:mi><mml:msqrt><mml:mi>L</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>) hash functions, instead of <italic>O</italic>(<italic>KL</italic>). We use this trick to generate <italic>L</italic> hash tables with bucket indices of width <italic>K</italic> bits.</p></sec><sec id="sec007"><title>3.4 Multi-Probe <sc>LSH</sc></title><p>Since generating hash functions can be computationally expensive and the memory required by the algorithm scales linearly with <italic>L</italic>, the number of hash tables, it is desirable to keep <italic>L</italic> small. The large memory footprint of vanilla LSH makes it impractical for many real applications. Here, we first describe four new variants of the vanilla <sc>LSH</sc> algorithm motivated by the intuition in Multi-probe LSH [<xref rid="pone.0191175.ref017" ref-type="bibr">17</xref>]. Multi-probe LSH obtains significantly higher recall than vanilla <sc>LSH</sc> while using the same number of hash tables. The main intuition for Multi-probe LSH is that in addition to looking at the hash bucket that a test query <italic>q</italic> falls in, it is also possible to look at the neighboring buckets in order to find its near neighbor candidates. Multi-probe LSH in [<xref rid="pone.0191175.ref017" ref-type="bibr">17</xref>] suggests exploring neighboring buckets in order of their Hamming distance from the bucket in which <italic>q</italic> falls. They show (empirically) that these neighboring buckets contain the near neighbors with very high probability. Though Multi-probe LSH achieves higher recall for the same number of hash tables, it makes more probes as it searches multiple buckets per table. The advantage of searching multiple buckets over generating more tables is that less memory and time is required for table creation.</p><p>The original Multi-probe LSH algorithm was developed for Euclidean distance. However, that algorithm does not immediately translate to our setting of cosine similarity. For example, in generating the list of other buckets inspected, [<xref rid="pone.0191175.ref017" ref-type="bibr">17</xref>] utilizes the distance of the hash value to the bucket boundary&#x02014;this makes sense when the hash value is a real number, but we have bits. We present four variants of Multi-probe LSH for cosine similarity:
<list list-type="bullet"><list-item><p><bold>Random Flip Q:</bold> Our baseline version first computes the initial <sc>LSH</sc> of a test query <italic>q</italic> to give the <italic>L</italic> bucket ids. Next, we create <italic>F</italic> alternate bucket ids by flipping a set of coordinates randomly in each <italic>g</italic><sub><italic>j</italic></sub>(<italic>q</italic>). For scalability, we restrict our implementation to flipping a single bit out of the <italic>K</italic> possible bits each time, and ensure that the sampling is done without repetition. Since the hash functions are randomly chosen, we implement this by simply flipping the first bit, then revert it and flipping the second bit, until we reach the <italic>F</italic>&#x02019;th bit.</p></list-item><list-item><p><bold>Random Flip B:</bold> The second variant is another baseline similar to the previous one. Instead of just flipping the bits for only the test query, here we flips bits for <italic>both</italic> the test query <italic>and</italic> all the queries in the database: this increases the &#x0201c;radius&#x0201d; of the search. We treat each database point as if it were a query, and flip a random bit in each of its hash representations <italic>F</italic> times over. Note that this method requires applying flipping to all the queries in the database. This is a one-time operation done while creating the database. We generate up to <italic>F</italic> variants of each hash, so for each query, first its <italic>L</italic> LSH representations of length <italic>K</italic> are generated. On each of the <italic>L</italic> representations, flipping of bits is applied <italic>F</italic> times to generate <italic>LF</italic> representations of a query.</p></list-item><list-item><p><bold>Distance Flip Q:</bold> The third variant is a smarter version of Random Flip Q. It selects coordinates based on the <italic>distance</italic> of <italic>q</italic> from the random hyperplane (hash function) used to create this coordinate. The distance of the test query <italic>q</italic> from the random hyperplane <italic>&#x003b1;</italic> is the absolute value which we get before applying the sign function on it (see Section 3.2), i.e., abs(<italic>&#x003b1;</italic> &#x022c5; <italic>q</italic>), the distance of <italic>q</italic> from hyperplane <italic>&#x003b1;</italic>. This method flips up to <italic>F</italic> coordinates in order of increasing distance from the hyperplane. That is, for each group of <italic>K</italic> hash values, we sort by the distance to the hyperplane, and swap each of the first <italic>F</italic> of these in turn. As with Random Flip Q, we restrict to flip only a single bit in each repetition, so <italic>F</italic> &#x02264; <italic>K</italic>.</p></list-item><list-item><p><bold>Distance Flip B:</bold> Our fourth variant flips bits for both the test query and for the queries in the database (i.e., the intelligent version of the second baseline). Like Random Flip B, it rquires us to flip all database items, which is a one-time data pre-processing step.</p></list-item></list></p><p>The map-reduce implementation of Multi-probe LSH follows the same structure as the vanilla one&#x02014;the map phase of the first map-reduce job generates the alternate bucket-ids for both the test query and the queries in the database. For all LSH methods, the first preprocessing step is the same, which is to evaluate the hash functions to generate <inline-formula id="pone.0191175.e010"><alternatives><graphic xlink:href="pone.0191175.e010.jpg" id="pone.0191175.e010g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M10"><mml:mrow><mml:mi>K</mml:mi><mml:msqrt><mml:mi>L</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula> bits. The second step is to generate tables indexed by the hash function id and bucket id. Within the map job, each query is mapped to its various indices. For multiprobe LSH, each query is also mapped to additional indices. Within the reduce job, all queries with the same index are collected and all colliding pair of queries (that share the same index) are output. The final step is to compute similarity for the colliding pairs and only keep those pairs that are above the threshold <italic>&#x003c4;</italic> (based on exact comparison using their original feature representation).</p></sec><sec id="sec008"><title>3.5 Time cost</title><p>The exact running time of these algorithms is hard to predict, as it depends on the distribution of the data, as well as the configuration of the computing environment (number of machines, communication topology etc.). Broadly speaking, the time cost is comprised of the preprocessing (the one-time cost to build the database of queries), and the runtime cost to process a new set of query look-ups. The communication cost of our algorithms in the Map-Reduce framework is low, since the majority of the work is embarassingly parallel. Across all our methods, at most <inline-formula id="pone.0191175.e011"><alternatives><graphic xlink:href="pone.0191175.e011.jpg" id="pone.0191175.e011g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M11"><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mi>K</mml:mi><mml:msqrt><mml:mi>L</mml:mi></mml:msqrt><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> hash function evaluations are needed. While it may seem that the multiprobe LSH methods require more hash function evaluations, we aim to choose the parameters <italic>K</italic> and <italic>L</italic> so that less work is needed overall in order to achieve the same level of recall compared to the vanilla LSH methods. The final step, to compute the true similarity of the retrieved pairs, is proportional to the number of collisions. We expect the proposed methods to be faster here, since there should be fewer candidates to test. This stands in contrast to the naive exact method, which performs an all-pairs comparison.</p><p>Due to the variation in real world configurations, we do not explicitly measure the time taken to perform the experiments. Rather, we make use of the number of comparisons as a surrogate. Our informal tests indicate that this is a robust measure of effort required, since the total CPU time was broadly proportional to this measure across a number of different configurations, while we find that the number of comparisons is not subject to interference from external factors (overall cluster loading etc.).</p></sec></sec><sec id="sec009"><title>4 Experiments</title><sec id="sec010"><title>4.1 Data</title><p>We use two data sources for our experiments. The first is the <monospace>AOL-logs</monospace> dataset that contains search queries posed to the AOL search engine and that dataset was made available in 2006 [<xref rid="pone.0191175.ref021" ref-type="bibr">21</xref>]. This data is accessible from the figshare repository, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.5527231.v1">https://doi.org/10.6084/m9.figshare.5527231.v1</ext-link>. We also use a partial sample of query logs from a commercial search engine, denoted as Qlogs. Note that realistic query log information is considered confidential and contains potentially sensitive information about individuals. We are therefore careful in our handling of the data, and report only aggregate results and carefully chosen examples. We do not have permission to share the Qlogs data further, but to allow reproduction of results we show all our analyses on the public data. We were provided access to this data on request to Yahoo via an electronic file. Requests for access to this data can be addressed to Yahoo&#x02019;s academic relations manager, mailto:<email>kimcapps@oath.com</email>.</p><p>As Qlogs reaches hundreds of millions of queries (approximately 600<italic>M</italic> unique queries), we generated multiple datasets from Qlogs by sampling at various rates: <monospace>Qlogs001</monospace> represents a 1% sample, <monospace>Qlogs010</monospace> represents a 10% sample and <monospace>Qlogs100</monospace> represents the entire Qlogs. The smaller datasets are primarily used to explore parameter ranges and identify suitable values that we then use to experiment with the larger dataset. For each query <italic>q</italic>, a feature vector in a high dimensional feature space, denoted as <bold>q</bold> = (<italic>f</italic><sub>1</sub>, <italic>f</italic><sub>2</sub>, &#x022ef;, <italic>f</italic><sub><italic>D</italic></sub>), was created by setting <italic>f</italic><sub><italic>i</italic></sub> to be the click through rate of url <italic>i</italic> when shown in the search results page of search-query <italic>q</italic>. Note that in our real implementation, <bold>q</bold> is represented as a sparse feature vector with only non-zero click-through rate features being present. In a pre-processing step, we remove all queries with at most five clicked urls. <xref ref-type="table" rid="pone.0191175.t001">Table 1</xref> summarizes the statistics of our query-log datasets.</p><table-wrap id="pone.0191175.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t001</object-id><label>Table 1</label><caption><title>Query-logs statistics.</title></caption><alternatives><graphic id="pone.0191175.t001g" xlink:href="pone.0191175.t001"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Data</th><th align="center" rowspan="1" colspan="1"><italic>N</italic></th><th align="center" rowspan="1" colspan="1"><italic>D</italic></th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><monospace>AOL-logs</monospace></td><td align="center" rowspan="1" colspan="1">0.3 &#x000d7; 10<sup>6</sup></td><td align="center" rowspan="1" colspan="1">0.7 &#x000d7; 10<sup>6</sup></td></tr><tr><td align="center" rowspan="1" colspan="1"><monospace>Qlogs001</monospace></td><td align="center" rowspan="1" colspan="1">6 &#x000d7; 10<sup>6</sup></td><td align="center" rowspan="1" colspan="1">66 &#x000d7; 10<sup>6</sup></td></tr><tr><td align="center" rowspan="1" colspan="1"><monospace>Qlogs010</monospace></td><td align="center" rowspan="1" colspan="1">62 &#x000d7; 10<sup>6</sup></td><td align="center" rowspan="1" colspan="1">464 &#x000d7; 10<sup>6</sup></td></tr><tr><td align="center" rowspan="1" colspan="1"><monospace>Qlogs100</monospace></td><td align="center" rowspan="1" colspan="1">617 &#x000d7; 10<sup>6</sup></td><td align="center" rowspan="1" colspan="1">2.4 &#x000d7; 10<sup>9</sup></td></tr></tbody></table></alternatives></table-wrap><p><bold>Test Data.</bold> In all experiments we use a randomly sampled set of 2000 queries <italic>Q</italic>, as the test set. That is, we want to find set <italic>T</italic>, where <italic>T</italic> = {<italic>l</italic>&#x02223;<italic>s</italic>(<italic>q</italic>, <italic>q</italic>&#x02032;) &#x02265; <italic>&#x003c4;</italic>}, <italic>s</italic>(<italic>q</italic>, <italic>q</italic>&#x02032;) is cosine similarity, and <italic>q</italic>&#x02032; &#x02208; <italic>C</italic> for <italic>C</italic> &#x02208; {<code position="float" orientation="portrait" xml:space="preserve">Qlogs001</code>, <code position="float" orientation="portrait" xml:space="preserve">Qlogs010</code>, <code position="float" orientation="portrait" xml:space="preserve">Qlogs100</code>,<code position="float" orientation="portrait" xml:space="preserve">AOL-logs</code>}. For most experiments, we set the similarity threshold <italic>&#x003c4;</italic> = 0.7, meaning that for <italic>q</italic>, candidates <italic>q</italic>&#x02032; having cosine similarity of larger than or equal to 0.7 are retrieved.</p><p><bold>Evaluation Metrics.</bold> We use two metrics for evaluation: recall and number of comparisons. The recall of an <sc>LSH</sc> algorithm measures how well the algorithm can retrieve the <italic>true</italic> similar candidates. The number of comparisons performed by an algorithm is computed as the average number of pairwise comparisons done per test query, and measures the total computation done. The aim is to maximize recall and to minimize the number of comparisons.</p></sec><sec id="sec011"><title>4.2 Evaluating vanilla <sc>LSH</sc></title><p>First, we vary the similarity threshold parameter <italic>&#x003c4;</italic> in the range {0.7, 0.8, 0.9} while fixing <italic>K</italic> = 16 and <italic>L</italic> = 10 for the <monospace>AOL-logs</monospace> and <monospace>Qlogs001</monospace> datasets. <xref ref-type="table" rid="pone.0191175.t002">Table 2</xref> shows that <italic>&#x003c4;</italic> = 0.9 achieves higher recall than <italic>&#x003c4;</italic> = 0.7. This is expected as finding near duplicates is actually easier than finding near neighbors that satisfy only a looser similarity criterion. For the rest of this paper, <italic>&#x003c4;</italic> is set as 0.7 since it represents the more challenging case.</p><table-wrap id="pone.0191175.t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t002</object-id><label>Table 2</label><caption><title>Varying <italic>&#x003c4;</italic> with fixed <italic>K</italic> = 16 and <italic>L</italic> = 10.</title></caption><alternatives><graphic id="pone.0191175.t002g" xlink:href="pone.0191175.t002"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="2" colspan="1"><italic>&#x003c4;</italic></th><th align="center" colspan="2" rowspan="1"><monospace>AOL-logs</monospace></th><th align="center" colspan="2" rowspan="1"><monospace>Qlogs001</monospace></th></tr><tr><th align="right" rowspan="1" colspan="1">Comparisons</th><th align="center" rowspan="1" colspan="1">Recall</th><th align="right" rowspan="1" colspan="1">Comparisons</th><th align="center" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="char" char="." rowspan="1" colspan="1">0.7</td><td align="right" rowspan="1" colspan="1">57</td><td align="char" char="." rowspan="1" colspan="1">.63</td><td align="right" rowspan="1" colspan="1">1052</td><td align="char" char="." rowspan="1" colspan="1">.67</td></tr><tr><td align="char" char="." rowspan="1" colspan="1">0.8</td><td align="right" rowspan="1" colspan="1"/><td align="char" char="." rowspan="1" colspan="1">.84</td><td align="right" rowspan="1" colspan="1"/><td align="char" char="." rowspan="1" colspan="1">.81</td></tr><tr><td align="char" char="." rowspan="1" colspan="1">0.9</td><td align="right" rowspan="1" colspan="1"/><td align="char" char="." rowspan="1" colspan="1">.98</td><td align="right" rowspan="1" colspan="1"/><td align="char" char="." rowspan="1" colspan="1">.96</td></tr></tbody></table></alternatives></table-wrap><p>In the second experiment, we vary <italic>R</italic> to be in {1, 4, 7, 10}, corresponding to values of <italic>L</italic> of {1, 10, 28, 55}, while fixing <italic>K</italic> = 16 on the <monospace>AOL-logs</monospace> and <monospace>Qlogs001</monospace> datasets. Recall that <italic>L</italic> denotes the number of hash tables and <italic>K</italic> is the width of the index of the buckets in the table. Increasing <italic>K</italic> results in increasing precision of the candidate pairs by reducing false positives, but <italic>L</italic> needs to be correspondingly increased in order to maintain good recall (i.e. reduce false negatives). <xref ref-type="table" rid="pone.0191175.t003">Table 3</xref> shows that increasing <italic>L</italic> leads to better recall, at the cost of more comparisons on both datasets. In addition, large <italic>L</italic> means generating many random projection bits and hash tables which is both time and memory intensive. Hence, we fix <italic>L</italic> = 10, to achieve reasonable recall with a tolerable number of comparisons.</p><table-wrap id="pone.0191175.t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t003</object-id><label>Table 3</label><caption><title>Varying <italic>L</italic> with fixed <italic>K</italic> = 16 and <italic>&#x003c4;</italic> = 0.7.</title></caption><alternatives><graphic id="pone.0191175.t003g" xlink:href="pone.0191175.t003"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="2" colspan="1">L</th><th align="center" colspan="2" rowspan="1"><monospace>AOL-logs</monospace></th><th align="center" colspan="2" rowspan="1"><monospace>Qlogs001</monospace></th></tr><tr><th align="right" rowspan="1" colspan="1">Comparisons</th><th align="center" rowspan="1" colspan="1">Recall</th><th align="right" rowspan="1" colspan="1">Comparisons</th><th align="center" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1">1</td><td align="right" rowspan="1" colspan="1">7</td><td align="char" char="." rowspan="1" colspan="1">.28</td><td align="right" rowspan="1" colspan="1">106</td><td align="char" char="." rowspan="1" colspan="1">.36</td></tr><tr><td align="center" style="background-color:#E5E8E8" rowspan="1" colspan="1">10</td><td align="right" style="background-color:#E5E8E8" rowspan="1" colspan="1">57</td><td align="char" char="." style="background-color:#E5E8E8" rowspan="1" colspan="1">.63</td><td align="right" style="background-color:#E5E8E8" rowspan="1" colspan="1">1052</td><td align="char" char="." style="background-color:#E5E8E8" rowspan="1" colspan="1">.67</td></tr><tr><td align="center" rowspan="1" colspan="1">28</td><td align="right" rowspan="1" colspan="1">152</td><td align="char" char="." rowspan="1" colspan="1">.77</td><td align="right" rowspan="1" colspan="1">2908</td><td align="char" char="." rowspan="1" colspan="1">.78</td></tr><tr><td align="center" rowspan="1" colspan="1">55</td><td align="right" rowspan="1" colspan="1">297</td><td align="char" char="." rowspan="1" colspan="1">.89</td><td align="right" rowspan="1" colspan="1">5648</td><td align="char" char="." rowspan="1" colspan="1">.84</td></tr></tbody></table></alternatives></table-wrap><p>Next, we vary <italic>K</italic> in {4, 8, 16} while fixing <italic>L</italic> = 10. As expected, <xref ref-type="table" rid="pone.0191175.t004">Table 4</xref> shows that increasing <italic>K</italic> reduces the number of comparisons and worsens recall on both datasets. This is intuitive as the larger value of <italic>K</italic> leads to larger gap between probabilities of collision for queries that are close and those that are far. Henceforth, we fix <italic>K</italic> = 16 to have an acceptable number of comparisons.</p><table-wrap id="pone.0191175.t004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t004</object-id><label>Table 4</label><caption><title>Varying <italic>K</italic> with fixed <italic>L</italic> = 10 with <italic>&#x003c4;</italic> = 0.7.</title></caption><alternatives><graphic id="pone.0191175.t004g" xlink:href="pone.0191175.t004"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="2" colspan="1">K</th><th align="center" colspan="2" rowspan="1"><monospace>AOL-logs</monospace></th><th align="center" colspan="2" rowspan="1"><monospace>Qlogs001</monospace></th></tr><tr><th align="right" rowspan="1" colspan="1">Comparisons</th><th align="center" rowspan="1" colspan="1">Recall</th><th align="right" rowspan="1" colspan="1">Comparisons</th><th align="center" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1">4</td><td align="right" rowspan="1" colspan="1">112,347</td><td align="char" char="." rowspan="1" colspan="1">.98</td><td align="right" rowspan="1" colspan="1">2,29,2670</td><td align="char" char="." rowspan="1" colspan="1">.96</td></tr><tr><td align="center" rowspan="1" colspan="1">8</td><td align="right" rowspan="1" colspan="1">11,008</td><td align="char" char="." rowspan="1" colspan="1">.90</td><td align="right" rowspan="1" colspan="1">221,132</td><td align="char" char="." rowspan="1" colspan="1">.88</td></tr><tr><td align="center" style="background-color:#E5E8E8" rowspan="1" colspan="1">16</td><td align="right" style="background-color:#E5E8E8" rowspan="1" colspan="1">57</td><td align="char" char="." style="background-color:#E5E8E8" rowspan="1" colspan="1">.63</td><td align="right" style="background-color:#E5E8E8" rowspan="1" colspan="1">1,052</td><td align="char" char="." style="background-color:#E5E8E8" rowspan="1" colspan="1">.67</td></tr></tbody></table></alternatives></table-wrap><p>In the fourth experiment, we fix <italic>L</italic> = 10 and <italic>K</italic> = 16 as determined above, and we increase the size of training data. <xref ref-type="table" rid="pone.0191175.t005">Table 5</xref> demonstrates that as we increase data size, the number of comparisons done by the algorithm also increase. This result indicates that <italic>K</italic> needs to be tuned with respect to a specific dataset, as a larger <italic>K</italic> will reduce the probability of dissimilar queries falling within the same bucket. <italic>K</italic> and <italic>L</italic> can be tuned by randomly sampling a small set of queries. In this paper, we randomly select 2000 queries to tune parameter <italic>K</italic>.</p><table-wrap id="pone.0191175.t005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t005</object-id><label>Table 5</label><caption><title>Fixed <italic>K</italic> = 16 and <italic>L</italic> = 10 with <italic>&#x003c4;</italic> = 0.7.</title></caption><alternatives><graphic id="pone.0191175.t005g" xlink:href="pone.0191175.t005"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Data</th><th align="right" rowspan="1" colspan="1">Comparisons</th><th align="center" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><monospace>AOL-logs</monospace></td><td align="right" rowspan="1" colspan="1">57</td><td align="center" rowspan="1" colspan="1">.63</td></tr><tr><td align="center" rowspan="1" colspan="1"><monospace>Qlogs001</monospace></td><td align="right" rowspan="1" colspan="1">1,052</td><td align="center" rowspan="1" colspan="1">.67</td></tr><tr><td align="center" rowspan="1" colspan="1"><monospace>Qlogs010</monospace></td><td align="right" rowspan="1" colspan="1">10,515</td><td align="center" rowspan="1" colspan="1">.64</td></tr><tr><td align="center" rowspan="1" colspan="1"><monospace>Qlogs100</monospace></td><td align="right" rowspan="1" colspan="1">105,126</td><td align="center" rowspan="1" colspan="1">-</td></tr></tbody></table></alternatives></table-wrap><p>
<xref ref-type="table" rid="pone.0191175.t006">Table 6</xref> shows the best choices of <italic>K</italic> for our datasets. We note that on <monospace>Qlogs100</monospace> the precision/recall cannot be computed, as it was computationally infeasible to find the exact similar neighbors. On our biggest dataset of 600M queries, we set <italic>K</italic> = 24 and <italic>L</italic> = 10. These settings require only 464 comparisons (on average) to find approximate neighbors compared to exact cosine similarity that involves brute force search over all 600M queries.</p><table-wrap id="pone.0191175.t006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t006</object-id><label>Table 6</label><caption><title>Best parameter settings of <italic>K</italic> (minimizing comparisons and maximizing recall) with <italic>L</italic> = 10.</title></caption><alternatives><graphic id="pone.0191175.t006g" xlink:href="pone.0191175.t006"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Data</th><th align="right" rowspan="1" colspan="1">Comparisons</th><th align="center" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><monospace>AOL-logs</monospace> (<italic>K</italic> = 16)</td><td align="right" rowspan="1" colspan="1">57</td><td align="center" rowspan="1" colspan="1">.63</td></tr><tr><td align="center" rowspan="1" colspan="1"><monospace>Qlogs001</monospace> (<italic>K</italic> = 16)</td><td align="right" rowspan="1" colspan="1">1,052</td><td align="center" rowspan="1" colspan="1">.67</td></tr><tr><td align="center" rowspan="1" colspan="1"><monospace>Qlogs010</monospace> (<italic>K</italic> = 20)</td><td align="right" rowspan="1" colspan="1">695</td><td align="center" rowspan="1" colspan="1">.53</td></tr><tr><td align="center" rowspan="1" colspan="1"><monospace>Qlogs100</monospace> (<italic>K</italic> = 24)</td><td align="right" rowspan="1" colspan="1">464</td><td align="center" rowspan="1" colspan="1">-</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec012"><title>4.3 Evaluating multi-probe <sc>LSH</sc></title><p>First, we compare flipping <italic>F</italic> bits in the query only. We evaluate two approaches: Random Flip Q and Distance Flip Q. We make several observations from <xref ref-type="table" rid="pone.0191175.t007">Table 7</xref>: 1) As expected, increasing the number of flips improves recall at the expense of more comparisons for both Distance Flip Q and Random Flip Q. 2) The last row of <xref ref-type="table" rid="pone.0191175.t007">Table 7</xref> shows that when we flip all <italic>K</italic> bits (<italic>F</italic> = 16), Distance Flip Q and Random Flip Q converge to the same algorithm, as expected. 3) We see that Distance Flip Q has significantly better recall than Random Flip Q with a similar number of comparisons. In the second row of the table with <italic>F</italic> = 2, the recall of Distance Flip Q is nine points better than that of Random Flip Q.</p><table-wrap id="pone.0191175.t007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t007</object-id><label>Table 7</label><caption><title>Flipping the bits in the query only with <italic>K</italic> = 16 and <italic>L</italic> = 10 on <monospace>AOL-logs</monospace> with <italic>&#x003c4;</italic> = 0.7.</title></caption><alternatives><graphic id="pone.0191175.t007g" xlink:href="pone.0191175.t007"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Method</th><th align="center" colspan="2" rowspan="1">Random Flip Q</th><th align="center" colspan="2" rowspan="1">Distance Flip Q</th></tr><tr><th align="center" rowspan="1" colspan="1"><italic>F</italic></th><th align="center" rowspan="1" colspan="1">Comparisons</th><th align="center" rowspan="1" colspan="1">Recall</th><th align="center" rowspan="1" colspan="1">Comparisons</th><th align="center" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">108</td><td align="char" char="." rowspan="1" colspan="1">.65</td><td align="center" rowspan="1" colspan="1">106</td><td align="char" char="." rowspan="1" colspan="1">.72</td></tr><tr><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">159</td><td align="char" char="." rowspan="1" colspan="1">.66</td><td align="center" rowspan="1" colspan="1"><italic>155</italic></td><td align="char" char="." rowspan="1" colspan="1"><bold>.75</bold></td></tr><tr><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">311</td><td align="char" char="." rowspan="1" colspan="1">.70</td><td align="center" rowspan="1" colspan="1">303</td><td align="char" char="." rowspan="1" colspan="1">.79</td></tr><tr><td align="center" rowspan="1" colspan="1">10</td><td align="center" rowspan="1" colspan="1">557</td><td align="char" char="." rowspan="1" colspan="1">.75</td><td align="center" rowspan="1" colspan="1">552</td><td align="char" char="." rowspan="1" colspan="1">.81</td></tr><tr><td align="center" rowspan="1" colspan="1">16</td><td align="center" rowspan="1" colspan="1">839</td><td align="char" char="." rowspan="1" colspan="1">.82</td><td align="center" rowspan="1" colspan="1">839</td><td align="char" char="." rowspan="1" colspan="1">.82</td></tr></tbody></table></alternatives></table-wrap><p>
<xref ref-type="table" rid="pone.0191175.t008">Table 8</xref> shows the result of flipping <italic>F</italic> bits in both query and the database. In the second row of <xref ref-type="table" rid="pone.0191175.t008">Table 8</xref> with <italic>F</italic> = 2, Distance Flip B has thirteen points better recall than Random Flip B with a similar number of comparisons. Comparing across the second row of Tables <xref ref-type="table" rid="pone.0191175.t007">7</xref> and <xref ref-type="table" rid="pone.0191175.t008">8</xref> shows that flipping bits in both query and database has better recall at the expense of more comparisons. This is expected as flipping both means that we increase our &#x0201c;radius of search&#x0201d; to include queries at distance two (one flip in query, one flip in database), and hence have more queries in each table when we probe. We also compared distance-based flipping with random flipping on different input sizes, and found that distance-based flipping always has much better recall compared to random flipping (for brevity, we omit these numbers).</p><table-wrap id="pone.0191175.t008" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t008</object-id><label>Table 8</label><caption><title>Flipping the bits in both the query and the database with <italic>K</italic> = 16 and <italic>L</italic> = 10 on <monospace>AOL-logs</monospace> with <italic>&#x003c4;</italic> = 0.7.</title></caption><alternatives><graphic id="pone.0191175.t008g" xlink:href="pone.0191175.t008"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Method</th><th align="center" colspan="2" rowspan="1">Random Flip B</th><th align="center" colspan="2" rowspan="1">Distance Flip B</th></tr><tr><th align="center" rowspan="1" colspan="1"><italic>F</italic></th><th align="center" rowspan="1" colspan="1">Comparisons</th><th align="center" rowspan="1" colspan="1">Recall</th><th align="center" rowspan="1" colspan="1">Comparisons</th><th align="center" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1">1</td><td align="center" rowspan="1" colspan="1">204</td><td align="char" char="." rowspan="1" colspan="1">.71</td><td align="center" rowspan="1" colspan="1">192</td><td align="char" char="." rowspan="1" colspan="1">.80</td></tr><tr><td align="center" rowspan="1" colspan="1">2</td><td align="center" rowspan="1" colspan="1">433</td><td align="char" char="." rowspan="1" colspan="1">.73</td><td align="center" rowspan="1" colspan="1"><italic>405</italic></td><td align="char" char="." rowspan="1" colspan="1"><bold>.86</bold></td></tr><tr><td align="center" rowspan="1" colspan="1">5</td><td align="center" rowspan="1" colspan="1">1557</td><td align="char" char="." rowspan="1" colspan="1">.86</td><td align="center" rowspan="1" colspan="1">1475</td><td align="char" char="." rowspan="1" colspan="1">.93</td></tr><tr><td align="center" rowspan="1" colspan="1">10</td><td align="center" rowspan="1" colspan="1">4138</td><td align="char" char="." rowspan="1" colspan="1">.94</td><td align="center" rowspan="1" colspan="1">4059</td><td align="char" char="." rowspan="1" colspan="1">.96</td></tr><tr><td align="center" rowspan="1" colspan="1">16</td><td align="center" rowspan="1" colspan="1">5922</td><td align="char" char="." rowspan="1" colspan="1">.96</td><td align="center" rowspan="1" colspan="1">5922</td><td align="char" char="." rowspan="1" colspan="1">.96</td></tr></tbody></table></alternatives></table-wrap><p>We select <italic>F</italic> = 2 as the best parameter setting with goal of maximizing recall by restricting comparisons to a minimum. For better recall at the expense of more comparisons, <italic>F</italic> = 5 can also be selected. However, results in Tables <xref ref-type="table" rid="pone.0191175.t007">7</xref> and <xref ref-type="table" rid="pone.0191175.t008">8</xref> indicate that <italic>F</italic> &#x0003e; 5 does <italic>not</italic> increase recall significantly while leading to more comparisons.</p><p>
<xref ref-type="table" rid="pone.0191175.t009">Table 9</xref> gives the results of both variants of distance-based Multi Probe, i.e. Distance Flip Q and Distance Flip B, on different sized datasets. We present results with the parameters <italic>L</italic> = 10, <italic>F</italic> = 2, and value of <italic>K</italic> chosen as per the values used in the final vanilla LSH experiment. As observed there, flipping bits in both query and the database is significantly better in terms of recall with more comparisons. The second and third row of the table respectively shows that flipping bits in both query and the database has eight points better recall on both <monospace>Qlogs001</monospace> and <monospace>Qlogs010</monospace> datasets. With the goal of maximizing recall with some extra comparisons, we select Distance Flip B as our preferred algorithm. Distance Flip B maximizes recall with few tables and comparisons. On our entire corpus (<monospace>Qlogs100</monospace>) with hundreds of millions of queries, Distance Flip B only requires 3,427 comparisons per test query, compared to hundreds of millions of comparisons by the exact brute force algorithm. Distance Flip B returns 9 neighbors on average per given query, averaged over 2000 random test queries. Here, many queries are long, and have few neighbors.</p><table-wrap id="pone.0191175.t009" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t009</object-id><label>Table 9</label><caption><title>Best parameter settings of <italic>K</italic> (minimizing comparisons and maximizing recall) with <italic>L</italic> = 10, <italic>F</italic> = 2, <italic>&#x003c4;</italic> = 0.7.</title></caption><alternatives><graphic id="pone.0191175.t009g" xlink:href="pone.0191175.t009"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Method</th><th align="center" colspan="2" rowspan="1">Distance Flip Q</th><th align="center" colspan="2" rowspan="1">Distance Flip B</th></tr><tr><th align="center" rowspan="1" colspan="1">Data</th><th align="center" rowspan="1" colspan="1">Comps.</th><th align="center" rowspan="1" colspan="1">Recall</th><th align="center" rowspan="1" colspan="1">Comps.</th><th align="center" rowspan="1" colspan="1">Recall</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><monospace>AOL-logs</monospace> (<italic>K</italic> = 16)</td><td align="center" rowspan="1" colspan="1">155</td><td align="center" rowspan="1" colspan="1">.75</td><td align="center" rowspan="1" colspan="1">405</td><td align="center" rowspan="1" colspan="1">.86</td></tr><tr><td align="center" rowspan="1" colspan="1"><monospace>Qlogs001</monospace> (<italic>K</italic> = 16)</td><td align="center" rowspan="1" colspan="1">2980</td><td align="center" rowspan="1" colspan="1">.76</td><td align="center" rowspan="1" colspan="1">7904</td><td align="center" rowspan="1" colspan="1">.84</td></tr><tr><td align="center" rowspan="1" colspan="1"><monospace>Qlogs010</monospace> (<italic>K</italic> = 20)</td><td align="center" rowspan="1" colspan="1">1954</td><td align="center" rowspan="1" colspan="1">.64</td><td align="center" rowspan="1" colspan="1">5242</td><td align="center" rowspan="1" colspan="1">.72</td></tr><tr><td align="center" rowspan="1" colspan="1"><monospace>Qlogs100</monospace> (<italic>K</italic> = 24)</td><td align="center" rowspan="1" colspan="1">1280</td><td align="center" rowspan="1" colspan="1">-</td><td align="center" rowspan="1" colspan="1">3427</td><td align="center" rowspan="1" colspan="1">-</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec013"><title>4.4 Discussion</title><p>Tables <xref ref-type="table" rid="pone.0191175.t010">10</xref> and <xref ref-type="table" rid="pone.0191175.t011">11</xref> shows some qualitative results for a set of arbitrarily chosen queries. These results are found by applying our system (Distance Flip B with parameters <italic>L</italic> = 10, <italic>K</italic> = 24, and <italic>F</italic> = 2) on <monospace>Qlogs100</monospace>. These results help to highlight several applications that can take significant advantage of the approximate Distance Flip B algorithm presented in this paper. For example, the second column in <xref ref-type="table" rid="pone.0191175.t010">Table 10</xref> shows that the returned approximate similar neighbors can be useful in finding related queries [<xref rid="pone.0191175.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0191175.ref002" ref-type="bibr">2</xref>]. The first column in <xref ref-type="table" rid="pone.0191175.t011">Table 11</xref> shows an example where we find several popular spelling errors automatically, which can usefully be used for query suggestion.</p><table-wrap id="pone.0191175.t010" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t010</object-id><label>Table 10</label><caption><title>10 similar neighbors returned by Distance Flip B with <italic>L</italic> = 10, <italic>K</italic> = 24, and <italic>F</italic> = 2 on <monospace>Qlogs100</monospace> for two example queries.</title></caption><alternatives><graphic id="pone.0191175.t010g" xlink:href="pone.0191175.t010"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">how lbs in a ton</th><th align="center" rowspan="1" colspan="1">coldwell banker baileys harbor</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1">how much lbs is a ton</td><td align="center" rowspan="1" colspan="1">coldwell banker sturgeon bay wi</td></tr><tr><td align="center" rowspan="1" colspan="1">number of pounds in a ton</td><td align="center" rowspan="1" colspan="1">coldwell banker door county</td></tr><tr><td align="center" rowspan="1" colspan="1">how many lb are in a ton</td><td align="center" rowspan="1" colspan="1">door county wi mls listings</td></tr><tr><td align="center" rowspan="1" colspan="1">How many pounds are in a ton?</td><td align="center" rowspan="1" colspan="1">door county realtors sturgeon bay</td></tr><tr><td align="center" rowspan="1" colspan="1">how many pounds in a ton</td><td align="center" rowspan="1" colspan="1">DOOR CTY REAL</td></tr><tr><td align="center" rowspan="1" colspan="1">1 short ton equals how many pounds</td><td align="center" rowspan="1" colspan="1">door county coldwell banker</td></tr><tr><td align="center" rowspan="1" colspan="1">how many lbs in a ton?</td><td align="center" rowspan="1" colspan="1">door realty</td></tr><tr><td align="center" rowspan="1" colspan="1">how many pounds in a ton?</td><td align="center" rowspan="1" colspan="1">coldwell banker door county horizons</td></tr><tr><td align="center" rowspan="1" colspan="1">How many pounds are in a ton</td><td align="center" rowspan="1" colspan="1">door county coldwell banker real estate</td></tr><tr><td align="center" rowspan="1" colspan="1">how many lb in a ton</td><td align="center" rowspan="1" colspan="1">coldwell banker door county wisconsin</td></tr></tbody></table></alternatives></table-wrap><table-wrap id="pone.0191175.t011" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t011</object-id><label>Table 11</label><caption><title>10 similar neighbors returned by Distance Flip B with <italic>L</italic> = 10, <italic>K</italic> = 24, and <italic>F</italic> = 2 on <monospace>Qlogs100</monospace> for two example queries.</title></caption><alternatives><graphic id="pone.0191175.t011g" xlink:href="pone.0191175.t011"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">michaels</th><th align="center" rowspan="1" colspan="1">trumbull ct weather</th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1">maichaels</td><td align="center" rowspan="1" colspan="1">trumbull ct weather forecast</td></tr><tr><td align="center" rowspan="1" colspan="1">machaels</td><td align="center" rowspan="1" colspan="1">weather in trumbull ct</td></tr><tr><td align="center" rowspan="1" colspan="1">mechaels</td><td align="center" rowspan="1" colspan="1">weather in trumbull ct 06611</td></tr><tr><td align="center" rowspan="1" colspan="1">miachaels</td><td align="center" rowspan="1" colspan="1">trumbull weather forecast</td></tr><tr><td align="center" rowspan="1" colspan="1">michaeils</td><td align="center" rowspan="1" colspan="1">trumbull ct 06611</td></tr><tr><td align="center" rowspan="1" colspan="1">michaelos</td><td align="center" rowspan="1" colspan="1">trumbull weather ct</td></tr><tr><td align="center" rowspan="1" colspan="1">michaeks</td><td align="center" rowspan="1" colspan="1">trumbull ct weather report</td></tr><tr><td align="center" rowspan="1" colspan="1">michaeels</td><td align="center" rowspan="1" colspan="1">trumbull connecticut weather</td></tr><tr><td align="center" rowspan="1" colspan="1">michaelas</td><td align="center" rowspan="1" colspan="1">weather 06611</td></tr><tr><td align="center" rowspan="1" colspan="1">michae;ls</td><td align="center" rowspan="1" colspan="1">weather trumbull ct</td></tr></tbody></table></alternatives></table-wrap><p>One interesting application of near-neighbor finding is to understand specific intents behind the user query. Given a user&#x02019;s query, Bing, Google, and Yahoo often delivers direct display results that summarize expected contents of the query. For instance, when a query &#x0201c;f stock price&#x0201d; is issued to search engines, the quick summary of the stock quote with a chart is delivered to the user as the part of the search engine result page. Such direct display results are expected to reduce the number of unnecessary clicks by providing the user with the appropriate content early on. However, when the query &#x0201c;f today closing price&#x0201d; is issued to search engines, the three major search engines fail to deliver the same direct display experience to the user query, even though its query intent is strongly related to &#x0201c;f stock price&#x0201d;. By employing an algorithm similar to Distance Flip B, we can build a synonym database, which will help trigger the same direct display among related queries. The first column of <xref ref-type="table" rid="pone.0191175.t010">Table 10</xref> and the second column of <xref ref-type="table" rid="pone.0191175.t011">Table 11</xref> show examples of near-duplicate queries that can be automatically answered [<xref rid="pone.0191175.ref004" ref-type="bibr">4</xref>].</p><p>Another application is to remove duplicated instances in a set of suggested results. When a query set is retrieved from a repository and presented to users, it is important to remove similar queries from the set so that the user is not distracted by duplicated results. Given a set of queries, we can apply Distance Flip B algorithm to build a lookup table of near-duplicates in order to find the &#x0201c;duplicated query terms&#x0201d; efficiently. As &#x0201c;near-duplicates&#x0201d; among query terms typically require a &#x0201c;higher&#x0201d; degree of similarity (relatively easier problem) than &#x0201c;relatedness&#x0201d;, we can tune parameters (<italic>K</italic>, <italic>L</italic>, <italic>F</italic>) based on a specific <italic>&#x003c4;</italic> (e.g <italic>&#x003c4;</italic> = 0.9) from training samples. The second column in <xref ref-type="table" rid="pone.0191175.t011">Table 11</xref> illustrates several effective duplicates: &#x0201c;trumbull weather ct&#x0201d; and &#x0201c;weather in trumbull ct&#x0201d;.</p></sec></sec><sec id="sec014"><title>5 Related work</title><p>There has been much work in last decade focusing on approximate algorithms for finding similar objects, too much to survey in full, so we highlight some important related publications. From the NLP community, prior work on <sc>LSH</sc> for noun clustering [<xref rid="pone.0191175.ref010" ref-type="bibr">10</xref>] applied the original version of <sc>LSH</sc> based on Point Location in Equal Balls (PLEB) [<xref rid="pone.0191175.ref014" ref-type="bibr">14</xref>, <xref rid="pone.0191175.ref015" ref-type="bibr">15</xref>]. The disadvantage of vanilla <sc>LSH</sc> algorithm is that it involves generating a large number of hash functions (in the range <italic>L</italic> = 1000) and sorting bit vectors of large width (<italic>K</italic> = 3000). To address that issue, Goyal et al. [<xref rid="pone.0191175.ref018" ref-type="bibr">18</xref>] proposed a new variant of PLEB that is faster than the original <sc>LSH</sc> algorithm but that still requires large number of hash functions (<italic>L</italic> = 1000). In addition, their work can be seen as an implementing a special case of Andoni and Indyk&#x02019;s <sc>LSH</sc> algorithm, that was applied to the problem of detecting new events from a stream of Twitter posts [<xref rid="pone.0191175.ref022" ref-type="bibr">22</xref>].</p><p>A major distinction of our research is that existing work deals with approximating cosine similarity by Hamming distance [<xref rid="pone.0191175.ref010" ref-type="bibr">10</xref>, <xref rid="pone.0191175.ref018" ref-type="bibr">18</xref>, <xref rid="pone.0191175.ref023" ref-type="bibr">23</xref>&#x02013;<xref rid="pone.0191175.ref025" ref-type="bibr">25</xref>]. Moran et al. [<xref rid="pone.0191175.ref025" ref-type="bibr">25</xref>] proposed a data-driven non-uniform bit allocation across hyperplanes that uses fewer bits than many existing <sc>LSH</sc> schemes to approximate cosine similarity by Hamming distance. In all these existing problem settings, the goal is to minimize both false positives and negatives. However, we focus on minimizing false negatives with zero tolerance for false positives. [<xref rid="pone.0191175.ref026" ref-type="bibr">26</xref>] developed a distributed version of the LSH algorithm, for the Jaccard distance metric, that scales to very large text corpora by virtue of being implemented on a map-reduce, and by using clever sampling schemes in order to reduce the communication cost. Our work addresses the cosine similarity metric, and uses bit flipping in a distributed manner to reduce the number of hash tables in LSH and hence the memory.</p><p>Other work in this area has addressed engineering throughput for massively parallel computation [<xref rid="pone.0191175.ref027" ref-type="bibr">27</xref>], distributed LSH for Euclidean distance [<xref rid="pone.0191175.ref028" ref-type="bibr">28</xref>], and variants such as &#x0201c;entropy-based LSH&#x0201d;, also for Euclidean distance [<xref rid="pone.0191175.ref029" ref-type="bibr">29</xref>].</p></sec><sec id="sec015"><title>6 Conclusion</title><p>In this work, we applied the vanilla <sc>LSH</sc> algorithm of Andoni et al. to search query similarity applications. We proposed four variants of <sc>LSH</sc> that aim to reduce the number of hash tables used. Two of our variants achieve significantly better recall than vanilla LSH while using the same number of hash tables. We also present a framework on Hadoop that efficiently finds nearest neighbors for a given query from a commercial large-scale query logs in sublinear time. On our entire corpus (<monospace>Qlogs100</monospace>) with hundreds of millions of queries, Distance Flip B only requires 3,427 comparisons compared to hundreds of millions of comparisons by exact brute force algorithm. In future, we plan to extend our <sc>LSH</sc> framework to several large-scale NLP, search, and social media applications.</p></sec></body><back><ref-list><title>References</title><ref id="pone.0191175.ref001"><label>1</label><mixed-citation publication-type="other">Jones R, Rey B, Madani O, Greiner W. Generating Query Substitutions. In: ACM International Conference on World Wide Web (WWW); 2006.</mixed-citation></ref><ref id="pone.0191175.ref002"><label>2</label><mixed-citation publication-type="other">Jain A, Ozertem U, Velipasaoglu E. Synthesizing High Utility Suggestions for Rare Web Search Queries. In: ACM SIGIR Conference on Research and Development in Information Retrieval; 2011.</mixed-citation></ref><ref id="pone.0191175.ref003"><label>3</label><mixed-citation publication-type="other">Song Y, Zhou D, He Lw. Query Suggestion by Constructing Term-transition Graphs. In: ACM International Conference on Web Search and Data Mining (WSDM); 2012.</mixed-citation></ref><ref id="pone.0191175.ref004"><label>4</label><mixed-citation publication-type="other">Lee C, Jain A, Lai L. Assisting web search users by destination reachability. In: Conference on Information and Knowledge Management; 2011.</mixed-citation></ref><ref id="pone.0191175.ref005"><label>5</label><mixed-citation publication-type="other">Ahmad F, Kondrak G. Learning a Spelling Error Model from Search Query Logs. In: Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing; 2005.</mixed-citation></ref><ref id="pone.0191175.ref006"><label>6</label><mixed-citation publication-type="other">Li Y, Duan H, Zhai C. A Generalized Hidden Markov Model with Discriminative Training for Query Spelling Correction. In: ACM SIGIR Conference on Research and Development in Information Retrieval; 2012.</mixed-citation></ref><ref id="pone.0191175.ref007"><label>7</label><mixed-citation publication-type="other">Song Y, Zhou D, He Lw. Post-ranking Query Suggestion by Diversifying Search Results. In: Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR); 2011.</mixed-citation></ref><ref id="pone.0191175.ref008"><label>8</label><mixed-citation publication-type="other">Petrovic S, Osborne M, Lavrenko V. Using paraphrases for improving first story detection in news and Twitter. In: Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies; 2012.</mixed-citation></ref><ref id="pone.0191175.ref009"><label>9</label><mixed-citation publication-type="other">Ganitkevitch J, Van Durme B, Callison-Burch C. PPDB: The Paraphrase Database. In: North American Chapter of the Association for Computational Linguistics (NAACL); 2013.</mixed-citation></ref><ref id="pone.0191175.ref010"><label>10</label><mixed-citation publication-type="other">Ravichandran D, Pantel P, Hovy E. Randomized algorithms and NLP: using locality sensitive hash function for high speed noun clustering. In: Annual Meeting of the Association for Computational Linguistics; 2005.</mixed-citation></ref><ref id="pone.0191175.ref011"><label>11</label><mixed-citation publication-type="other">Agirre E, Alfonseca E, Hall K, Kravalova J, Pa&#x0015f;ca M, Soroa A. A study on similarity and relatedness using distributional and WordNet-based approaches. In: Proceedings of HLT-NAACL; 2009.</mixed-citation></ref><ref id="pone.0191175.ref012"><label>12</label><mixed-citation publication-type="journal">
<name><surname>Turney</surname><given-names>PD</given-names></name>, <name><surname>Pantel</surname><given-names>P</given-names></name>. <article-title>From Frequency to Meaning: Vector Space Models of Semantics</article-title>. <source>Journal of Artificial Intelligence Research</source>. <year>2010</year>;<volume>37</volume>:<fpage>141</fpage>.</mixed-citation></ref><ref id="pone.0191175.ref013"><label>13</label><mixed-citation publication-type="other">Velikovich L, Blair-Goldensohn S, Hannan K, McDonald R. The viability of web-derived polarity lexicons. In: Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics; 2010.</mixed-citation></ref><ref id="pone.0191175.ref014"><label>14</label><mixed-citation publication-type="other">Indyk P, Motwani R. Approximate nearest neighbors: towards removing the curse of dimensionality. In: ACM symposium on Theory of computing. STOC; 1998.</mixed-citation></ref><ref id="pone.0191175.ref015"><label>15</label><mixed-citation publication-type="other">Charikar MS. Similarity estimation techniques from rounding algorithms. In: ACM symposium on Theory of computing; 2002.</mixed-citation></ref><ref id="pone.0191175.ref016"><label>16</label><mixed-citation publication-type="journal">
<name><surname>Andoni</surname><given-names>A</given-names></name>, <name><surname>Indyk</surname><given-names>P</given-names></name>. <article-title>Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions</article-title>. <source>Communications of the ACM</source>. <year>2008</year>;<volume>51</volume>(<issue>1</issue>):<fpage>117</fpage>&#x02013;<lpage>122</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/1327452.1327494">10.1145/1327452.1327494</ext-link></comment></mixed-citation></ref><ref id="pone.0191175.ref017"><label>17</label><mixed-citation publication-type="other">Lv Q, Josephson W, Wang Z, Charikar M, Li K. Multi-probe LSH: efficient indexing for high-dimensional similarity search. In: International conference on Very large data bases (VLDB); 2007.</mixed-citation></ref><ref id="pone.0191175.ref018"><label>18</label><mixed-citation publication-type="other">Goyal A, Daum&#x000e9; III H, Guerra R. Fast Large-Scale Approximate Graph Construction for NLP. In: Conference on Empirical Methods in Natural Language Processing and and Computational Natural Language Learning; 2012.</mixed-citation></ref><ref id="pone.0191175.ref019"><label>19</label><mixed-citation publication-type="journal">
<name><surname>Achlioptas</surname><given-names>D</given-names></name>. <article-title>Database-friendly random projections: Johnson-Lindenstrauss with binary coins</article-title>. <source>J Comput Syst Sci</source>. <year>2003</year>;<volume>66</volume>(<issue>4</issue>):<fpage>671</fpage>&#x02013;<lpage>687</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0022-0000(03)00025-4">10.1016/S0022-0000(03)00025-4</ext-link></comment></mixed-citation></ref><ref id="pone.0191175.ref020"><label>20</label><mixed-citation publication-type="other">Weinberger K, Dasgupta A, Langford J, Smola A, Attenberg J. Feature Hashing for Large Scale Multitask Learning. In: ACM International Conference on Machine Learning (ICML); 2009. p. 1113&#x02013;1120.</mixed-citation></ref><ref id="pone.0191175.ref021"><label>21</label><mixed-citation publication-type="other">Pass G, Chowdhury A, Torgeson C. A picture of search. In: International conference on Scalable information systems; 2006.</mixed-citation></ref><ref id="pone.0191175.ref022"><label>22</label><mixed-citation publication-type="other">Petrovi&#x00107; S, Osborne M, Lavrenko V. Streaming First Story Detection with application to Twitter. In: Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics; 2010.</mixed-citation></ref><ref id="pone.0191175.ref023"><label>23</label><mixed-citation publication-type="other">Van Durme B, Lall A. Online Generation of Locality Sensitive Hash Signatures. In: ACL; 2010.</mixed-citation></ref><ref id="pone.0191175.ref024"><label>24</label><mixed-citation publication-type="other">Van Durme B, Lall A. Efficient Online Locality Sensitive Hashing via Reservoir Counting. In: ACL; 2011.</mixed-citation></ref><ref id="pone.0191175.ref025"><label>25</label><mixed-citation publication-type="other">Moran S, Lavrenko V, Osborne M. Variable Bit Quantisation for LSH. In: Annual Meeting of the Association for Computational Linguistics; 2013.</mixed-citation></ref><ref id="pone.0191175.ref026"><label>26</label><mixed-citation publication-type="journal">
<name><surname>Zadeh</surname><given-names>RB</given-names></name>, <name><surname>Goel</surname><given-names>A</given-names></name>. <article-title>Dimension independent similarity computation</article-title>. <source>The Journal of Machine Learning Research</source>. <year>2013</year>;<volume>14</volume>(<issue>1</issue>):<fpage>1605</fpage>&#x02013;<lpage>1626</lpage>.</mixed-citation></ref><ref id="pone.0191175.ref027"><label>27</label><mixed-citation publication-type="journal">
<name><surname>Sundaram</surname><given-names>N</given-names></name>, <name><surname>Turmukhametova</surname><given-names>A</given-names></name>, <name><surname>Satish</surname><given-names>N</given-names></name>, <name><surname>Mostak</surname><given-names>T</given-names></name>, <name><surname>Indyk</surname><given-names>P</given-names></name>, <name><surname>Madden</surname><given-names>S</given-names></name>, <etal>et al</etal>
<article-title>Streaming Similarity Search over one Billion Tweets using Parallel Locality-Sensitive Hashing</article-title>. <source>PVLDB</source>. <year>2013</year>;<volume>6</volume>(<issue>14</issue>):<fpage>1930</fpage>&#x02013;<lpage>1941</lpage>.</mixed-citation></ref><ref id="pone.0191175.ref028"><label>28</label><mixed-citation publication-type="other">Bahmani B, Goel A, Shinde R. Efficient distributed locality sensitive hashing. In: 21st ACM International Conference on Information and Knowledge Management, CIKM&#x02019;12, Maui, HI, USA, October 29&#x02014;November 02, 2012; 2012. p. 2174&#x02013;2178.</mixed-citation></ref><ref id="pone.0191175.ref029"><label>29</label><mixed-citation publication-type="other">Panigrahy R. Entropy based nearest neighbor search in high dimensions. In: Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2006, Miami, Florida, USA, January 22-26, 2006; 2006. p. 1186&#x02013;1195.</mixed-citation></ref></ref-list></back></article>