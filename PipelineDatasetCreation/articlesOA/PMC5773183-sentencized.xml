<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 39.96?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">5773183</article-id><article-id pub-id-type="publisher-id">PONE-D-17-17368</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0191175</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Eukaryota</subject><subj-group><subject>Plants</subject><subj-group><subject>Flowering Plants</subject><subj-group><subject>Vanilla</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Database and Informatics Methods</subject><subj-group><subject>Information Retrieval</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Recall (Memory)</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and Memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Recall (Memory)</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Software Engineering</subject><subj-group><subject>Preprocessing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Software Engineering</subject><subj-group><subject>Preprocessing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Database and Informatics Methods</subject><subj-group><subject>Database Searching</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Information Technology</subject><subj-group><subject>Natural Language Processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and Memory</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title><SecTag type="TITLE"><text><SENT sid="0" pm="."><plain>An evaluation of multi-probe locality sensitive hashing for computing similarities over web-scale query logs </plain></SENT>
</text></SecTag></article-title><alt-title alt-title-type="running-head">An evaluation of multi-probe locality sensitive hashing for computing similarities over web-scale query logs</alt-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0698-0922</contrib-id><name><surname>Cormode</surname><given-names>Graham</given-names></name><role content-type="http://credit.casrai.org/">Writing – original draft</role><role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role><xref ref-type="aff" rid="aff001"><sup>1</sup></xref><xref ref-type="corresp" rid="cor001">*</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Dasgupta</surname><given-names>Anirban</given-names></name><role content-type="http://credit.casrai.org/">Writing – original draft</role><role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role><xref ref-type="aff" rid="aff002"><sup>2</sup></xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Goyal</surname><given-names>Amit</given-names></name><role content-type="http://credit.casrai.org/">Writing – original draft</role><role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Lee</surname><given-names>Chi Hoon</given-names></name><role content-type="http://credit.casrai.org/">Writing – original draft</role><role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role><xref ref-type="aff" rid="aff003"><sup>3</sup></xref></contrib></contrib-group><aff id="aff001">
<label>1</label>
<addr-line>Department of Computer Science, University of Warwick, Coventry, United Kingdom</addr-line>
</aff><aff id="aff002">
<label>2</label>
<addr-line>Computer Science and Engineering, IIT Gandhinagar, Gandhinagar, India</addr-line>
</aff><aff id="aff003">
<label>3</label>
<addr-line>Yahoo Research, Sunnyvale CA, United States of America</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Wang</surname><given-names>Yeng-Tseng</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">
<addr-line>Kaohsiung Medical University, TAIWAN</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><text><SENT sid="1" pm="."><plain>Competing Interests: This work of GC is supported in part by European Research Council grant ERC-2014-CoG 647557, the Yahoo Faculty Research and Engagement Program and a Royal Society Wolfson Research Merit Award. </plain></SENT>
<SENT sid="2" pm="."><plain>These funders did not have any role in the development and execution of this work. </plain></SENT>
<SENT sid="3" pm="."><plain>The work of AD, AG, CHL was carried out while they were employed by Yahoo Research. </plain></SENT>
<SENT sid="4" pm="."><plain>The funder provided support in the form of salaries for authors AD, AG, CHL, but did not have any additional role in the study design, data analysis, decision to publish, or preparation of the manuscript. </plain></SENT>
<SENT sid="5" pm="."><plain>The specific roles of these authors are articulated in the ‘author contributions’ section. </plain></SENT>
<SENT sid="6" pm="."><plain>Some of the data used in the method evaluation study was provided by Yahoo and cannot be shared further by the authors. </plain></SENT>
<SENT sid="7" pm="."><plain>Please see the Data Availability Statement for more information about data access. </plain></SENT>
<SENT sid="8" pm="."><plain>This restriction notwithstanding, these interests do not alter our adherence to PLOS ONE policies on sharing data and materials. </plain></SENT>
</text></p></fn><corresp id="cor001">* E-mail: <email>G.Cormode@warwick.ac.uk</email></corresp></author-notes><pub-date pub-type="collection"><year>2018</year></pub-date><pub-date pub-type="epub"><day>18</day><month>1</month><year>2018</year></pub-date><volume>13</volume><issue>1</issue><elocation-id>e0191175</elocation-id><history><date date-type="received"><day>5</day><month>5</month><year>2017</year></date><date date-type="accepted"><day>3</day><month>12</month><year>2017</year></date></history><permissions><copyright-statement>© 2018 Cormode et al</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Cormode et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0191175.pdf"/><abstract><p><SecTag type="ABS"><text><SENT sid="9" pm="."><plain>Many modern applications of AI such as web search, mobile browsing, image processing, and natural language processing rely on finding similar items from a large database of complex objects. </plain></SENT>
<SENT sid="10" pm="."><plain>Due to the very large scale of data involved (e.g., users’ queries from commercial search engines), computing such near or nearest neighbors is a non-trivial task, as the computational cost grows significantly with the number of items. </plain></SENT>
<SENT sid="11" pm="."><plain>To address this challenge, we adopt Locality Sensitive Hashing (a.k.a, LSH) methods and evaluate four variants in a distributed computing environment (specifically, Hadoop). </plain></SENT>
<SENT sid="12" pm="."><plain>We identify several optimizations which improve performance, suitable for deployment in very large scale settings. </plain></SENT>
<SENT sid="13" pm="."><plain>The experimental results demonstrate our variants of LSH achieve the robust performance with better recall compared with “vanilla” LSH, even when using the same amount of space. </plain></SENT>
</text></SecTag></p></abstract><funding-group><award-group id="award001"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>647557</award-id><principal-award-recipient><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0698-0922</contrib-id><name><surname>Cormode</surname><given-names>Graham</given-names></name></principal-award-recipient></award-group><award-group id="award002"><funding-source><institution>Yahoo Faculty Research Engagement Program</institution></funding-source><principal-award-recipient><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0698-0922</contrib-id><name><surname>Cormode</surname><given-names>Graham</given-names></name></principal-award-recipient></award-group><award-group id="award003"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000288</institution-id><institution>Royal Society</institution></institution-wrap></funding-source><principal-award-recipient><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0698-0922</contrib-id><name><surname>Cormode</surname><given-names>Graham</given-names></name></principal-award-recipient></award-group><funding-statement>This work of GC is supported in part by European Research Council grant ERC-2014-CoG 647557, the Yahoo Faculty Research and Engagement Program and a Royal Society Wolfson Research Merit Award. These funders did not have any role in the development and execution of this work. The work of AD, AG, CHL was carried out while they were employed by Yahoo Research. Yahoo provided the query log data used to evaluate the compared methods. The funder provided support in the form of salaries for authors AD, AG, CHL, and computing resources, but did not have any additional role in the study design, data analysis, decision to publish, or preparation of the manuscript. The specific roles of these authors are articulated in the ‘author contributions’ section.</funding-statement></funding-group><counts><fig-count count="0"/><table-count count="11"/><page-count count="14"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>* Deidentified AOL data is available from the figshare repository <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.5527231.v1">https://doi.org/10.6084/m9.figshare.5527231.v1</ext-link>* QLogs data is proprietary and cannot be released by us. Requests to access this data can be addressed to Yahoo’s academic relations manager, <email>kimcapps@oath.com</email>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>* Deidentified AOL data is available from the figshare repository <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.5527231.v1">https://doi.org/10.6084/m9.figshare.5527231.v1</ext-link>* QLogs data is proprietary and cannot be released by us. Requests to access this data can be addressed to Yahoo’s academic relations manager, <email>kimcapps@oath.com</email>.</p></notes></front><body><SecTag type="INTRO"><sec id="sec001"><title><text><SENT sid="14" pm="."><plain>1 Introduction </plain></SENT>
</text></title><p><text><SENT sid="15" pm="."><plain>Every day, hundreds of millions of people visit web sites and commercial search engines to pose queries on topics of their interest. </plain></SENT>
<SENT sid="16" pm="."><plain>Such queries are typically just a few key words intended to specify the topic that the user has in mind. </plain></SENT>
<SENT sid="17" pm="."><plain>To provide users with a high quality service, search engines such as Bing, Google, and Yahoo require intelligent analysis to realize users’ implicit intents. </plain></SENT>
<SENT sid="18" pm="."><plain>The key resource that they have to help tease out the intent is their large history of requests, in the form of large scale query logs, as well as the log of user actions on the corresponding result pages. </plain></SENT>
<SENT sid="19" pm="."><plain>A key primitive in learning users’ intents is finding the nearest neighbors for a user-given query. </plain></SENT>
<SENT sid="20" pm="."><plain>Computing nearest neighbors is useful for many search-related problems on the Web and Mobile such as finding related queries [1–3], finding near-duplicate queries [4], spelling correction [5, 6], and diversifying search results [7]; and Natural Language Processing (NLP) tasks such as paraphrasing [8, 9], calculating distributional similarity [10–12], and creating sentiment lexicons from large-scale Web data [13]. </plain></SENT>
</text></p><p><text><SENT sid="21" pm="."><plain>In this paper, we focus on the problem of finding nearest neighbors over very large data sets, and ground our study with the application of searching for the best match of a given query from very large scale query logs from a large search engine. </plain></SENT>
<SENT sid="22" pm="."><plain>In order to understand the implicit users’ intent, each query is initially represented in a high dimensional feature space, where each dimension corresponds to a clicked url. </plain></SENT>
<SENT sid="23" pm="."><plain>Given the importance of this question, it is critical to design algorithms that can scale to many queries over huge logs, and allow online and offline computation. </plain></SENT>
<SENT sid="24" pm="."><plain>However, computing nearest neighbors of a query can be very costly. </plain></SENT>
<SENT sid="25" pm="."><plain>Naive solutions that involve a linear search of the set of possibilities are simply infeasible in these settings due to the computational cost of processing hundreds of millions of queries. </plain></SENT>
<SENT sid="26" pm="."><plain>Even though distributed computing environments such as Hadoop make it feasible to store and search large data sets in parallel, the naive pairwise computation is still infeasible. </plain></SENT>
<SENT sid="27" pm="."><plain>The reason is that the total amount of work performed is still huge, and simply throwing more resources at the problem is not effective. </plain></SENT>
<SENT sid="28" pm="."><plain>Given a log of hundreds of millions queries, most are “far” from a query of interest, and we should aim to avoid doing many “useless” comparisons that only confirm that other queries are indeed far from it. </plain></SENT>
</text></p><p><text><SENT sid="29" pm="."><plain>In order to address the computational challenge, this paper aims to find nearest neighbors by doing a small number of comparisons—that is, sublinear in the dataset size—instead of brute force linear search. </plain></SENT>
<SENT sid="30" pm="."><plain>In addition to minimizing the number of comparisons, we aim to retrieve neighboring candidates with 100% precision and high recall. </plain></SENT>
<SENT sid="31" pm="."><plain>It is important that the false positive rate (ratio of “incorrectly” identifying queries as close) is penalized more severely than the false negative rate (ratio of missing “true” neighbors). </plain></SENT>
</text></p><p><text><SENT sid="32" pm="."><plain>When seeking exact matches for queries, effective solutions are based on storing values in a hash table and mapping in via hash functions. </plain></SENT>
<SENT sid="33" pm="."><plain>The generalization of this approach to approximate matches is the framework of Locality Sensitive Hashing, where queries are more likely to collide under the hash function if they are more alike, and less likely to collide if they are less alike. </plain></SENT>
<SENT sid="34" pm="."><plain>The methods we propose in this paper meet our criteria by extending Locality Sensitive Hashing [14–16]. </plain></SENT>
<SENT sid="35" pm="."><plain>In particular, we apply the framework within a distributed system, Hadoop, and take advantage of its distributed computing power. </plain></SENT>
</text></p><p><text><SENT sid="36" pm="."><plain>Our work makes the following contributions: We describe four variants of vanilla LSH motivated by the research on Multi-Probe LSH [17]. </plain></SENT>
<SENT sid="37" pm="."><plain>We show that two of these achieve much better recall than vanilla LSH using the same number of hash tables. </plain></SENT>
<SENT sid="38" pm="."><plain>The main idea behind these variants is to intelligently probe multiple “nearby” buckets within a table that have high probability of containing near neighbors of a query.We present a framework on Hadoop that efficiently finds nearest neighbors for a given query from commercial large-scale query logs in sublinear time.We discuss the applicability of our framework on two real-world applications: finding related queries and removing (near) duplicate queries. </plain></SENT>
<SENT sid="39" pm="."><plain>The algorithms presented in this paper are currently being implemented for production use within a large search provider. </plain></SENT>
</text></p></sec></SecTag><sec id="sec002"><title><text><SENT sid="40" pm="."><plain>2 Problem statement </plain></SENT>
</text></title><p><text><SENT sid="41" pm="."><plain>We start with user query logs C having query vectors collected from a commercial search engine over some domain (e.g. URLs); closeness of queries is measured via cosine similarity on the corresponding vectors. </plain></SENT>
<SENT sid="42" pm="."><plain>Given a set of queries Q and similarity threshold τ, the problem is to develop a batch process to return a small set T of candidate neighbors from C for each query q ∈ Q such that: T = {l ∣ s(l, q) ≥ τ, l ∈ C}, where s(q1, q2) is a function to compute a similarity score between query feature vector q1 and q2;T achieves 100% precision with “large” recall. </plain></SENT>
<SENT sid="43" pm="."><plain>That is, our aim is to achieve high recall, while using a scalable efficient algorithm. </plain></SENT>
</text></p><p><text><SENT sid="44" pm="."><plain>The exact brute force algorithm to solve the above problem would be to compute s(l, q) for all q ∈ Q and all l ∈ C and return those (l, q) where s(l, q) &gt; τ. This approach is computationally infeasible on a single machine, even if the size of Q is of the order of few thousands when the size of C is hundreds of millions. </plain></SENT>
<SENT sid="46" pm="."><plain>Even in a distributed setting such as Hadoop, the resulting communication needed between machines makes this strategy impractical. </plain></SENT>
</text></p><p><text><SENT sid="47" pm="."><plain>Our aim is to study locality sensitive hashing techniques that enable us to return a set of candidate neighbors while performing a much smaller (sublinear in |Q| × |C|) set of comparisons. </plain></SENT>
<SENT sid="48" pm="."><plain>In order to tackle this scalability problem, we explore the combination of distributed computation using a map-reduce platform (Hadoop) as well as locality sensitive hashing (LSH) algorithms. </plain></SENT>
<SENT sid="49" pm="."><plain>We explore a few commonly known variants of LSH and suggest several variants that are suitable to the map-reduce platform. </plain></SENT>
<SENT sid="50" pm="."><plain>The methods that we propose meet the practical requirements of a real life search engine backend, and demonstrates how to use locality sensitive hashing on a distributed platform. </plain></SENT>
</text></p></sec><sec id="sec003"><title><text><SENT sid="51" pm="."><plain>3 Proposed approach </plain></SENT>
</text></title><p><text><SENT sid="52" pm="."><plain>We describe a distributed Locality Sensitive Hashing framework based on map-reduce. </plain></SENT>
<SENT sid="53" pm="."><plain>First, we present the “vanilla” LSH algorithm due to Andoni and Indyk [16]. </plain></SENT>
<SENT sid="54" pm="."><plain>This algorithm builds on prior work on LSH and Point Location in Equal Balls (PLEB) [14, 15]. </plain></SENT>
<SENT sid="55" pm="."><plain>Subsequent prior work on new variants of PLEB [18] for distributional similarity can be seen as implementing a special case of Andoni and Indyk’s LSH algorithm. </plain></SENT>
<SENT sid="56" pm="."><plain>We next present four variants of vanilla LSH motivated by the technique of Multi-Probe LSH [17]. </plain></SENT>
<SENT sid="57" pm="."><plain>A significant drawback of vanilla LSH is that it requires a large number of hash tables in order to achieve good recall in finding nearest neighbors, making the algorithm memory intensive. </plain></SENT>
<SENT sid="58" pm="."><plain>The goal of Multi-probe LSH is to get significantly better recall than the vanilla LSH with the same number of hash tables. </plain></SENT>
<SENT sid="59" pm="."><plain>The main idea behind Multi-probe LSH is to look up multiple buckets within a table that have a high probability of containing the nearest neighbors of a query. </plain></SENT>
<SENT sid="60" pm="."><plain>We present the high-level ideas behind the Multi-probe LSH algorithm; for more details, the reader is referred to [17]. </plain></SENT>
</text></p><sec id="sec004"><title><text><SENT sid="61" pm="."><plain>3.1 Vanilla LSH </plain></SENT>
</text></title><p><text><SENT sid="62" pm="."><plain>The LSH algorithm relies on the existence of a family of locality sensitive hash functions. </plain></SENT>
<SENT sid="63" pm="."><plain>Let H be a family of hash functions mapping RD to some universe S. For any two query terms p, q, we choose h ∈ H uniformly at random and analyze the probability that h(p) = h(q). </plain></SENT>
<SENT sid="65" pm="."><plain>Suppose d is a distance function (e.g. cosine distance), R &gt; 0 is a distance threshold, and c &gt; 1 an approximation factor. </plain></SENT>
<SENT sid="66" pm="."><plain>Let P1, P2 ∈ (0, 1) be two probability thresholds. </plain></SENT>
<SENT sid="67" pm="."><plain>The family H of hash functions is called a (R, cR, P1, P2) locality sensitive family if it satisfies the following conditions: If d(p, q) ≤ R, then Pr[h(p) = h(q)] ≥ P1,If d(p, q) ≥ cR, then Pr[h(p) = h(q)] ≤ P2 </plain></SENT>
</text></p><p><text><SENT sid="68" pm="."><plain>An LSH family is generally interesting when P1 &gt; P2. </plain></SENT>
<SENT sid="69" pm="."><plain>However, the difference between P1 and P2 can be very small. </plain></SENT>
<SENT sid="70" pm="."><plain>Given a family H of hash functions with parameters (R, cR, P1, P2), the LSH algorithm amplifies the gap between the two probabilities P1 and P2 by concatenating K hash functions to create g(⋅) as: g(q) = (h1(q), h2(q), …, hK(q)). </plain></SENT>
<SENT sid="71" pm="."><plain>A larger value of K leads to a larger gap between probabilities of collision for close neighbors (i.e. distance less than R) and those for neighbors that are far (i.e. distance more than cR); the corresponding probabilities are P1K and P2K respectively. </plain></SENT>
<SENT sid="72" pm="."><plain>This amplification ensures high precision by reducing the probability of dissimilar queries having the same hash value. </plain></SENT>
<SENT sid="73" pm="."><plain>To increase the recall of the LSH algorithm, Andoni et al. use L hash tables, each constructed using a different gj(⋅) function, where each gj(⋅) is defined as gj(q) = (h1,j(q), h2,j(q), …, hK,j(q))); ∀1 ≤ j ≤ L. </plain></SENT>
</text></p><p><text><SENT sid="74" pm="."><plain>Algorithm 1 Locality Sensitive Hashing Algorithm </plain></SENT>
</text></p><p specific-use="line"><text><SENT sid="75" pm="."><plain>Preprocessing: Input is N queries with their respective feature vectors. Select L functions gj, j = 1, 2, …, L, setting gj(q) = (h1,j(q), h2,j(q), …, hK,j(q)), where {hi,j, i ∈ [1, K], j ∈ [1, L]} are chosen at random from the LSH family.Construct L hash tables, ∀1 ≤ j ≤ L. All queries with the same gj value (∀1 ≤ j ≤ L) are placed in the same bucket. </plain></SENT>
</text></p><p specific-use="line"><text><SENT sid="77" pm="."><plain>Query: Set of M test queries. </plain></SENT>
<SENT sid="78" pm="."><plain>Let q denote a test query. For each j = 1, 2, …, L Retrieve all the queries from bucket gj(q)Compute cosine similarity between query q and all retrieved queries. </plain></SENT>
<SENT sid="79" pm="."><plain>Return all the queries within threshold τ. </plain></SENT>
</text></p></sec><sec id="sec005"><title><text><SENT sid="80" pm="."><plain>3.2 LSH for cosine similarity </plain></SENT>
</text></title><p><text><SENT sid="81" pm="."><plain>For cosine similarity we adapt the LSH family defined by Charikar [15]. </plain></SENT>
<SENT sid="82" pm="."><plain>The cosine similarity between two queries p,q∈RD is (p.q∥p∥∥q∥). </plain></SENT>
<SENT sid="83" pm="."><plain>The LSH functions for cosine similarity use a random vector α∈RD to define a hash function as hα(p) = sign(α ⋅ p). </plain></SENT>
<SENT sid="84" pm="."><plain>A negative sign is interpreted as 0 and positive sign as 1 to generate indices of buckets in the hash tables (i.e. the range of each gj) as K bit vectors. </plain></SENT>
<SENT sid="85" pm="."><plain>To create α, we exploit the intuition in [19] and sample each coordinate of α from {−1, +1} with equal probability. </plain></SENT>
<SENT sid="86" pm="."><plain>In practice, these are generated by hash functions that maps that index to {−1, +1} (a.k.a. the “hashing trick” of [20]). </plain></SENT>
<SENT sid="87" pm="."><plain>This lets us avoid explicitly storing a (huge) D × K × L random projection matrix. </plain></SENT>
</text></p><p><text><SENT sid="88" pm="."><plain>Algorithm 1 gives the algorithm for creating and querying the data structure. </plain></SENT>
<SENT sid="89" pm="."><plain>In a preprocessing step, the algorithm takes as input N queries along with the associated feature vectors. </plain></SENT>
<SENT sid="90" pm="."><plain>In our application, each query is represented using an extremely sparse and high dimensional feature vector constructed as follows: for query q, we take all the webpages (urls) that any user has clicked on when querying for q. Using this representation, we generate the L different hash values for each query q, where each such hash value is again the concatenation of K hash functions. </plain></SENT>
<SENT sid="92" pm="."><plain>These L hash values per query are then used to create L hash tables. </plain></SENT>
<SENT sid="93" pm="."><plain>Since the width of the index of each bucket is K and each coordinate is one bit, each hash table contains 2K buckets. </plain></SENT>
<SENT sid="94" pm="."><plain>Each query term is placed in its respective buckets in each of the L hash tables. </plain></SENT>
</text></p><p><text><SENT sid="95" pm="."><plain>To retrieve near neighbors, we first find all query terms appearing in the buckets associated with each of the M test queries. </plain></SENT>
<SENT sid="96" pm="."><plain>We compute cosine similarity between each of the retrieved terms and the input test queries and return all those queries as neighbors which are within a similarity threshold (τ). </plain></SENT>
</text></p><p><text><SENT sid="97" pm="."><plain>The above algorithm fits the Map-Reduce setting quite naturally. </plain></SENT>
<SENT sid="98" pm="."><plain>We describe a batch setting which performs the LSH on all queries together to perform an all-pairs comparison; other variations are possible depending on the setting. </plain></SENT>
<SENT sid="99" pm="."><plain>Our implementation performs two map-reduce iterations: in the first phase, the map jobs read in all the queries and their vector representation and outputs key-value pairs that contain the hash-function id (∈[1, L]) and the bucket id (∈[0, 2K − 1]) as the keys and the query as the value. </plain></SENT>
<SENT sid="100" pm="."><plain>The reduce jobs then aggregate all queries belonging to a single bucket for a particular hash function, and output candidate pairs. </plain></SENT>
<SENT sid="101" pm="."><plain>A second map-reduce job then joins these candidate query pairs with their respective feature vectors, computes the exact cosine similarity, and outputs the pairs that have similarity larger than τ, ensuring that our precision is 100%. </plain></SENT>
<SENT sid="102" pm="."><plain>To only consider matches between the M test queries and the N stored queries, we simply tag each query with its type (test or stored), and only consider candidate pairs that have one of each type. </plain></SENT>
<SENT sid="103" pm="."><plain>Our experiments show that this map-reduce implementation scales to hundreds of millions of queries. </plain></SENT>
</text></p></sec><sec id="sec006"><title><text><SENT sid="104" pm="."><plain>3.3 Reusing hash functions </plain></SENT>
</text></title><p><text><SENT sid="105" pm="."><plain>Directly implementing vanilla LSH requires L × K hash functions. </plain></SENT>
<SENT sid="106" pm="."><plain>But generating hash functions is computationally expensive as it takes time to read all features and evaluate hash functions over all those features to generate a single bit. </plain></SENT>
<SENT sid="107" pm="."><plain>To reduce the number of hash functions evaluations, we use a trick from Andoni and Indyk [16] in which hash functions are reused to generate L tables. K is assumed to be even and R≈L. </plain></SENT>
<SENT sid="108" pm="."><plain>We generate fj(q) = (h1,j(q), h2,j(q), …, hK/2,j(q))) of length k/2. </plain></SENT>
<SENT sid="109" pm="."><plain>Next, we define g(q) = (fa, fb), where 1 ≤ a &lt; b ≤ R. Using such pairings, we can thus generate L=R(R-1)2 hash indices. </plain></SENT>
<SENT sid="111" pm="."><plain>This scheme requires O(KL) hash functions, instead of O(KL). </plain></SENT>
<SENT sid="112" pm="."><plain>We use this trick to generate L hash tables with bucket indices of width K bits. </plain></SENT>
</text></p></sec><sec id="sec007"><title><text><SENT sid="113" pm="."><plain>3.4 Multi-Probe LSH </plain></SENT>
</text></title><p><text><SENT sid="114" pm="."><plain>Since generating hash functions can be computationally expensive and the memory required by the algorithm scales linearly with L, the number of hash tables, it is desirable to keep L small. </plain></SENT>
<SENT sid="115" pm="."><plain>The large memory footprint of vanilla LSH makes it impractical for many real applications. </plain></SENT>
<SENT sid="116" pm="."><plain>Here, we first describe four new variants of the vanilla LSH algorithm motivated by the intuition in Multi-probe LSH [17]. </plain></SENT>
<SENT sid="117" pm="."><plain>Multi-probe LSH obtains significantly higher recall than vanilla LSH while using the same number of hash tables. </plain></SENT>
<SENT sid="118" pm="."><plain>The main intuition for Multi-probe LSH is that in addition to looking at the hash bucket that a test query q falls in, it is also possible to look at the neighboring buckets in order to find its near neighbor candidates. </plain></SENT>
<SENT sid="119" pm="."><plain>Multi-probe LSH in [17] suggests exploring neighboring buckets in order of their Hamming distance from the bucket in which q falls. </plain></SENT>
<SENT sid="120" pm="."><plain>They show (empirically) that these neighboring buckets contain the near neighbors with very high probability. </plain></SENT>
<SENT sid="121" pm="."><plain>Though Multi-probe LSH achieves higher recall for the same number of hash tables, it makes more probes as it searches multiple buckets per table. </plain></SENT>
<SENT sid="122" pm="."><plain>The advantage of searching multiple buckets over generating more tables is that less memory and time is required for table creation. </plain></SENT>
</text></p><p><text><SENT sid="123" pm="."><plain>The original Multi-probe LSH algorithm was developed for Euclidean distance. </plain></SENT>
<SENT sid="124" pm="."><plain>However, that algorithm does not immediately translate to our setting of cosine similarity. </plain></SENT>
<SENT sid="125" pm="."><plain>For example, in generating the list of other buckets inspected, [17] utilizes the distance of the hash value to the bucket boundary—this makes sense when the hash value is a real number, but we have bits. </plain></SENT>
<SENT sid="126" pm="."><plain>We present four variants of Multi-probe LSH for cosine similarity: Random Flip Q: Our baseline version first computes the initial LSH of a test query q to give the L bucket ids. </plain></SENT>
<SENT sid="127" pm="."><plain>Next, we create F alternate bucket ids by flipping a set of coordinates randomly in each gj(q). </plain></SENT>
<SENT sid="128" pm="."><plain>For scalability, we restrict our implementation to flipping a single bit out of the K possible bits each time, and ensure that the sampling is done without repetition. </plain></SENT>
<SENT sid="129" pm="."><plain>Since the hash functions are randomly chosen, we implement this by simply flipping the first bit, then revert it and flipping the second bit, until we reach the F’th bit.Random Flip B: The second variant is another baseline similar to the previous one. </plain></SENT>
<SENT sid="130" pm="."><plain>Instead of just flipping the bits for only the test query, here we flips bits for both the test query and all the queries in the database: this increases the “radius” of the search. </plain></SENT>
<SENT sid="131" pm="."><plain>We treat each database point as if it were a query, and flip a random bit in each of its hash representations F times over. Note that this method requires applying flipping to all the queries in the database. </plain></SENT>
<SENT sid="133" pm="."><plain>This is a one-time operation done while creating the database. </plain></SENT>
<SENT sid="134" pm="."><plain>We generate up to F variants of each hash, so for each query, first its L LSH representations of length K are generated. </plain></SENT>
<SENT sid="135" pm="."><plain>On each of the L representations, flipping of bits is applied F times to generate LF representations of a query.Distance Flip Q: The third variant is a smarter version of Random Flip Q. It selects coordinates based on the distance of q from the random hyperplane (hash function) used to create this coordinate. </plain></SENT>
<SENT sid="137" pm="."><plain>The distance of the test query q from the random hyperplane α is the absolute value which we get before applying the sign function on it (see Section 3.2), i.e., abs(α ⋅ q), the distance of q from hyperplane α. This method flips up to F coordinates in order of increasing distance from the hyperplane. </plain></SENT>
<SENT sid="139" pm="."><plain>That is, for each group of K hash values, we sort by the distance to the hyperplane, and swap each of the first F of these in turn. </plain></SENT>
<SENT sid="140" pm="."><plain>As with Random Flip Q, we restrict to flip only a single bit in each repetition, so F ≤ K.Distance Flip B: Our fourth variant flips bits for both the test query and for the queries in the database (i.e., the intelligent version of the second baseline). </plain></SENT>
<SENT sid="141" pm="."><plain>Like Random Flip B, it rquires us to flip all database items, which is a one-time data pre-processing step. </plain></SENT>
</text></p><p><text><SENT sid="142" pm="."><plain>The map-reduce implementation of Multi-probe LSH follows the same structure as the vanilla one—the map phase of the first map-reduce job generates the alternate bucket-ids for both the test query and the queries in the database. </plain></SENT>
<SENT sid="143" pm="."><plain>For all LSH methods, the first preprocessing step is the same, which is to evaluate the hash functions to generate KL bits. </plain></SENT>
<SENT sid="144" pm="."><plain>The second step is to generate tables indexed by the hash function id and bucket id. </plain></SENT>
<SENT sid="145" pm="."><plain>Within the map job, each query is mapped to its various indices. </plain></SENT>
<SENT sid="146" pm="."><plain>For multiprobe LSH, each query is also mapped to additional indices. </plain></SENT>
<SENT sid="147" pm="."><plain>Within the reduce job, all queries with the same index are collected and all colliding pair of queries (that share the same index) are output. </plain></SENT>
<SENT sid="148" pm="."><plain>The final step is to compute similarity for the colliding pairs and only keep those pairs that are above the threshold τ (based on exact comparison using their original feature representation). </plain></SENT>
</text></p></sec><sec id="sec008"><title><text><SENT sid="149" pm="."><plain>3.5 Time cost </plain></SENT>
</text></title><p><text><SENT sid="150" pm="."><plain>The exact running time of these algorithms is hard to predict, as it depends on the distribution of the data, as well as the configuration of the computing environment (number of machines, communication topology etc.). </plain></SENT>
<SENT sid="151" pm="."><plain>Broadly speaking, the time cost is comprised of the preprocessing (the one-time cost to build the database of queries), and the runtime cost to process a new set of query look-ups. </plain></SENT>
<SENT sid="152" pm="."><plain>The communication cost of our algorithms in the Map-Reduce framework is low, since the majority of the work is embarassingly parallel. </plain></SENT>
<SENT sid="153" pm="."><plain>Across all our methods, at most O(KL) hash function evaluations are needed. </plain></SENT>
<SENT sid="154" pm="."><plain>While it may seem that the multiprobe LSH methods require more hash function evaluations, we aim to choose the parameters K and L so that less work is needed overall in order to achieve the same level of recall compared to the vanilla LSH methods. </plain></SENT>
<SENT sid="155" pm="."><plain>The final step, to compute the true similarity of the retrieved pairs, is proportional to the number of collisions. </plain></SENT>
<SENT sid="156" pm="."><plain>We expect the proposed methods to be faster here, since there should be fewer candidates to test. </plain></SENT>
<SENT sid="157" pm="."><plain>This stands in contrast to the naive exact method, which performs an all-pairs comparison. </plain></SENT>
</text></p><p><text><SENT sid="158" pm="."><plain>Due to the variation in real world configurations, we do not explicitly measure the time taken to perform the experiments. </plain></SENT>
<SENT sid="159" pm="."><plain>Rather, we make use of the number of comparisons as a surrogate. </plain></SENT>
<SENT sid="160" pm="."><plain>Our informal tests indicate that this is a robust measure of effort required, since the total CPU time was broadly proportional to this measure across a number of different configurations, while we find that the number of comparisons is not subject to interference from external factors (overall cluster loading etc.). </plain></SENT>
</text></p></sec></sec><sec id="sec009"><title><text><SENT sid="161" pm="."><plain>4 Experiments </plain></SENT>
</text></title><sec id="sec010"><title><text><SENT sid="162" pm="."><plain>4.1 Data </plain></SENT>
</text></title><p><text><SENT sid="163" pm="."><plain>We use two data sources for our experiments. </plain></SENT>
<SENT sid="164" pm="."><plain>The first is the AOL-logs dataset that contains search queries posed to the AOL search engine and that dataset was made available in 2006 [21]. </plain></SENT>
<SENT sid="165" pm="."><plain>This data is accessible from the figshare repository, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.5527231.v1">https://doi.org/10.6084/m9.figshare.5527231.v1</ext-link>. </plain></SENT>
<SENT sid="166" pm="."><plain>We also use a partial sample of query logs from a commercial search engine, denoted as Qlogs. </plain></SENT>
<SENT sid="167" pm="."><plain>Note that realistic query log information is considered confidential and contains potentially sensitive information about individuals. </plain></SENT>
<SENT sid="168" pm="."><plain>We are therefore careful in our handling of the data, and report only aggregate results and carefully chosen examples. </plain></SENT>
<SENT sid="169" pm="."><plain>We do not have permission to share the Qlogs data further, but to allow reproduction of results we show all our analyses on the public data. </plain></SENT>
<SENT sid="170" pm="."><plain>We were provided access to this data on request to Yahoo via an electronic file. </plain></SENT>
<SENT sid="171" pm="."><plain>Requests for access to this data can be addressed to Yahoo’s academic relations manager, mailto:kimcapps@oath.com. </plain></SENT>
</text></p><p><text><SENT sid="172" pm="."><plain>As Qlogs reaches hundreds of millions of queries (approximately 600M unique queries), we generated multiple datasets from Qlogs by sampling at various rates: Qlogs001 represents a 1% sample, Qlogs010 represents a 10% sample and Qlogs100 represents the entire Qlogs. </plain></SENT>
<SENT sid="173" pm="."><plain>The smaller datasets are primarily used to explore parameter ranges and identify suitable values that we then use to experiment with the larger dataset. </plain></SENT>
<SENT sid="174" pm="."><plain>For each query q, a feature vector in a high dimensional feature space, denoted as q = (f1, f2, ⋯, fD), was created by setting fi to be the click through rate of url i when shown in the search results page of search-query q. Note that in our real implementation, q is represented as a sparse feature vector with only non-zero click-through rate features being present. </plain></SENT>
<SENT sid="176" pm="."><plain>In a pre-processing step, we remove all queries with at most five clicked urls. Table 1 summarizes the statistics of our query-log datasets. </plain></SENT>
</text></p><SecTag type="TABLE"><table-wrap id="pone.0191175.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t001</object-id><label>Table 1</label><caption><title><text><SENT sid="177" pm="."><plain>Query-logs statistics. </plain></SENT>
</text></title></caption><alternatives><graphic id="pone.0191175.t001g" xlink:href="pone.0191175.t001"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1"><text><SENT sid="178" pm="."><plain>Data </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="179" pm="."><plain>N </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="180" pm="."><plain>D </plain></SENT>
</text></th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="181" pm="."><plain>AOL-logs </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="182" pm="."><plain>0.3 × 106 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="183" pm="."><plain>0.7 × 106 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="184" pm="."><plain>Qlogs001 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="185" pm="."><plain>6 × 106 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="186" pm="."><plain>66 × 106 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="187" pm="."><plain>Qlogs010 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="188" pm="."><plain>62 × 106 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="189" pm="."><plain>464 × 106 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="190" pm="."><plain>Qlogs100 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="191" pm="."><plain>617 × 106 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="192" pm="."><plain>2.4 × 109 </plain></SENT>
</text></td></tr></tbody></table></alternatives></table-wrap></SecTag><p><text><SENT sid="193" pm="."><plain>Test Data. In all experiments we use a randomly sampled set of 2000 queries Q, as the test set. </plain></SENT>
<SENT sid="194" pm="."><plain>That is, we want to find set T, where T = {l∣s(q, q′) ≥ τ}, s(q, q′) is cosine similarity, and q′ ∈ C for C ∈ {Qlogs001, Qlogs010, Qlogs100,AOL-logs}. </plain></SENT>
<SENT sid="195" pm="."><plain>For most experiments, we set the similarity threshold τ = 0.7, meaning that for q, candidates q′ having cosine similarity of larger than or equal to 0.7 are retrieved. </plain></SENT>
</text></p><p><text><SENT sid="196" pm="."><plain>Evaluation Metrics. We use two metrics for evaluation: recall and number of comparisons. </plain></SENT>
<SENT sid="197" pm="."><plain>The recall of an LSH algorithm measures how well the algorithm can retrieve the true similar candidates. </plain></SENT>
<SENT sid="198" pm="."><plain>The number of comparisons performed by an algorithm is computed as the average number of pairwise comparisons done per test query, and measures the total computation done. </plain></SENT>
<SENT sid="199" pm="."><plain>The aim is to maximize recall and to minimize the number of comparisons. </plain></SENT>
</text></p></sec><sec id="sec011"><title><text><SENT sid="200" pm="."><plain>4.2 Evaluating vanilla LSH </plain></SENT>
</text></title><p><text><SENT sid="201" pm="."><plain>First, we vary the similarity threshold parameter τ in the range {0.7, 0.8, 0.9} while fixing K = 16 and L = 10 for the AOL-logs and Qlogs001 datasets. Table 2 shows that τ = 0.9 achieves higher recall than τ = 0.7. </plain></SENT>
<SENT sid="202" pm="."><plain>This is expected as finding near duplicates is actually easier than finding near neighbors that satisfy only a looser similarity criterion. </plain></SENT>
<SENT sid="203" pm="."><plain>For the rest of this paper, τ is set as 0.7 since it represents the more challenging case. </plain></SENT>
</text></p><SecTag type="TABLE"><table-wrap id="pone.0191175.t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t002</object-id><label>Table 2</label><caption><title><text><SENT sid="204" pm="."><plain>Varying τ with fixed K = 16 and L = 10. </plain></SENT>
</text></title></caption><alternatives><graphic id="pone.0191175.t002g" xlink:href="pone.0191175.t002"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="2" colspan="1"><text><SENT sid="205" pm="."><plain>τ </plain></SENT>
</text></th><th align="center" colspan="2" rowspan="1"><text><SENT sid="206" pm="."><plain>AOL-logs </plain></SENT>
</text></th><th align="center" colspan="2" rowspan="1"><text><SENT sid="207" pm="."><plain>Qlogs001 </plain></SENT>
</text></th></tr><tr><th align="right" rowspan="1" colspan="1"><text><SENT sid="208" pm="."><plain>Comparisons </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="209" pm="."><plain>Recall </plain></SENT>
</text></th><th align="right" rowspan="1" colspan="1"><text><SENT sid="210" pm="."><plain>Comparisons </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="211" pm="."><plain>Recall </plain></SENT>
</text></th></tr></thead><tbody><tr><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="212" pm="."><plain>0.7 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="213" pm="."><plain>57 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="214" pm="."><plain>.63 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="215" pm="."><plain>1052 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="216" pm="."><plain>.67 </plain></SENT>
</text></td></tr><tr><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="217" pm="."><plain>0.8 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"/><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="218" pm="."><plain>.84 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"/><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="219" pm="."><plain>.81 </plain></SENT>
</text></td></tr><tr><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="220" pm="."><plain>0.9 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"/><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="221" pm="."><plain>.98 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"/><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="222" pm="."><plain>.96 </plain></SENT>
</text></td></tr></tbody></table></alternatives></table-wrap></SecTag><p><text><SENT sid="223" pm="."><plain>In the second experiment, we vary R to be in {1, 4, 7, 10}, corresponding to values of L of {1, 10, 28, 55}, while fixing K = 16 on the AOL-logs and Qlogs001 datasets. </plain></SENT>
<SENT sid="224" pm="."><plain>Recall that L denotes the number of hash tables and K is the width of the index of the buckets in the table. </plain></SENT>
<SENT sid="225" pm="."><plain>Increasing K results in increasing precision of the candidate pairs by reducing false positives, but L needs to be correspondingly increased in order to maintain good recall (i.e. reduce false negatives). Table 3 shows that increasing L leads to better recall, at the cost of more comparisons on both datasets. </plain></SENT>
<SENT sid="226" pm="."><plain>In addition, large L means generating many random projection bits and hash tables which is both time and memory intensive. </plain></SENT>
<SENT sid="227" pm="."><plain>Hence, we fix L = 10, to achieve reasonable recall with a tolerable number of comparisons. </plain></SENT>
</text></p><SecTag type="TABLE"><table-wrap id="pone.0191175.t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t003</object-id><label>Table 3</label><caption><title><text><SENT sid="228" pm="."><plain>Varying L with fixed K = 16 and τ = 0.7. </plain></SENT>
</text></title></caption><alternatives><graphic id="pone.0191175.t003g" xlink:href="pone.0191175.t003"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="2" colspan="1"><text><SENT sid="229" pm="."><plain>L </plain></SENT>
</text></th><th align="center" colspan="2" rowspan="1"><text><SENT sid="230" pm="."><plain>AOL-logs </plain></SENT>
</text></th><th align="center" colspan="2" rowspan="1"><text><SENT sid="231" pm="."><plain>Qlogs001 </plain></SENT>
</text></th></tr><tr><th align="right" rowspan="1" colspan="1"><text><SENT sid="232" pm="."><plain>Comparisons </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="233" pm="."><plain>Recall </plain></SENT>
</text></th><th align="right" rowspan="1" colspan="1"><text><SENT sid="234" pm="."><plain>Comparisons </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="235" pm="."><plain>Recall </plain></SENT>
</text></th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="236" pm="."><plain>1 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="237" pm="."><plain>7 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="238" pm="."><plain>.28 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="239" pm="."><plain>106 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="240" pm="."><plain>.36 </plain></SENT>
</text></td></tr><tr><td align="center" style="background-color:#E5E8E8" rowspan="1" colspan="1"><text><SENT sid="241" pm="."><plain>10 </plain></SENT>
</text></td><td align="right" style="background-color:#E5E8E8" rowspan="1" colspan="1"><text><SENT sid="242" pm="."><plain>57 </plain></SENT>
</text></td><td align="char" char="." style="background-color:#E5E8E8" rowspan="1" colspan="1"><text><SENT sid="243" pm="."><plain>.63 </plain></SENT>
</text></td><td align="right" style="background-color:#E5E8E8" rowspan="1" colspan="1"><text><SENT sid="244" pm="."><plain>1052 </plain></SENT>
</text></td><td align="char" char="." style="background-color:#E5E8E8" rowspan="1" colspan="1"><text><SENT sid="245" pm="."><plain>.67 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="246" pm="."><plain>28 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="247" pm="."><plain>152 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="248" pm="."><plain>.77 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="249" pm="."><plain>2908 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="250" pm="."><plain>.78 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="251" pm="."><plain>55 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="252" pm="."><plain>297 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="253" pm="."><plain>.89 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="254" pm="."><plain>5648 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="255" pm="."><plain>.84 </plain></SENT>
</text></td></tr></tbody></table></alternatives></table-wrap></SecTag><p><text><SENT sid="256" pm="."><plain>Next, we vary K in {4, 8, 16} while fixing L = 10. </plain></SENT>
<SENT sid="257" pm="."><plain>As expected, Table 4 shows that increasing K reduces the number of comparisons and worsens recall on both datasets. </plain></SENT>
<SENT sid="258" pm="."><plain>This is intuitive as the larger value of K leads to larger gap between probabilities of collision for queries that are close and those that are far. </plain></SENT>
<SENT sid="259" pm="."><plain>Henceforth, we fix K = 16 to have an acceptable number of comparisons. </plain></SENT>
</text></p><SecTag type="TABLE"><table-wrap id="pone.0191175.t004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t004</object-id><label>Table 4</label><caption><title><text><SENT sid="260" pm="."><plain>Varying K with fixed L = 10 with τ = 0.7. </plain></SENT>
</text></title></caption><alternatives><graphic id="pone.0191175.t004g" xlink:href="pone.0191175.t004"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="2" colspan="1"><text><SENT sid="261" pm="."><plain>K </plain></SENT>
</text></th><th align="center" colspan="2" rowspan="1"><text><SENT sid="262" pm="."><plain>AOL-logs </plain></SENT>
</text></th><th align="center" colspan="2" rowspan="1"><text><SENT sid="263" pm="."><plain>Qlogs001 </plain></SENT>
</text></th></tr><tr><th align="right" rowspan="1" colspan="1"><text><SENT sid="264" pm="."><plain>Comparisons </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="265" pm="."><plain>Recall </plain></SENT>
</text></th><th align="right" rowspan="1" colspan="1"><text><SENT sid="266" pm="."><plain>Comparisons </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="267" pm="."><plain>Recall </plain></SENT>
</text></th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="268" pm="."><plain>4 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="269" pm="."><plain>112,347 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="270" pm="."><plain>.98 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="271" pm="."><plain>2,29,2670 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="272" pm="."><plain>.96 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="273" pm="."><plain>8 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="274" pm="."><plain>11,008 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="275" pm="."><plain>.90 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="276" pm="."><plain>221,132 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="277" pm="."><plain>.88 </plain></SENT>
</text></td></tr><tr><td align="center" style="background-color:#E5E8E8" rowspan="1" colspan="1"><text><SENT sid="278" pm="."><plain>16 </plain></SENT>
</text></td><td align="right" style="background-color:#E5E8E8" rowspan="1" colspan="1"><text><SENT sid="279" pm="."><plain>57 </plain></SENT>
</text></td><td align="char" char="." style="background-color:#E5E8E8" rowspan="1" colspan="1"><text><SENT sid="280" pm="."><plain>.63 </plain></SENT>
</text></td><td align="right" style="background-color:#E5E8E8" rowspan="1" colspan="1"><text><SENT sid="281" pm="."><plain>1,052 </plain></SENT>
</text></td><td align="char" char="." style="background-color:#E5E8E8" rowspan="1" colspan="1"><text><SENT sid="282" pm="."><plain>.67 </plain></SENT>
</text></td></tr></tbody></table></alternatives></table-wrap></SecTag><p><text><SENT sid="283" pm="."><plain>In the fourth experiment, we fix L = 10 and K = 16 as determined above, and we increase the size of training data. Table 5 demonstrates that as we increase data size, the number of comparisons done by the algorithm also increase. </plain></SENT>
<SENT sid="284" pm="."><plain>This result indicates that K needs to be tuned with respect to a specific dataset, as a larger K will reduce the probability of dissimilar queries falling within the same bucket. K and L can be tuned by randomly sampling a small set of queries. </plain></SENT>
<SENT sid="285" pm="."><plain>In this paper, we randomly select 2000 queries to tune parameter K. </plain></SENT>
</text></p><SecTag type="TABLE"><table-wrap id="pone.0191175.t005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t005</object-id><label>Table 5</label><caption><title><text><SENT sid="286" pm="."><plain>Fixed K = 16 and L = 10 with τ = 0.7. </plain></SENT>
</text></title></caption><alternatives><graphic id="pone.0191175.t005g" xlink:href="pone.0191175.t005"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1"><text><SENT sid="287" pm="."><plain>Data </plain></SENT>
</text></th><th align="right" rowspan="1" colspan="1"><text><SENT sid="288" pm="."><plain>Comparisons </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="289" pm="."><plain>Recall </plain></SENT>
</text></th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="290" pm="."><plain>AOL-logs </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="291" pm="."><plain>57 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="292" pm="."><plain>.63 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="293" pm="."><plain>Qlogs001 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="294" pm="."><plain>1,052 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="295" pm="."><plain>.67 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="296" pm="."><plain>Qlogs010 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="297" pm="."><plain>10,515 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="298" pm="."><plain>.64 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="299" pm="."><plain>Qlogs100 </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="300" pm="."><plain>105,126 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="301" pm="."><plain>- </plain></SENT>
</text></td></tr></tbody></table></alternatives></table-wrap></SecTag><p><text><SENT sid="302" pm="."><plain>Table 6 shows the best choices of K for our datasets. </plain></SENT>
<SENT sid="303" pm="."><plain>We note that on Qlogs100 the precision/recall cannot be computed, as it was computationally infeasible to find the exact similar neighbors. </plain></SENT>
<SENT sid="304" pm="."><plain>On our biggest dataset of 600M queries, we set K = 24 and L = 10. </plain></SENT>
<SENT sid="305" pm="."><plain>These settings require only 464 comparisons (on average) to find approximate neighbors compared to exact cosine similarity that involves brute force search over all 600M queries. </plain></SENT>
</text></p><SecTag type="TABLE"><table-wrap id="pone.0191175.t006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t006</object-id><label>Table 6</label><caption><title><text><SENT sid="306" pm="."><plain>Best parameter settings of K (minimizing comparisons and maximizing recall) with L = 10. </plain></SENT>
</text></title></caption><alternatives><graphic id="pone.0191175.t006g" xlink:href="pone.0191175.t006"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1"><text><SENT sid="307" pm="."><plain>Data </plain></SENT>
</text></th><th align="right" rowspan="1" colspan="1"><text><SENT sid="308" pm="."><plain>Comparisons </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="309" pm="."><plain>Recall </plain></SENT>
</text></th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="310" pm="."><plain>AOL-logs (K = 16) </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="311" pm="."><plain>57 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="312" pm="."><plain>.63 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="313" pm="."><plain>Qlogs001 (K = 16) </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="314" pm="."><plain>1,052 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="315" pm="."><plain>.67 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="316" pm="."><plain>Qlogs010 (K = 20) </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="317" pm="."><plain>695 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="318" pm="."><plain>.53 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="319" pm="."><plain>Qlogs100 (K = 24) </plain></SENT>
</text></td><td align="right" rowspan="1" colspan="1"><text><SENT sid="320" pm="."><plain>464 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="321" pm="."><plain>- </plain></SENT>
</text></td></tr></tbody></table></alternatives></table-wrap></SecTag></sec><sec id="sec012"><title><text><SENT sid="322" pm="."><plain>4.3 Evaluating multi-probe LSH </plain></SENT>
</text></title><p><text><SENT sid="323" pm="."><plain>First, we compare flipping F bits in the query only. </plain></SENT>
<SENT sid="324" pm="."><plain>We evaluate two approaches: Random Flip Q and Distance Flip Q. We make several observations from Table 7: 1) As expected, increasing the number of flips improves recall at the expense of more comparisons for both Distance Flip Q and Random Flip Q. 2) The last row of Table 7 shows that when we flip all K bits (F = 16), Distance Flip Q and Random Flip Q converge to the same algorithm, as expected. </plain></SENT>
<SENT sid="327" pm="."><plain>3) We see that Distance Flip Q has significantly better recall than Random Flip Q with a similar number of comparisons. </plain></SENT>
<SENT sid="328" pm="."><plain>In the second row of the table with F = 2, the recall of Distance Flip Q is nine points better than that of Random Flip Q. </plain></SENT>
</text></p><SecTag type="TABLE"><table-wrap id="pone.0191175.t007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t007</object-id><label>Table 7</label><caption><title><text><SENT sid="329" pm="."><plain>Flipping the bits in the query only with K = 16 and L = 10 on AOL-logs with τ = 0.7. </plain></SENT>
</text></title></caption><alternatives><graphic id="pone.0191175.t007g" xlink:href="pone.0191175.t007"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1"><text><SENT sid="330" pm="."><plain>Method </plain></SENT>
</text></th><th align="center" colspan="2" rowspan="1"><text><SENT sid="331" pm="."><plain>Random Flip Q </plain></SENT>
</text></th><th align="center" colspan="2" rowspan="1"><text><SENT sid="332" pm="."><plain>Distance Flip Q </plain></SENT>
</text></th></tr><tr><th align="center" rowspan="1" colspan="1"><text><SENT sid="333" pm="."><plain>F </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="334" pm="."><plain>Comparisons </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="335" pm="."><plain>Recall </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="336" pm="."><plain>Comparisons </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="337" pm="."><plain>Recall </plain></SENT>
</text></th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="338" pm="."><plain>1 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="339" pm="."><plain>108 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="340" pm="."><plain>.65 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="341" pm="."><plain>106 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="342" pm="."><plain>.72 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="343" pm="."><plain>2 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="344" pm="."><plain>159 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="345" pm="."><plain>.66 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="346" pm="."><plain>155 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="347" pm="."><plain>.75 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="348" pm="."><plain>5 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="349" pm="."><plain>311 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="350" pm="."><plain>.70 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="351" pm="."><plain>303 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="352" pm="."><plain>.79 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="353" pm="."><plain>10 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="354" pm="."><plain>557 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="355" pm="."><plain>.75 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="356" pm="."><plain>552 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="357" pm="."><plain>.81 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="358" pm="."><plain>16 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="359" pm="."><plain>839 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="360" pm="."><plain>.82 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="361" pm="."><plain>839 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="362" pm="."><plain>.82 </plain></SENT>
</text></td></tr></tbody></table></alternatives></table-wrap></SecTag><p><text><SENT sid="363" pm="."><plain>Table 8 shows the result of flipping F bits in both query and the database. </plain></SENT>
<SENT sid="364" pm="."><plain>In the second row of Table 8 with F = 2, Distance Flip B has thirteen points better recall than Random Flip B with a similar number of comparisons. </plain></SENT>
<SENT sid="365" pm="."><plain>Comparing across the second row of Tables 7 and 8 shows that flipping bits in both query and database has better recall at the expense of more comparisons. </plain></SENT>
<SENT sid="366" pm="."><plain>This is expected as flipping both means that we increase our “radius of search” to include queries at distance two (one flip in query, one flip in database), and hence have more queries in each table when we probe. </plain></SENT>
<SENT sid="367" pm="."><plain>We also compared distance-based flipping with random flipping on different input sizes, and found that distance-based flipping always has much better recall compared to random flipping (for brevity, we omit these numbers). </plain></SENT>
</text></p><SecTag type="TABLE"><table-wrap id="pone.0191175.t008" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t008</object-id><label>Table 8</label><caption><title><text><SENT sid="368" pm="."><plain>Flipping the bits in both the query and the database with K = 16 and L = 10 on AOL-logs with τ = 0.7. </plain></SENT>
</text></title></caption><alternatives><graphic id="pone.0191175.t008g" xlink:href="pone.0191175.t008"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1"><text><SENT sid="369" pm="."><plain>Method </plain></SENT>
</text></th><th align="center" colspan="2" rowspan="1"><text><SENT sid="370" pm="."><plain>Random Flip B </plain></SENT>
</text></th><th align="center" colspan="2" rowspan="1"><text><SENT sid="371" pm="."><plain>Distance Flip B </plain></SENT>
</text></th></tr><tr><th align="center" rowspan="1" colspan="1"><text><SENT sid="372" pm="."><plain>F </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="373" pm="."><plain>Comparisons </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="374" pm="."><plain>Recall </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="375" pm="."><plain>Comparisons </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="376" pm="."><plain>Recall </plain></SENT>
</text></th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="377" pm="."><plain>1 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="378" pm="."><plain>204 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="379" pm="."><plain>.71 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="380" pm="."><plain>192 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="381" pm="."><plain>.80 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="382" pm="."><plain>2 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="383" pm="."><plain>433 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="384" pm="."><plain>.73 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="385" pm="."><plain>405 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="386" pm="."><plain>.86 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="387" pm="."><plain>5 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="388" pm="."><plain>1557 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="389" pm="."><plain>.86 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="390" pm="."><plain>1475 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="391" pm="."><plain>.93 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="392" pm="."><plain>10 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="393" pm="."><plain>4138 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="394" pm="."><plain>.94 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="395" pm="."><plain>4059 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="396" pm="."><plain>.96 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="397" pm="."><plain>16 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="398" pm="."><plain>5922 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="399" pm="."><plain>.96 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="400" pm="."><plain>5922 </plain></SENT>
</text></td><td align="char" char="." rowspan="1" colspan="1"><text><SENT sid="401" pm="."><plain>.96 </plain></SENT>
</text></td></tr></tbody></table></alternatives></table-wrap></SecTag><p><text><SENT sid="402" pm="."><plain>We select F = 2 as the best parameter setting with goal of maximizing recall by restricting comparisons to a minimum. </plain></SENT>
<SENT sid="403" pm="."><plain>For better recall at the expense of more comparisons, F = 5 can also be selected. </plain></SENT>
<SENT sid="404" pm="."><plain>However, results in Tables 7 and 8 indicate that F &gt; 5 does not increase recall significantly while leading to more comparisons. </plain></SENT>
</text></p><p><text><SENT sid="405" pm="."><plain>Table 9 gives the results of both variants of distance-based Multi Probe, i.e. Distance Flip Q and Distance Flip B, on different sized datasets. </plain></SENT>
<SENT sid="406" pm="."><plain>We present results with the parameters L = 10, F = 2, and value of K chosen as per the values used in the final vanilla LSH experiment. </plain></SENT>
<SENT sid="407" pm="."><plain>As observed there, flipping bits in both query and the database is significantly better in terms of recall with more comparisons. </plain></SENT>
<SENT sid="408" pm="."><plain>The second and third row of the table respectively shows that flipping bits in both query and the database has eight points better recall on both Qlogs001 and Qlogs010 datasets. </plain></SENT>
<SENT sid="409" pm="."><plain>With the goal of maximizing recall with some extra comparisons, we select Distance Flip B as our preferred algorithm. </plain></SENT>
<SENT sid="410" pm="."><plain>Distance Flip B maximizes recall with few tables and comparisons. </plain></SENT>
<SENT sid="411" pm="."><plain>On our entire corpus (Qlogs100) with hundreds of millions of queries, Distance Flip B only requires 3,427 comparisons per test query, compared to hundreds of millions of comparisons by the exact brute force algorithm. </plain></SENT>
<SENT sid="412" pm="."><plain>Distance Flip B returns 9 neighbors on average per given query, averaged over 2000 random test queries. </plain></SENT>
<SENT sid="413" pm="."><plain>Here, many queries are long, and have few neighbors. </plain></SENT>
</text></p><SecTag type="TABLE"><table-wrap id="pone.0191175.t009" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t009</object-id><label>Table 9</label><caption><title><text><SENT sid="414" pm="."><plain>Best parameter settings of K (minimizing comparisons and maximizing recall) with L = 10, F = 2, τ = 0.7. </plain></SENT>
</text></title></caption><alternatives><graphic id="pone.0191175.t009g" xlink:href="pone.0191175.t009"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1"><text><SENT sid="415" pm="."><plain>Method </plain></SENT>
</text></th><th align="center" colspan="2" rowspan="1"><text><SENT sid="416" pm="."><plain>Distance Flip Q </plain></SENT>
</text></th><th align="center" colspan="2" rowspan="1"><text><SENT sid="417" pm="."><plain>Distance Flip B </plain></SENT>
</text></th></tr><tr><th align="center" rowspan="1" colspan="1"><text><SENT sid="418" pm="."><plain>Data </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="419" pm="."><plain>Comps. </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="420" pm="."><plain>Recall </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="421" pm="."><plain>Comps. </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="422" pm="."><plain>Recall </plain></SENT>
</text></th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="423" pm="."><plain>AOL-logs (K = 16) </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="424" pm="."><plain>155 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="425" pm="."><plain>.75 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="426" pm="."><plain>405 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="427" pm="."><plain>.86 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="428" pm="."><plain>Qlogs001 (K = 16) </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="429" pm="."><plain>2980 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="430" pm="."><plain>.76 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="431" pm="."><plain>7904 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="432" pm="."><plain>.84 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="433" pm="."><plain>Qlogs010 (K = 20) </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="434" pm="."><plain>1954 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="435" pm="."><plain>.64 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="436" pm="."><plain>5242 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="437" pm="."><plain>.72 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="438" pm="."><plain>Qlogs100 (K = 24) </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="439" pm="."><plain>1280 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="440" pm="."><plain>- </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="441" pm="."><plain>3427 </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="442" pm="."><plain>- </plain></SENT>
</text></td></tr></tbody></table></alternatives></table-wrap></SecTag></sec><sec id="sec013"><title><text><SENT sid="443" pm="."><plain>4.4 Discussion </plain></SENT>
</text></title><p><text><SENT sid="444" pm="."><plain>Tables 10 and 11 shows some qualitative results for a set of arbitrarily chosen queries. </plain></SENT>
<SENT sid="445" pm="."><plain>These results are found by applying our system (Distance Flip B with parameters L = 10, K = 24, and F = 2) on Qlogs100. </plain></SENT>
<SENT sid="446" pm="."><plain>These results help to highlight several applications that can take significant advantage of the approximate Distance Flip B algorithm presented in this paper. </plain></SENT>
<SENT sid="447" pm="."><plain>For example, the second column in Table 10 shows that the returned approximate similar neighbors can be useful in finding related queries [1, 2]. </plain></SENT>
<SENT sid="448" pm="."><plain>The first column in Table 11 shows an example where we find several popular spelling errors automatically, which can usefully be used for query suggestion. </plain></SENT>
</text></p><SecTag type="TABLE"><table-wrap id="pone.0191175.t010" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t010</object-id><label>Table 10</label><caption><title><text><SENT sid="449" pm="."><plain>10 similar neighbors returned by Distance Flip B with L = 10, K = 24, and F = 2 on Qlogs100 for two example queries. </plain></SENT>
</text></title></caption><alternatives><graphic id="pone.0191175.t010g" xlink:href="pone.0191175.t010"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1"><text><SENT sid="450" pm="."><plain>how lbs in a ton </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="451" pm="."><plain>coldwell banker baileys harbor </plain></SENT>
</text></th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="452" pm="."><plain>how much lbs is a ton </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="453" pm="."><plain>coldwell banker sturgeon bay wi </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="454" pm="."><plain>number of pounds in a ton </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="455" pm="."><plain>coldwell banker door county </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="456" pm="."><plain>how many lb are in a ton </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="457" pm="."><plain>door county wi mls listings </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="458" pm="?"><plain>How many pounds are in a ton? </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="459" pm="."><plain>door county realtors sturgeon bay </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="460" pm="."><plain>how many pounds in a ton </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="461" pm="."><plain>DOOR CTY REAL </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="462" pm="."><plain>1 short ton equals how many pounds </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="463" pm="."><plain>door county coldwell banker </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="464" pm="?"><plain>how many lbs in a ton? </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="465" pm="."><plain>door realty </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="466" pm="?"><plain>how many pounds in a ton? </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="467" pm="."><plain>coldwell banker door county horizons </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="468" pm="."><plain>How many pounds are in a ton </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="469" pm="."><plain>door county coldwell banker real estate </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="470" pm="."><plain>how many lb in a ton </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="471" pm="."><plain>coldwell banker door county wisconsin </plain></SENT>
</text></td></tr></tbody></table></alternatives></table-wrap></SecTag><SecTag type="TABLE"><table-wrap id="pone.0191175.t011" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0191175.t011</object-id><label>Table 11</label><caption><title><text><SENT sid="472" pm="."><plain>10 similar neighbors returned by Distance Flip B with L = 10, K = 24, and F = 2 on Qlogs100 for two example queries. </plain></SENT>
</text></title></caption><alternatives><graphic id="pone.0191175.t011g" xlink:href="pone.0191175.t011"/><table frame="box" rules="all" border="0"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1"><text><SENT sid="473" pm="."><plain>michaels </plain></SENT>
</text></th><th align="center" rowspan="1" colspan="1"><text><SENT sid="474" pm="."><plain>trumbull ct weather </plain></SENT>
</text></th></tr></thead><tbody><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="475" pm="."><plain>maichaels </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="476" pm="."><plain>trumbull ct weather forecast </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="477" pm="."><plain>machaels </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="478" pm="."><plain>weather in trumbull ct </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="479" pm="."><plain>mechaels </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="480" pm="."><plain>weather in trumbull ct 06611 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="481" pm="."><plain>miachaels </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="482" pm="."><plain>trumbull weather forecast </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="483" pm="."><plain>michaeils </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="484" pm="."><plain>trumbull ct 06611 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="485" pm="."><plain>michaelos </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="486" pm="."><plain>trumbull weather ct </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="487" pm="."><plain>michaeks </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="488" pm="."><plain>trumbull ct weather report </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="489" pm="."><plain>michaeels </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="490" pm="."><plain>trumbull connecticut weather </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="491" pm="."><plain>michaelas </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="492" pm="."><plain>weather 06611 </plain></SENT>
</text></td></tr><tr><td align="center" rowspan="1" colspan="1"><text><SENT sid="493" pm="."><plain>michae;ls </plain></SENT>
</text></td><td align="center" rowspan="1" colspan="1"><text><SENT sid="494" pm="."><plain>weather trumbull ct </plain></SENT>
</text></td></tr></tbody></table></alternatives></table-wrap></SecTag><p><text><SENT sid="495" pm="."><plain>One interesting application of near-neighbor finding is to understand specific intents behind the user query. </plain></SENT>
<SENT sid="496" pm="."><plain>Given a user’s query, Bing, Google, and Yahoo often delivers direct display results that summarize expected contents of the query. </plain></SENT>
<SENT sid="497" pm="."><plain>For instance, when a query “f stock price” is issued to search engines, the quick summary of the stock quote with a chart is delivered to the user as the part of the search engine result page. </plain></SENT>
<SENT sid="498" pm="."><plain>Such direct display results are expected to reduce the number of unnecessary clicks by providing the user with the appropriate content early on. </plain></SENT>
<SENT sid="499" pm="."><plain>However, when the query “f today closing price” is issued to search engines, the three major search engines fail to deliver the same direct display experience to the user query, even though its query intent is strongly related to “f stock price”. </plain></SENT>
<SENT sid="500" pm="."><plain>By employing an algorithm similar to Distance Flip B, we can build a synonym database, which will help trigger the same direct display among related queries. </plain></SENT>
<SENT sid="501" pm="."><plain>The first column of Table 10 and the second column of Table 11 show examples of near-duplicate queries that can be automatically answered [4]. </plain></SENT>
</text></p><p><text><SENT sid="502" pm="."><plain>Another application is to remove duplicated instances in a set of suggested results. </plain></SENT>
<SENT sid="503" pm="."><plain>When a query set is retrieved from a repository and presented to users, it is important to remove similar queries from the set so that the user is not distracted by duplicated results. </plain></SENT>
<SENT sid="504" pm="."><plain>Given a set of queries, we can apply Distance Flip B algorithm to build a lookup table of near-duplicates in order to find the “duplicated query terms” efficiently. </plain></SENT>
<SENT sid="505" pm="."><plain>As “near-duplicates” among query terms typically require a “higher” degree of similarity (relatively easier problem) than “relatedness”, we can tune parameters (K, L, F) based on a specific τ (e.g τ = 0.9) from training samples. </plain></SENT>
<SENT sid="506" pm="."><plain>The second column in Table 11 illustrates several effective duplicates: “trumbull weather ct” and “weather in trumbull ct”. </plain></SENT>
</text></p></sec></sec><SecTag type="INTRO"><sec id="sec014"><title><text><SENT sid="507" pm="."><plain>5 Related work </plain></SENT>
</text></title><p><text><SENT sid="508" pm="."><plain>There has been much work in last decade focusing on approximate algorithms for finding similar objects, too much to survey in full, so we highlight some important related publications. </plain></SENT>
<SENT sid="509" pm="."><plain>From the NLP community, prior work on LSH for noun clustering [10] applied the original version of LSH based on Point Location in Equal Balls (PLEB) [14, 15]. </plain></SENT>
<SENT sid="510" pm="."><plain>The disadvantage of vanilla LSH algorithm is that it involves generating a large number of hash functions (in the range L = 1000) and sorting bit vectors of large width (K = 3000). </plain></SENT>
<SENT sid="511" pm="."><plain>To address that issue, Goyal et al. [18] proposed a new variant of PLEB that is faster than the original LSH algorithm but that still requires large number of hash functions (L = 1000). </plain></SENT>
<SENT sid="512" pm="."><plain>In addition, their work can be seen as an implementing a special case of Andoni and Indyk’s LSH algorithm, that was applied to the problem of detecting new events from a stream of Twitter posts [22]. </plain></SENT>
</text></p><p><text><SENT sid="513" pm="."><plain>A major distinction of our research is that existing work deals with approximating cosine similarity by Hamming distance [10, 18, 23–25]. </plain></SENT>
<SENT sid="514" pm="."><plain>Moran et al. [25] proposed a data-driven non-uniform bit allocation across hyperplanes that uses fewer bits than many existing LSH schemes to approximate cosine similarity by Hamming distance. </plain></SENT>
<SENT sid="515" pm="."><plain>In all these existing problem settings, the goal is to minimize both false positives and negatives. </plain></SENT>
<SENT sid="516" pm="."><plain>However, we focus on minimizing false negatives with zero tolerance for false positives. </plain></SENT>
<SENT sid="517" pm="."><plain>[26] developed a distributed version of the LSH algorithm, for the Jaccard distance metric, that scales to very large text corpora by virtue of being implemented on a map-reduce, and by using clever sampling schemes in order to reduce the communication cost. </plain></SENT>
<SENT sid="518" pm="."><plain>Our work addresses the cosine similarity metric, and uses bit flipping in a distributed manner to reduce the number of hash tables in LSH and hence the memory. </plain></SENT>
</text></p><p><text><SENT sid="519" pm="."><plain>Other work in this area has addressed engineering throughput for massively parallel computation [27], distributed LSH for Euclidean distance [28], and variants such as “entropy-based LSH”, also for Euclidean distance [29]. </plain></SENT>
</text></p></sec></SecTag><SecTag type="CONCL"><sec id="sec015"><title><text><SENT sid="520" pm="."><plain>6 Conclusion </plain></SENT>
</text></title><p><text><SENT sid="521" pm="."><plain>In this work, we applied the vanilla LSH algorithm of Andoni et al. to search query similarity applications. </plain></SENT>
<SENT sid="522" pm="."><plain>We proposed four variants of LSH that aim to reduce the number of hash tables used. </plain></SENT>
<SENT sid="523" pm="."><plain>Two of our variants achieve significantly better recall than vanilla LSH while using the same number of hash tables. </plain></SENT>
<SENT sid="524" pm="."><plain>We also present a framework on Hadoop that efficiently finds nearest neighbors for a given query from a commercial large-scale query logs in sublinear time. </plain></SENT>
<SENT sid="525" pm="."><plain>On our entire corpus (Qlogs100) with hundreds of millions of queries, Distance Flip B only requires 3,427 comparisons compared to hundreds of millions of comparisons by exact brute force algorithm. </plain></SENT>
<SENT sid="526" pm="."><plain>In future, we plan to extend our LSH framework to several large-scale NLP, search, and social media applications. </plain></SENT>
</text></p></sec></SecTag></body><back><SecTag type="REF"><ref-list><title>References</title><ref id="pone.0191175.ref001"><text><SENT sid="527" pm="."><plain>1Jones R, Rey B, Madani O, Greiner W. Generating Query Substitutions. </plain></SENT>
<SENT sid="529" pm="."><plain>In: ACM International Conference on World Wide Web (WWW); 2006. </plain></SENT>
</text></ref><ref id="pone.0191175.ref002"><text><SENT sid="530" pm="."><plain>2Jain A, Ozertem U, Velipasaoglu E. Synthesizing High Utility Suggestions for Rare Web Search Queries. </plain></SENT>
<SENT sid="532" pm="."><plain>In: ACM SIGIR Conference on Research and Development in Information Retrieval; 2011. </plain></SENT>
</text></ref><ref id="pone.0191175.ref003"><text><SENT sid="533" pm="."><plain>3Song Y, Zhou D, He Lw. </plain></SENT>
<SENT sid="534" pm="."><plain>Query Suggestion by Constructing Term-transition Graphs. </plain></SENT>
<SENT sid="535" pm="."><plain>In: ACM International Conference on Web Search and Data Mining (WSDM); 2012. </plain></SENT>
</text></ref><ref id="pone.0191175.ref004"><text><SENT sid="536" pm="."><plain>4Lee C, Jain A, Lai L. Assisting web search users by destination reachability. </plain></SENT>
<SENT sid="538" pm="."><plain>In: Conference on Information and Knowledge Management; 2011. </plain></SENT>
</text></ref><ref id="pone.0191175.ref005"><text><SENT sid="539" pm="."><plain>5Ahmad F, Kondrak G. Learning a Spelling Error Model from Search Query Logs. </plain></SENT>
<SENT sid="541" pm="."><plain>In: Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing; 2005. </plain></SENT>
</text></ref><ref id="pone.0191175.ref006"><text><SENT sid="542" pm="."><plain>6Li Y, Duan H, Zhai C. A Generalized Hidden Markov Model with Discriminative Training for Query Spelling Correction. </plain></SENT>
<SENT sid="544" pm="."><plain>In: ACM SIGIR Conference on Research and Development in Information Retrieval; 2012. </plain></SENT>
</text></ref><ref id="pone.0191175.ref007"><text><SENT sid="545" pm="."><plain>7Song Y, Zhou D, He Lw. </plain></SENT>
<SENT sid="546" pm="."><plain>Post-ranking Query Suggestion by Diversifying Search Results. </plain></SENT>
<SENT sid="547" pm="."><plain>In: Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR); 2011. </plain></SENT>
</text></ref><ref id="pone.0191175.ref008"><text><SENT sid="548" pm="."><plain>8Petrovic S, Osborne M, Lavrenko V. Using paraphrases for improving first story detection in news and Twitter. </plain></SENT>
<SENT sid="550" pm="."><plain>In: Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies; 2012. </plain></SENT>
</text></ref><ref id="pone.0191175.ref009"><text><SENT sid="551" pm="."><plain>9Ganitkevitch J, Van Durme B, Callison-Burch C. PPDB: The Paraphrase Database. </plain></SENT>
<SENT sid="553" pm="."><plain>In: North American Chapter of the Association for Computational Linguistics (NAACL); 2013. </plain></SENT>
</text></ref><ref id="pone.0191175.ref010"><text><SENT sid="554" pm="."><plain>10Ravichandran D, Pantel P, Hovy E. Randomized algorithms and NLP: using locality sensitive hash function for high speed noun clustering. </plain></SENT>
<SENT sid="556" pm="."><plain>In: Annual Meeting of the Association for Computational Linguistics; 2005. </plain></SENT>
</text></ref><ref id="pone.0191175.ref011"><text><SENT sid="557" pm="."><plain>11Agirre E, Alfonseca E, Hall K, Kravalova J, Paşca M, Soroa A. A study on similarity and relatedness using distributional and WordNet-based approaches. </plain></SENT>
<SENT sid="559" pm="."><plain>In: Proceedings of HLT-NAACL; 2009. </plain></SENT>
</text></ref><ref id="pone.0191175.ref012"><text><SENT sid="560" pm="."><plain>12 TurneyPD, PantelP. From Frequency to Meaning: Vector Space Models of Semantics. Journal of Artificial Intelligence Research. 2010;37:141. </plain></SENT>
</text></ref><ref id="pone.0191175.ref013"><text><SENT sid="561" pm="."><plain>13Velikovich L, Blair-Goldensohn S, Hannan K, McDonald R. The viability of web-derived polarity lexicons. </plain></SENT>
<SENT sid="563" pm="."><plain>In: Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. </plain></SENT>
<SENT sid="564" pm="."><plain>Association for Computational Linguistics; 2010. </plain></SENT>
</text></ref><ref id="pone.0191175.ref014"><text><SENT sid="565" pm="."><plain>14Indyk P, Motwani R. Approximate nearest neighbors: towards removing the curse of dimensionality. </plain></SENT>
<SENT sid="567" pm="."><plain>In: ACM symposium on Theory of computing. </plain></SENT>
<SENT sid="568" pm="."><plain>STOC; 1998. </plain></SENT>
</text></ref><ref id="pone.0191175.ref015"><text><SENT sid="569" pm="."><plain>15Charikar MS. </plain></SENT>
<SENT sid="570" pm="."><plain>Similarity estimation techniques from rounding algorithms. </plain></SENT>
<SENT sid="571" pm="."><plain>In: ACM symposium on Theory of computing; 2002. </plain></SENT>
</text></ref><ref id="pone.0191175.ref016"><text><SENT sid="572" pm="."><plain>16 AndoniA, IndykP. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. Communications of the ACM. 2008;51(1):117–122. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1145/1327452.1327494">10.1145/1327452.1327494</ext-link> </plain></SENT>
</text></ref><ref id="pone.0191175.ref017"><text><SENT sid="573" pm="."><plain>17Lv Q, Josephson W, Wang Z, Charikar M, Li K. Multi-probe LSH: efficient indexing for high-dimensional similarity search. </plain></SENT>
<SENT sid="575" pm="."><plain>In: International conference on Very large data bases (VLDB); 2007. </plain></SENT>
</text></ref><ref id="pone.0191175.ref018"><text><SENT sid="576" pm="."><plain>18Goyal A, Daumé III H, Guerra R. Fast Large-Scale Approximate Graph Construction for NLP. </plain></SENT>
<SENT sid="578" pm="."><plain>In: Conference on Empirical Methods in Natural Language Processing and and Computational Natural Language Learning; 2012. </plain></SENT>
</text></ref><ref id="pone.0191175.ref019"><text><SENT sid="579" pm="."><plain>19 AchlioptasD. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. J Comput Syst Sci. 2003;66(4):671–687. doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0022-0000(03)00025-4">10.1016/S0022-0000(03)00025-4</ext-link> </plain></SENT>
</text></ref><ref id="pone.0191175.ref020"><text><SENT sid="580" pm="."><plain>20Weinberger K, Dasgupta A, Langford J, Smola A, Attenberg J. Feature Hashing for Large Scale Multitask Learning. </plain></SENT>
<SENT sid="582" pm="."><plain>In: ACM International Conference on Machine Learning (ICML); 2009. p. 1113–1120. </plain></SENT>
</text></ref><ref id="pone.0191175.ref021"><text><SENT sid="584" pm="."><plain>21Pass G, Chowdhury A, Torgeson C. A picture of search. </plain></SENT>
<SENT sid="586" pm="."><plain>In: International conference on Scalable information systems; 2006. </plain></SENT>
</text></ref><ref id="pone.0191175.ref022"><text><SENT sid="587" pm="."><plain>22Petrović S, Osborne M, Lavrenko V. Streaming First Story Detection with application to Twitter. </plain></SENT>
<SENT sid="589" pm="."><plain>In: Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics; 2010. </plain></SENT>
</text></ref><ref id="pone.0191175.ref023"><text><SENT sid="590" pm="."><plain>23Van Durme B, Lall A. Online Generation of Locality Sensitive Hash Signatures. </plain></SENT>
<SENT sid="592" pm="."><plain>In: ACL; 2010. </plain></SENT>
</text></ref><ref id="pone.0191175.ref024"><text><SENT sid="593" pm="."><plain>24Van Durme B, Lall A. Efficient Online Locality Sensitive Hashing via Reservoir Counting. </plain></SENT>
<SENT sid="595" pm="."><plain>In: ACL; 2011. </plain></SENT>
</text></ref><ref id="pone.0191175.ref025"><text><SENT sid="596" pm="."><plain>25Moran S, Lavrenko V, Osborne M. Variable Bit Quantisation for LSH. </plain></SENT>
<SENT sid="598" pm="."><plain>In: Annual Meeting of the Association for Computational Linguistics; 2013. </plain></SENT>
</text></ref><ref id="pone.0191175.ref026"><text><SENT sid="599" pm="."><plain>26 ZadehRB, GoelA. Dimension independent similarity computation. The Journal of Machine Learning Research. 2013;14(1):1605–1626. </plain></SENT>
</text></ref><ref id="pone.0191175.ref027"><text><SENT sid="600" pm="."><plain>27 SundaramN, TurmukhametovaA, SatishN, MostakT, IndykP, MaddenS, et al Streaming Similarity Search over one Billion Tweets using Parallel Locality-Sensitive Hashing. PVLDB. 2013;6(14):1930–1941. </plain></SENT>
</text></ref><ref id="pone.0191175.ref028"><text><SENT sid="601" pm="."><plain>28Bahmani B, Goel A, Shinde R. Efficient distributed locality sensitive hashing. </plain></SENT>
<SENT sid="603" pm="."><plain>In: 21st ACM International Conference on Information and Knowledge Management, CIKM’12, Maui, HI, USA, October 29—November 02, 2012; 2012. p. 2174–2178. </plain></SENT>
</text></ref><ref id="pone.0191175.ref029"><text><SENT sid="605" pm="."><plain>29Panigrahy R. Entropy based nearest neighbor search in high dimensions. </plain></SENT>
<SENT sid="607" pm="."><plain>In: Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2006, Miami, Florida, USA, January 22-26, 2006; 2006. p. 1186–1195. </plain></SENT>
</text></ref></ref-list></SecTag></back></article>
