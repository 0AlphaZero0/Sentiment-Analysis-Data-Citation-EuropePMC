<sec sec-type="intro" id="SEC1"><title>1 INTRODUCTION</title><p>Several of the most intensively studied problems in computational biology are classification tasks: for instance, predicting the function of a gene, the disease state of a patient, the reaction of a patient to a therapy and the phenotype of an individual based on its genotype. The abstract task is to predict the class <italic>y</italic> of an biological subject based on its features <italic>x</italic>. Emerging and existing high-throughput technologies allow us to measure the features of genes, proteins and individuals at an unprecedented resolution and scale, and the hope is that this rich knowledge will lead to ever more accurate data classification.</p><p>One of the most prominent and most successful classification algorithms are Support Vector Machines (SVMs) (<xref ref-type="bibr" rid="B7">Cortes and Vapnik, 1995</xref>; <xref ref-type="bibr" rid="B21">Sch&#x000f6;lkopf and Smola, 2002</xref>). They are based on the idea to separate objects from two classes by means of a hyperplane; new test objects are then predicted to belong to one of these two classes depending on which half-space they are located in. Their popularity is due to several reasons: first, SVMs have shown excellent prediction accuracy in many studies (<xref ref-type="bibr" rid="B17">Noble, 2006</xref>). Second, SVMs can be directly applied to structured data, such as strings (<xref ref-type="bibr" rid="B13">Leslie <italic>et al.</italic>, 2002</xref>) or graphs (<xref ref-type="bibr" rid="B3">Borgwardt <italic>et al.</italic>, 2005</xref>), which are abundant in bioinformatics. Third, SVMs allow for straightforward data integration of several data types (<xref ref-type="bibr" rid="B12">Lanckriet <italic>et al.</italic>, 2004</xref>).</p><p>However, SVMs suffer from one limitation: it is unclear how to correct for confounding variables in SVM predictions. According to Meinert (<xref ref-type="bibr" rid="B15">Meinert and Tonascia, 1986</xref>), a confounder is defined as a variable which is related to two factors of interest, and which falsely obscures or accentuates the relationship between them. In this article, we present an SVM which can correct for observed confounding variables.</p><p>The detrimental effects of confounders are observable in many classification tasks in molecular biology, as illustrated by the following two examples: one may want to predict the phenotypes of plants based on their genotype, typically represented by single nucleotide polymorphisms that represent sequence variation in an individual. In this task, population structure, that is systematic ancestry differences between plants with different phenotypes, may have a confounding effect on the prediction (<xref ref-type="bibr" rid="B20">Price <italic>et al.</italic>, 2010</xref>). For instance, if there is a correlation between population structure and phenotype, the classifier may rely on SNPs that correlate with population structure, and subsequently, its predictions may be wrong on datasets from different geographic origins where the phenotype&#x02013;population correlation is less pronounced or not present.</p><p>Another example is drug treatment response in patients from gene expression profiles. Confounding factors may be the age, the gender or the ethnicity of the patients, each of which may correlate with the treatment response and the expression levels of certain genes (<xref ref-type="bibr" rid="B9">Holsboer, 2008</xref>). When predicting on patients with different age, sex or ethnic background, the learnt classifier may poorly generalize.</p><p>Our goal in this article is to define a confounder-correcting Support Vector Machine (ccSVM) that removes the confounding side information to the largest extent possible. To achieve this, we strive to make the classifier base its prediction on features that do not correlate with the confounding variable.</p><p>The remainder of this article is structured as follows. In <xref ref-type="sec" rid="SEC2">Section 2</xref>, we present the ccSVM (<xref ref-type="sec" rid="SEC2.3">Section 2.3</xref>), and the classifier (<xref ref-type="sec" rid="SEC2.1">Section 2.1</xref>) and the statistical dependence measure (<xref ref-type="sec" rid="SEC2.2">Section 2.2</xref>) it is based upon. We prove that the ccSVM can be computed highly efficiently with existing software packages in <xref ref-type="sec" rid="SEC2.4">Section 2.4</xref>. In <xref ref-type="sec" rid="SEC3">Section 3</xref>, we show that our method improves upon several state-of-the-art classifiers in tumor diagnosis (<xref ref-type="sec" rid="SEC3.3">Section 3.3</xref>), tuberculosis diagnosis (<xref ref-type="sec" rid="SEC3.4">Section 3.4</xref>) and plant phenotype prediction (<xref ref-type="sec" rid="SEC3.5">Section 3.5</xref>). In <xref ref-type="sec" rid="SEC4">Section 4</xref>, we summarize our findings and give an outlook to future work.</p></sec><sec id="SEC2"><title>2 ccSVM Approach</title><p>We first introduce the SVM (<xref ref-type="sec" rid="SEC2.1">Section 2.1</xref>) and the Hilbert-Schmidt Independence Criterion (HSIC) (<xref ref-type="sec" rid="SEC2.2">Section 2.2</xref>), that is the measure of statistical dependence that we use to then define our confounder-correcting SVM (<xref ref-type="sec" rid="SEC2.3">Section 2.3</xref>). In <xref ref-type="sec" rid="SEC2.4">Section 2.4</xref>, we show how to efficiently solve the ccSVM optimization problem.</p><sec id="SEC2.1"><title>2.1 SVMs</title><p>SVMs are supervised learning methods (<xref ref-type="bibr" rid="B21">Sch&#x000f6;lkopf and Smola, 2002</xref>; <xref ref-type="bibr" rid="B25">Vapnik and Chervonenkis, 1974</xref>) that are widely used in molecular biology (<xref ref-type="bibr" rid="B22">Sch&#x000f6;lkopf <italic>et al.</italic>, 2004</xref>). The SVM takes a set of input data with corresponding class labels, and predicts to which class a new input belongs. Suppose we are given the data (<bold>x</bold><sub>1</sub>,<italic>y</italic><sub>1</sub>),&#x000b7;&#x000b7;&#x000b7;,(<bold>x</bold><sub><italic>m</italic></sub>,<italic>y</italic><sub><italic>m</italic></sub>), where <bold>x</bold><sub><italic>i</italic></sub> is an observation and <italic>y</italic><sub><italic>i</italic></sub> is its class label (+1 or &#x02212;1). The original SVM assumes the data are separable by a hyperplane and obtains this hyperplane by maximizing the margin, that is the minimum distance between the hyperplane and points from each class. Once the hyperplane is learnt from the training data, it can be used to predict the class label of new test points. Suppose the hyperplane is in the form of <italic>f</italic>(<bold>x</bold>)=<bold>w</bold><sup><italic>T</italic></sup><bold>x</bold>+<italic>b</italic>, then the model is as follows:
<disp-formula id="M1"><label>(1)</label><graphic xlink:href="btr204m1"/></disp-formula>
subject to
<disp-formula id="M2"><label>(2)</label><graphic xlink:href="btr204m2"/></disp-formula></p><p>By considering the case when data are non-separable, a soft margin SVM was proposed to punish the training errors as follows (<xref ref-type="bibr" rid="B7">Cortes and Vapnik, 1995</xref>):
<disp-formula id="M3"><label>(3)</label><graphic xlink:href="btr204m3"/></disp-formula>
subject to
<disp-formula id="M4"><label>(4)</label><graphic xlink:href="btr204m4"/></disp-formula>
where <italic>C</italic> determines the trade-off between margin maximization and training errors minimization, and &#x003be;<sub><italic>i</italic></sub> is the term by which the object <italic>x</italic><sub><italic>i</italic></sub> violates the inequality (<xref ref-type="disp-formula" rid="M2">2</xref>). Once <bold>w</bold> and <italic>b</italic> are obtained, one can predict the class label for a new observation <bold>x</bold> by the decision function: sgn(<bold>w</bold><sup><italic>T</italic></sup><bold>x</bold>+<italic>b</italic>).</p><p>The dual problem of (<xref ref-type="disp-formula" rid="M3">3</xref>) is
<disp-formula id="M5"><label>(5)</label><graphic xlink:href="btr204m5"/></disp-formula>
under the constraints of
<disp-formula id="M6"><label>(6)</label><graphic xlink:href="btr204m6"/></disp-formula>
The Karush&#x02013;Kuhn&#x02013;Tucker conditions (<xref ref-type="bibr" rid="B11">Kuhn and Tucker, 1951</xref>) imply that <inline-formula><inline-graphic xlink:href="btr204i1.jpg"/></inline-formula>. Thus, after we obtain &#x003b1;<sub><italic>i</italic></sub> by solving (<xref ref-type="disp-formula" rid="M5">5</xref>), the decision function will be
<disp-formula><graphic xlink:href="btr204um1"/></disp-formula></p><p>The kernel trick is to replace <bold>x</bold><sup><italic>T</italic></sup><sub><italic>i</italic></sub><bold>x</bold><sub><italic>j</italic></sub> by <italic>k</italic>(<bold>x</bold><sub><italic>i</italic></sub>,<bold>x</bold><sub><italic>j</italic></sub>)=&#x003d5;(<bold>x</bold><sub><italic>i</italic></sub>)<sup><italic>T</italic></sup>&#x003d5;(<bold>x</bold><sub><italic>j</italic></sub>) in (<xref ref-type="disp-formula" rid="M5">5</xref>), where <italic>k</italic>(<bold>x</bold>,<bold>x</bold>&#x02032;) is a kernel function such that its discretization <bold>K</bold><sub><italic>ij</italic></sub>=<italic>k</italic>(<bold>x</bold><sub><italic>i</italic></sub>,<bold>x</bold><sub><italic>j</italic></sub>) is a positive definite matrix. The decision function can then be represented as
<disp-formula><graphic xlink:href="btr204um2"/></disp-formula></p></sec><sec id="SEC2.2"><title>2.2 HSIC</title><p>The HSIC is a measure of statistical independence (<xref ref-type="bibr" rid="B8">Gretton <italic>et al.</italic>, 2005</xref>). Intuitively, HSIC can be thought of as a squared correlation coefficient between two random variables <italic>x</italic> and <italic>z</italic> computed in feature spaces &#x02131; and &#x1d4a2;.</p><p>In more detail, let <italic>x</italic> be a random variable from the domain &#x1d4b3; and <italic>z</italic> a random variable from the domain &#x1d4b5;. Let &#x02131; and &#x1d4a2; be feature spaces on &#x1d4b3; and &#x1d4b5; with associated kernels <italic>k</italic>:&#x1d4b3;&#x000d7;&#x1d4b3;&#x02192;&#x0211d; and <italic>l</italic>:&#x1d4b5;&#x000d7;&#x1d4b5;&#x02192;&#x0211d;. If we draw pairs of samples (<italic>x</italic>,<italic>z</italic>) and (<italic>x</italic>&#x02032;,<italic>z</italic>&#x02032;) from <italic>x</italic> and <italic>z</italic> according to a joint probability distribution <italic>p</italic><sub>(<italic>x</italic>,<italic>z</italic>)</sub>, then the HSIC can be computed in terms of kernel functions via:
<disp-formula id="M7"><label>(7)</label><graphic xlink:href="btr204m7"/></disp-formula>
<disp-formula id="M8"><label>(8)</label><graphic xlink:href="btr204m8"/></disp-formula>
where <bold>E</bold> is the expectation operator. The empirical estimator of HSIC for a finite sample of points <italic>X</italic> and <italic>Z</italic> from <italic>x</italic> and <italic>z</italic> with <italic>p</italic><sub>(<italic>x</italic>,<italic>z</italic>)</sub> was shown in <xref ref-type="bibr" rid="B8">Gretton <italic>et al.</italic> (2005)</xref> to be
<disp-formula id="M9"><label>(9)</label><graphic xlink:href="btr204m9"/></disp-formula>
where <bold>tr</bold> is the trace of the products of the matrices, <bold>H</bold> is a centering matrix <inline-formula><inline-graphic xlink:href="btr204i2.jpg"/></inline-formula> (where &#x003b4;<sub>(<italic>i</italic>,<italic>j</italic>)</sub>=1 if <italic>i</italic>=<italic>j</italic> and &#x003b4;<sub>(<italic>i</italic>,<italic>j</italic>)</sub>=0 otherwise), <bold>K</bold> and <bold>L</bold> are the kernel matrices on the two random variables of size <italic>m</italic>&#x000d7;<italic>m</italic> and <italic>m</italic> is the number of observations. The larger HSIC, the more likely it is that <italic>X</italic> and <italic>Z</italic> are not independent from each other.</p></sec><sec id="SEC2.3"><title>2.3 The ccSVM</title><p>Via HSIC we can now define an SVM that can use side information to avoid confounding. Suppose <italic>m</italic> samples with their feature vectors (<bold>x</bold><sub>1</sub>,&#x02026;,<bold>x</bold><sub><italic>m</italic></sub>), class labels (<italic>y</italic><sub>1</sub>,&#x02026;,<italic>y</italic><sub><italic>m</italic></sub>) and side information (<bold>z</bold><sub>1</sub>,&#x02026;,<bold>z</bold><sub><italic>m</italic></sub>) are given. <bold>x</bold><sub><italic>i</italic></sub> is a <italic>n</italic>-dimensional column vector representing the features of sample <italic>i</italic>, <italic>y</italic><sub><italic>i</italic></sub>&#x02208;{&#x02212;1,+1} is the class label for <bold>x</bold><sub><italic>i</italic></sub> and <bold>z</bold><sub><italic>i</italic></sub> is the some kind of side information on object <italic>i</italic>, e.g. region, country, age, gender, lab membership or population structure.</p><p><bold>L</bold>&#x02208;&#x0211d;<sup><italic>m</italic>&#x000d7;<italic>m</italic></sup> is a predefined kernel matrix which is generated based on a kernel <italic>l</italic> on the side information, that is <bold>L</bold><sub><italic>ij</italic></sub>=<italic>l</italic>(<bold>z</bold><sub><italic>i</italic></sub>,<bold>z</bold><sub><italic>j</italic></sub>). We call <bold>L</bold> the side information kernel matrix.</p><p>We propose to obtain a classifier by minimizing the following objective function:
<disp-formula id="M10"><label>(10)</label><graphic xlink:href="btr204m10"/></disp-formula>
subject to
<disp-formula id="M11"><label>(11)</label><graphic xlink:href="btr204m11"/></disp-formula>
where &#x02299; represents the element-wise product of two vectors.</p><p>The objective function includes two terms. To minimize the first term is to maximize the classifier margin, as in a standard SVM. The second term tr(<bold>KHLH</bold>) is the HSIC, which measures the independence between two kernels, the reweighted kernel matrix <bold>K</bold> and side information kernel matrix <bold>L</bold>. Here the reweighted kernel <bold>K</bold> is the kernel after reweighting each feature by its weight in <bold>w</bold>.</p><p>To minimize HSIC is to make the dependence between the reweighted kernel matrix and the side information kernel matrix as small as possible. In other words, besides maximizing the margin, the ccSVM also tries to weaken the effect of the side information on the weight vector <bold>w</bold> of the classifier. It rewards solutions in which the input data&#x02014;after being reweighted by weight vector <bold>w</bold>&#x02014;are as independent as possible from the side information, thereby favoring a solution that does not rely on the side information. A constant &#x003bb;&#x0003e;0 determines the trade-off between margin maximization and dependence minimization.</p><p>Note that in practice, a separating hyperplane may not exist. A possible soft margin classifier can be obtained by minimizing the following objective function:
<disp-formula id="M12"><label>(12)</label><graphic xlink:href="btr204m12"/></disp-formula>
subject to
<disp-formula id="M13"><label>(13)</label><graphic xlink:href="btr204m13"/></disp-formula></p><p>Two constants <italic>C</italic> and &#x003bb; determine the trade-off among margin maximization, dependence minimization and training error minimization.</p></sec><sec id="SEC2.4"><title>2.4 Transformation into SVM problem with rescaled input</title><p>Next, we show how to solve the ccSVM optimization problem (<xref ref-type="disp-formula" rid="M12">12</xref>) by rescaling the input of a standard SVM. For this purpose, we denote <bold>HLH</bold> by <inline-formula><inline-graphic xlink:href="btr204i3.jpg"/></inline-formula>, and we define <bold>w</bold>=(<italic>w</italic><sub>1</sub>,&#x02026;,<italic>w</italic><sub><italic>n</italic></sub>)<sup><italic>T</italic></sup> and <bold>x</bold><sub><italic>i</italic></sub>=(<italic>x</italic><sub>1<italic>i</italic></sub>,&#x02026;,<italic>x</italic><sub><italic>ni</italic></sub>)<sup><italic>T</italic></sup>. Then HSIC in (<xref ref-type="disp-formula" rid="M12">12</xref>) can be written as
<disp-formula id="M14"><label>(14)</label><graphic xlink:href="btr204m14"/></disp-formula></p><p>Let <inline-formula><inline-graphic xlink:href="btr204i4.jpg"/></inline-formula>, then (<xref ref-type="disp-formula" rid="M14">14</xref>) is equal to:
<disp-formula><graphic xlink:href="btr204um3"/></disp-formula>
Thus, the objective function in (<xref ref-type="disp-formula" rid="M12">12</xref>) becomes
<disp-formula><graphic xlink:href="btr204um4"/></disp-formula></p><p>Let
<disp-formula id="M15"><label>(15)</label><graphic xlink:href="btr204m15"/></disp-formula>
and
<disp-formula id="M16"><label>(16)</label><graphic xlink:href="btr204m16"/></disp-formula>
for <italic>k</italic>=1,&#x02026;,<italic>n</italic>. Denote <inline-formula><inline-graphic xlink:href="btr204i5.jpg"/></inline-formula> and <inline-formula><inline-graphic xlink:href="btr204i6.jpg"/></inline-formula>. Then the optimization problem (<xref ref-type="disp-formula" rid="M12">12</xref>) becomes:
<disp-formula id="M17"><label>(17)</label><graphic xlink:href="btr204m17"/></disp-formula>
subject to
<disp-formula id="M18"><label>(18)</label><graphic xlink:href="btr204m18"/></disp-formula></p><p>Interestingly, the optimization problem (<xref ref-type="disp-formula" rid="M17">17</xref>) with the constraints in (<xref ref-type="disp-formula" rid="M18">18</xref>) is the standard SVM, which can be solved using libsvm (<xref ref-type="bibr" rid="B6">Chang and Lin, 2001</xref>) or other SVM software. Thus, in order to solve the ccSVM problem (<xref ref-type="disp-formula" rid="M12">12</xref>), one only needs to first rescale each feature according to the formula (<xref ref-type="disp-formula" rid="M16">16</xref>) and then solve a standard SVM problem (<xref ref-type="disp-formula" rid="M12">12</xref>). Note that Equation (<xref ref-type="disp-formula" rid="M17">17</xref>) uses a linear kernel <inline-formula><inline-graphic xlink:href="btr204i7.jpg"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="btr204i8.jpg"/></inline-formula>. While the rescaling step (<xref ref-type="disp-formula" rid="M16">16</xref>) does not lend itself to kernelization, one can kernelize (<xref ref-type="disp-formula" rid="M17">17</xref>) and (<xref ref-type="disp-formula" rid="M18">18</xref>) by replacing <inline-formula><inline-graphic xlink:href="btr204i9.jpg"/></inline-formula> in its dual problem.</p></sec></sec><sec id="SEC3"><title>3 EXPERIMENTS</title><p>In our experiments, we examine three different applications of the ccSVM in bioinformatics: microarray cross-platform comparability on a simulated dataset, disease outcome prediction with correction for various kinds of side information and phenotype prediction with population structure correction.</p><sec id="SEC3.1"><title>3.1 Parameter selection</title><p>There are two parameters in the ccSVM model (<xref ref-type="disp-formula" rid="M12">12</xref>): &#x003bb; and <italic>C</italic>. We choose the parameters based on cross-validation on the training dataset only. We split all the training data into several (for example, 5) folds, and each time we take 1-fold as test set and the others as training set. We first set &#x003bb;=0 and select the <italic>C</italic> by which we can get the best average area under curve (AUC) using a standard SVM. <italic>C</italic> can take one of the values in {2<sup>&#x02212;8</sup>,2<sup>&#x02212;4</sup>,2<sup>&#x02212;2</sup>,1,2<sup>2</sup>,2<sup>4</sup>,2<sup>8</sup>}. Then we fix <italic>C</italic> in the ccSVM, and select the &#x003bb; such that it gives the best average AUC in the ccSVM. &#x003bb; is chosen from the values {10<sup>&#x02212;8</sup>,10<sup>&#x02212;4</sup>,10<sup>&#x02212;2</sup>,1,10<sup>2</sup>,10<sup>4</sup>,10<sup>8</sup>}. This parameter selection is performed on the training dataset only.</p></sec><sec id="SEC3.2"><title>3.2 Comparison partners</title><p>We compare the ccSVM to the following comparison partners:
<list list-type="bullet"><list-item><p>Standard SVM: we use linear kernel <bold>K</bold><sub>SVM</sub>=<bold>X</bold><sup><italic>T</italic></sup><bold>X</bold> in the standard SVM, where <bold>X</bold>=(<bold>x</bold><sub>1</sub>,&#x02026;,<bold>x</bold><sub><italic>m</italic></sub>)&#x02208;&#x0211d;<sup><italic>n</italic>&#x000d7;<italic>m</italic></sup>.</p></list-item><list-item><p>(K+L)SVM: we integrate the side information with the original features by simply concatenating <bold>X</bold> and <bold>L</bold>. Thus, the number of features are <italic>n</italic>+<italic>m</italic>, where <italic>n</italic> is the number of original features, and <italic>m</italic> is the number of side features. The linear kernel will be <bold>K</bold><sub>(K+L)SVM</sub>=<bold>K</bold><sub>SVM</sub>+<bold>L</bold><sup><italic>T</italic></sup><bold>L</bold>. (K+L)SVM means that we use a standard SVM with kernel matrix <bold>K</bold><sub>(K+L)SVM</sub>.</p></list-item><list-item><p>pcaSVM: we consider the first component from principle component analysis (PCA) to be most related to the side information, and then weaken the side-effect by removing it from the kernel matrix. <xref ref-type="bibr" rid="B19">Price <italic>et al.</italic> (2006</xref>) used a similar approach to correct for stratification in genome-wide association studies. Suppose the largest eigenvalue of <bold>K</bold><sub>SVM</sub>=<bold>X</bold><sup><italic>T</italic></sup><bold>X</bold> is &#x003c3; and its corresponding eigenvector is <italic>v</italic>, then define the PCA correction kernel <bold>K</bold><sub>PCA</sub>=<bold>K</bold><sub>SVM</sub>&#x02212;&#x003c3;<italic>vv</italic><sup><italic>T</italic></sup>. pcaSVM means that we use a standard SVM with kernel matrix <bold>K</bold><sub>PCA</sub>.</p></list-item><list-item><p>Confounder correcting logistic regression (ccLR): we consider the following logistic model
<disp-formula><graphic xlink:href="btr204um5"/></disp-formula>
where <italic>p</italic> is the probability of a sample being in one class (e.g. the positive class), &#x003b2;<sub><italic>i</italic></sub> and <italic>u</italic><sub><italic>i</italic></sub> are parameters, <italic>x</italic><sub><italic>i</italic></sub> are the original features and <italic>l</italic><sub><italic>i</italic></sub> are the side features included in <bold>L</bold>. <xref ref-type="bibr" rid="B10">Kang <italic>et al.</italic> (2010</xref>) applied a related mixed-model approach to correct for population structure in genome-wide association studies. In contrast to our approach, they are interested in quantitative phenotypes. In our experiments, besides standard logistic regression with maximum likelihood, a sparse Bayesian logistic regression model BLogReg (<xref ref-type="bibr" rid="B5">Cawley and Talbot, 2006</xref>) is also used to estimate the parameters &#x003b2;<sub><italic>i</italic></sub> and <italic>u</italic><sub><italic>i</italic></sub>. ccLR with these two parameter estimation methods are denoted as ccLR(ML) and ccLR(BR), respectively.</p></list-item></list>
</p></sec><sec id="SEC3.3"><title>3.3 Microarray cross-platform comparability</title><p>In this experiment, we compared the sensitivity of the ccSVM to a standard SVM on a microarray dataset which consists of samples from two different labs. A synthetic dataset was also generated to compare the ccSVM and standard SVM.</p><p><italic>Data</italic>: <xref ref-type="bibr" rid="B26">P.Warnat <italic>et al.</italic> (2005</xref>) compared two studies on acute myeloid leukemia (AML): <xref ref-type="bibr" rid="B4">Bullinger <italic>et al.</italic> (2004</xref>) and <xref ref-type="bibr" rid="B24">Valk <italic>et al.</italic> (2004</xref>). The dataset Bullinger consists of 52 patients, and the dataset Valk of 97 patients. Both datasets share gene expression levels for <italic>n</italic>=7102 genes. The prediction task is to differentiate between cancerous and normal tissue. The experiments of Bullinger <italic>et al.</italic> were carried out on a cDNA platform while Valk <italic>et al.</italic> used oligonucleotide microarrays.</p><p>Besides the real data, we also generated a synthetic dataset based on Bullinger and Valk: we picked randomly half of the genes and centered them to zero mean for each gene and each dataset separately, and kept the other half genes uncentered. The centered genes have no correlation with the lab membership while many of the uncentered genes have a strong correlation. Hence, difference in mean expression level seems to distinguish the expression values from these two labs.</p><p>We defined the side information matrix <bold>L</bold>&#x02208;&#x0211d;<sup><italic>m</italic>&#x000d7;<italic>m</italic></sup> by the lab membership. <bold>L</bold><sub><italic>ij</italic></sub>=1 if patient <italic>i</italic> and patient <italic>j</italic> belong to the same lab, and <bold>L</bold><sub><italic>ij</italic></sub>=0 if the two patients belong to different labs.</p><p><italic>Experimental setting</italic>: we first did 50 times 5-fold random cross-validation on the real data using the ccSVM and SVM, and report their average AUCs, standard errors and <italic>t</italic>-test <italic>P</italic>-values. For the ccSVM, we split the data randomly into 5-folds. We used 4-folds for training and 1-fold for testing. Then we fixed the parameters &#x003bb; and <italic>C</italic> as explained in <xref ref-type="sec" rid="SEC3.1">Section 3.1</xref> with 4-fold cross-validation. With the obtained parameters, we trained the ccSVM on the training set and predicted on the test objects. The experiment was repeated five times until each fold served as test dataset once. For standard SVM and pcaSVM, we used the same experimental protocol, but we only needed to train <italic>C</italic> from the training data.</p><p>We then explored how the ccSVM corrects the normalized weight vector based on the synthetic data. We trained on a subset of the pooled Bullinger and Valk dataset. We determined the parameter <italic>C</italic> according to the experimental protocol outlined in <xref ref-type="sec" rid="SEC3.1">Section 3.1</xref> and fixed &#x003bb;=1. Therefore, we split the training set into 3-folds. With these optimized parameters, we trained our ccSVM jointly over all training objects and predicted on the test dataset. For training the standard SVM, we used the same experimental protocol.</p><p><italic>Results</italic>: for the real data, we obtain an average AUC value of 0.911&#x000b1;0.002 for the ccSVM and an AUC value of 0.822&#x000b1;0.003 for the standard SVM. The <italic>P</italic>-value of the <italic>t</italic>-test is 4.8e-40. This result shows that our method is superior to the standard SVM.</p><p>For the synthetic data, we can see from <xref ref-type="fig" rid="F1">Figure 1</xref> that the ccSVM assigns large weights to genes that weakly correlate with the lab membership while the standard SVM assigns the weights without paying attention to the correlation to the lab membership.
<fig id="F1" position="float"><label>Fig. 1.</label><caption><p>Genes are sorted according to the weight vector of the ccSVM (blue dashed line) and according to the weight vector of the standard SVM (green line). The correlation coefficient between each gene expression level and lab membership is calculated. The averaged absolute correlation coefficient of the top <italic>i</italic> genes is plotted for gene <italic>i</italic>.</p></caption><graphic xlink:href="btr204f1"/></fig></p></sec><sec id="SEC3.4"><title>3.4 Disease outcome prediction with various confounding factors</title><p>In this experiment, we analyzed the ability of the ccSVM to predict active tuberculosis based on blood transcriptional profiles. We used ethnicity, age and gender as confounding information.</p><p><italic>Data</italic>: we obtained the dataset from <xref ref-type="bibr" rid="B2">Berry <italic>et al.</italic> (2010</xref>). It includes 103 blood samples from patients with active tuberculosis and 40 blood samples from healthy controls. The transcriptional signature of the blood samples were measured in a subsequent microarray experiment with <italic>n</italic>=48 803 gene expression levels.</p><p>We used three different confounding factors: ethnicity, gender and age. For ethnicity, we defined the information matrix as follows: <bold>L</bold><sub><italic>ij</italic></sub>=1 if the patient <italic>i</italic> and <italic>j</italic> belong to the same ethnic group, <bold>L</bold><sub><italic>ij</italic></sub>=0 if they do not. For gender, we defined <bold>L</bold> similarly: <bold>L</bold><sub><italic>ij</italic></sub>=1 if the patient <italic>i</italic> and <italic>j</italic> have the same gender, <bold>L</bold><sub><italic>ij</italic></sub>=0 if the patients have different gender. We used a Gaussian kernel for age as side information.</p><p><italic>Experimental setting</italic>: for the ccSVM, standard SVM, pcaSVM and (K+L)SVM, we used the same experimental setting as described in <xref ref-type="sec" rid="SEC3.3">Section 3.3</xref>. We again utilized the same experimental design for ccLR, but instead of setting the parameters (&#x003bb;,<italic>C</italic>), we determined the parameters &#x003b2;<sub>0</sub>,&#x02026;,&#x003b2;<sub><italic>n</italic></sub> and <italic>u</italic><sub>1</sub>,&#x02026;,<italic>u</italic><sub><italic>m</italic></sub>.</p><p>We ran 50 times random 5-fold cross-validation for standard SVM, pcaSVM,(K+L)SVM and ccSVM, and reported their corresponding average AUCs and standard errors. We also performed a <italic>t</italic>-test between the 50 AUCs of competing partners and 50 AUCs of ccSVM, and recorded the <italic>P</italic>-values. As ccLR and BLogReg did not work well, we performed logistic regression with maximum likelihood estimation in 10 times 5-fold cross-validation and reported the averaged AUC.</p><p><italic>Results</italic>: <xref ref-type="table" rid="T1">Table 1</xref> shows the prediction results for random cross-validation. Regarding the AUC values, ccSVMs with side information of ethnicity and age are slightly better than the other SVM approaches, while ccSVM with gender as side information works similar with the other SVMs. The logistic regression approach is not able to classify the data correctly regardless of which side information is used.
<table-wrap id="T1" position="float"><label>Table 1.</label><caption><p>AUC and <italic>P</italic>-values for ccSVM, standard SVM, pcaSVM, (K+L)SVM and ccLR for the three different confounding variables on the Tuberculosis dataset</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th align="left" rowspan="1" colspan="1">Side information</th><th align="left" rowspan="1" colspan="1">AUC<sub>ccSVM</sub></th><th align="left" rowspan="1" colspan="1">AUC<sub>SVM</sub></th><th align="left" rowspan="1" colspan="1">p<sub>SVM</sub></th><th align="left" rowspan="1" colspan="1">AUC<sub>pcaSVM</sub></th><th align="left" rowspan="1" colspan="1">p<sub>pcaSVM</sub></th><th align="left" rowspan="1" colspan="1">AUC<sub>(K+L)SVM</sub></th><th align="left" rowspan="1" colspan="1">p<sub>(K+L)SVM</sub></th><th align="left" rowspan="1" colspan="1">AUC<sub>ccLR(ML)</sub></th></tr></thead><tbody align="left"><tr><td align="left" rowspan="1" colspan="1">Ethnicity</td><td align="left" rowspan="1" colspan="1">0.955&#x000b1;0.002</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">6.3e-05</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">3.6e-09</td><td align="left" rowspan="1" colspan="1">0.942&#x000b1;0.003</td><td align="left" rowspan="1" colspan="1">1.2e-04</td></tr><tr><td align="left" rowspan="1" colspan="1">Age</td><td align="left" rowspan="1" colspan="1">0.967&#x000b1;0.002</td><td align="left" rowspan="1" colspan="1">0.939&#x000b1;0.003</td><td align="left" rowspan="1" colspan="1">3.8e-12</td><td align="left" rowspan="1" colspan="1">0.933&#x000b1;0.003</td><td align="left" rowspan="1" colspan="1">1.5e-18</td><td align="left" rowspan="1" colspan="1">0.943&#x000b1;0.002</td><td align="left" rowspan="1" colspan="1">4.0e-16</td><td align="left" rowspan="1" colspan="1">0.49</td></tr><tr><td align="left" rowspan="1" colspan="1">Gender</td><td align="left" rowspan="1" colspan="1">0.938&#x000b1;0.003</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">2.8e-01</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">6.2e-01</td><td align="left" rowspan="1" colspan="1">0.941&#x000b1;0.003</td><td align="left" rowspan="1" colspan="1">1.7e-01</td><td align="left" rowspan="1" colspan="1">0.499</td></tr></tbody></table></table-wrap>
</p><p><italic>Weight vector analysis</italic>: we examined the weight vector of the ccSVM to get a further understanding for its improved performance. Specifically, we trained on four ethnic groups and then used it to predict on a fifth. In <xref ref-type="fig" rid="F2">Figure 2</xref>, we plot the averaged absolute correlation coefficients between membership in one ethnicity (African) and the expression levels of the 10 000 top ranked genes. We can observe that the ccSVM assigns the largest weights to genes that do not correlate with the confounder, while the standard SVM is unaware of the confounder and puts large weight on the features that correlate with the confounding variable.
<fig id="F2" position="float"><label>Fig. 2.</label><caption><p>Gene expression levels are sorted according to the weight vector of ccSVM (blue dashed line) and according to the weight vector of standard SVM (green line). The correlation coefficient between each gene expression level and ethnic origin (African) is calculated. The averaged absolute correlation coefficient of the top <italic>i</italic> genes is plotted for gene <italic>i</italic>.</p></caption><graphic xlink:href="btr204f2"/></fig></p></sec><sec id="SEC3.5"><title>3.5 Phenotype prediction with population structure correction</title><p>In this experiment, we assessed the performance of the ccSVM in comparison to the standard SVM, (K+L)SVM, pcaSVM and ccLR on phenotype prediction from SNP data in <italic>Arabidopsis thaliana</italic>.</p><p><italic>Data</italic> : we used data from the genome-wide association study in <italic>A.thaliana</italic> conducted by <xref ref-type="bibr" rid="B1">Atwell <italic>et al.</italic> (2010</xref>). The dataset consists of <italic>m</italic>=177 samples and <italic>n</italic>=216 130 single nucleotide polymorphisms (SNPs). An SNP is a fixed position in the genome which exists in two different variations between individuals. We examined five binary phenotypes, namely the presence and absence of chlorosis at 22&#x000b0;C (PID:169), of anthocyanin at 16&#x000b0;C (PID:171) and at 22&#x000b0;C (PID:172) and of leaf roll at 10&#x000b0;C (PID:176) and at 22&#x000b0;C (PID:178).</p><p>We used population structure as side information and computed a side information kernel matrix <bold>L</bold>&#x02208;&#x0211d;<sup><italic>m</italic>&#x000d7;<italic>m</italic></sup>. Population structure is defined by the different allele frequencies between subpopulations. If the phenotype prevalence also differs between these subpopulations, it can lead to spurious associations between the phenotype and SNPs that are associated with a subpopulation in which one phenotype is prevalent (<xref ref-type="bibr" rid="B14">Marchini <italic>et al.</italic>, 2004</xref>). Each entry <bold>L</bold><sub><italic>ij</italic></sub> is here defined as the number of common SNPs between sample <italic>i</italic> and sample <italic>j</italic>.</p><p><italic>Experimental setting</italic>: for this experiment, we used the same experimental setting as described in <xref ref-type="sec" rid="SEC3.4">Subsection 3.4</xref>.</p><p><italic>Results</italic>: prediction results are reported in <xref ref-type="table" rid="T2">Table 2</xref>. For all the phenotypes except leaf roll at 22&#x000b0;C (PID:178), ccSVM yields better AUC values than the state-of-the-art competitors. Regarding the <italic>P</italic>-values, we see that the improvement of our method against standard SVM, pcaSVM and (K+L)SVM is significant for the phenotypes chlorosis at 22&#x000b0;C (PID:169), anthocyanin at 16&#x000b0;C (PID:171), anthocyanin at 22&#x000b0;C (PID:172) and leaf roll at 10&#x000b0;C (PID:176).
<table-wrap id="T2" position="float"><label>Table 2.</label><caption><p>AUC and <italic>P</italic>-values for ccSVM, standard SVM, pcaSVM, (K+L)SVM and ccLR for the five different <italic>Arabidopsis</italic> phenotypes</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th align="left" rowspan="1" colspan="1">PID</th><th align="left" rowspan="1" colspan="1">Phenotype</th><th align="left" rowspan="1" colspan="1">AUC<sub>ccSVM</sub></th><th align="left" rowspan="1" colspan="1">AUC<sub>SVM</sub></th><th align="left" rowspan="1" colspan="1">p<sub>SVM</sub></th><th align="left" rowspan="1" colspan="1">AUC<sub>pcaSVM</sub></th><th align="left" rowspan="1" colspan="1">p<sub>pcaSVM</sub></th><th align="left" rowspan="1" colspan="1">AUC<sub>(K+L)SVM</sub></th><th align="left" rowspan="1" colspan="1">p<sub>(K+L)SVM</sub></th><th align="left" rowspan="1" colspan="1">AUC<sub>ccLR(ML)</sub></th><th align="left" rowspan="1" colspan="1">AUC<sub>ccLR(BR)</sub></th></tr></thead><tbody align="left"><tr><td align="left" rowspan="1" colspan="1">169</td><td align="left" rowspan="1" colspan="1">Chlorosis at 22&#x000b0;C</td><td align="left" rowspan="1" colspan="1">0.658&#x000b1;0.004</td><td align="left" rowspan="1" colspan="1">0.623&#x000b1;0.004</td><td align="left" rowspan="1" colspan="1">8.3e-10</td><td align="left" rowspan="1" colspan="1">0.625&#x000b1;0.004</td><td align="left" rowspan="1" colspan="1">6.4e-09</td><td align="left" rowspan="1" colspan="1">0.574&#x000b1;0.004</td><td align="left" rowspan="1" colspan="1">2.2e-28</td><td align="left" rowspan="1" colspan="1">0.632&#x000b1;0.006</td><td align="left" rowspan="1" colspan="1">0.523&#x000b1;0.004</td></tr><tr><td align="left" rowspan="1" colspan="1">171</td><td align="left" rowspan="1" colspan="1">Anthocyanin at 16&#x000b0;C</td><td align="left" rowspan="1" colspan="1">0.590&#x000b1;0.005</td><td align="left" rowspan="1" colspan="1">0.568&#x000b1;0.005</td><td align="left" rowspan="1" colspan="1">1.2e-03</td><td align="left" rowspan="1" colspan="1">0.570&#x000b1;0.004</td><td align="left" rowspan="1" colspan="1">2.1e-03</td><td align="left" rowspan="1" colspan="1">0.560&#x000b1;0.004</td><td align="left" rowspan="1" colspan="1">2.1e-06</td><td align="left" rowspan="1" colspan="1">0.571&#x000b1;0.012</td><td align="left" rowspan="1" colspan="1">0.571&#x000b1; 0.003</td></tr><tr><td align="left" rowspan="1" colspan="1">172</td><td align="left" rowspan="1" colspan="1">Anthocyanin at 22&#x000b0;C</td><td align="left" rowspan="1" colspan="1">0.628&#x000b1;0.003</td><td align="left" rowspan="1" colspan="1">0.610&#x000b1;0.003</td><td align="left" rowspan="1" colspan="1">2.7e-05</td><td align="left" rowspan="1" colspan="1">0.610&#x000b1;0.004</td><td align="left" rowspan="1" colspan="1">1.2e-04</td><td align="left" rowspan="1" colspan="1">0.576&#x000b1;0.003</td><td align="left" rowspan="1" colspan="1">1.8e-21</td><td align="left" rowspan="1" colspan="1">0.613&#x000b1;0.004</td><td align="left" rowspan="1" colspan="1">0.552&#x000b1;0.004</td></tr><tr><td align="left" rowspan="1" colspan="1">176</td><td align="left" rowspan="1" colspan="1">Leaf Roll at 10&#x000b0;C</td><td align="left" rowspan="1" colspan="1">0.720&#x000b1;0.002</td><td align="left" rowspan="1" colspan="1">0.695&#x000b1;0.003</td><td align="left" rowspan="1" colspan="1">2.6e-09</td><td align="left" rowspan="1" colspan="1">0.697&#x000b1;0.003</td><td align="left" rowspan="1" colspan="1">3.8e-08</td><td align="left" rowspan="1" colspan="1">0.653&#x000b1;0.003</td><td align="left" rowspan="1" colspan="1">3.3e-31</td><td align="left" rowspan="1" colspan="1">0.691&#x000b1;0.010</td><td align="left" rowspan="1" colspan="1">0.550&#x000b1;0.003</td></tr><tr><td align="left" rowspan="1" colspan="1">178</td><td align="left" rowspan="1" colspan="1">Leaf Roll at 22&#x000b0;C</td><td align="left" rowspan="1" colspan="1">0.587&#x000b1;0.007</td><td align="left" rowspan="1" colspan="1">0.575&#x000b1;0.006</td><td align="left" rowspan="1" colspan="1">1.8e-01</td><td align="left" rowspan="1" colspan="1">0.591&#x000b1;0.005</td><td align="left" rowspan="1" colspan="1">6.0e-01</td><td align="left" rowspan="1" colspan="1">0.580&#x000b1;0.006</td><td align="left" rowspan="1" colspan="1">4.1e-01</td><td align="left" rowspan="1" colspan="1">0.573&#x000b1;0.006</td><td align="left" rowspan="1" colspan="1">0.476&#x000b1;0.008</td></tr></tbody></table></table-wrap>
</p><p><italic>Weight vector analysis</italic>: in <xref ref-type="fig" rid="F3">Figure 3</xref>, we compare the normalized weight vectors obtained by ccSVM and standard SVM for two phenotypes by looking at one representative each. We first pick up the top 100 features selected by standard SVM, and then see how the ccSVM corrects the weights of these features. When the ccSVM curve is lower than the standard SVM curve (negative peak), it means that the corresponding SNPs are likely to be correlated with the confounder and ccSVM weights them down for classification. The SNPs whose weights are scaled up (positive peaks) are less correlated with the confounding side information.
<fig id="F3" position="float"><label>Fig. 3.</label><caption><p>SNPs are sorted by their absolute weight of the standard SVM. The green line shows the weights of the standard SVM, the blue dashed line shows the weights of ccSVM. Both weight vectors are normalized. The <italic>Arabidopsis</italic> phenotypes are shown in the following order (from top to bottom): anthocyanin at 16&#x000b0;C (PID:171,&#x003bb;=10<sup>&#x02212;2</sup>), chlorosis at 22&#x000b0;C (PID:169,&#x003bb;=10<sup>8</sup>).</p></caption><graphic xlink:href="btr204f3"/></fig></p><p>We can see from the figure that both parameter &#x003bb; and the number of negative peaks increases from the top to the bottom. This implies the confounding information increases from top to bottom. For the phenotype anthocyanin at 16&#x000b0;C (PID:171), the top figure shows that there are almost no large negative peaks in the ccSVM curve. This implies there are few spurious associations for the ccSVM to correct. For the phenotype chlorosis at 22&#x000b0;C (PID:169), we can see that ccSVM scales all SNPs down which the standard SVM assigns large weights to. It is likely that they are all correlated with the confounding variable.</p><p><italic>Functional investigation</italic>: we did further analysis for the phenotype chlorosis at 22&#x000b0;C (PID:169). In order to do this, we used the complete dataset as training set and determined &#x003bb; and <italic>C</italic> via cross-validation as described in <xref ref-type="sec" rid="SEC3.1">Section 3.1</xref>.</p><p>First, we selected the top 500 SNPs from the weight vector of ccSVM; these are the SNPs that correspond to the 500 largest absolute entries in the weight vector. After normalizing these entries in both weight vectors, we selected all SNPs which were upscaled by the ccSVM by at least a factor of two. For these 217 SNPs, we searched for nearby genes (&#x000b1;15 kb) which are known to be associated with chlorosis by using a candidate gene list from <xref ref-type="bibr" rid="B1">Atwell <italic>et al.</italic> (2010</xref>).</p><p>The results are shown in <xref ref-type="table" rid="T3">Table 3</xref>. <italic>pen-3-1</italic> mutants show a chlorosis response after being attacked by <italic>Erysiphe cichoracearum</italic>. It is assumed that the gene <italic>PEN3</italic> contributes to defense at the cell wall and intracellularly (<xref ref-type="bibr" rid="B23">Stein <italic>et al.</italic>, 2006</xref>). The <italic>mos6</italic> mutants suppress <italic>snc1</italic> resistance and hence exhibit enhanced disease susceptibility to virulent pathogens (<xref ref-type="bibr" rid="B18">Palma <italic>et al.</italic>, 2005</xref>). The gene <italic>CDR1</italic> is known to be involved in disease resistance signaling (<xref ref-type="bibr" rid="B27">Xia <italic>et al.</italic>, 2004</xref>), and <italic>ahg2-1</italic> mutants have an elevated resistance to bacterial pathogens (<xref ref-type="bibr" rid="B16">Nishimura <italic>et al.</italic>, 2009</xref>).
<table-wrap id="T3" position="float"><label>Table 3.</label><caption><p>Summary of ccSVM results for the presence or absence of chlorosis at 22&#x000b0;C (PID:169)</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th align="left" rowspan="1" colspan="1">Rank</th><th align="left" rowspan="1" colspan="1">Chrom</th><th align="left" rowspan="1" colspan="1">Pos</th><th align="left" rowspan="1" colspan="1">Gene</th><th align="left" rowspan="1" colspan="1">Gene ID</th><th align="left" rowspan="1" colspan="1">dist(Gene)</th></tr></thead><tbody align="left"><tr><td align="left" rowspan="1" colspan="1">109</td><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">22050068</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">6365</td></tr><tr><td align="left" rowspan="1" colspan="1">110</td><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">22056970</td><td align="left" rowspan="1" colspan="1">PDR8/PEN3</td><td align="left" rowspan="1" colspan="1">AT1G59870</td><td align="left" rowspan="1" colspan="1">13267</td></tr><tr><td align="left" rowspan="1" colspan="1">111</td><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">22057369</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">13666</td></tr><tr><td align="left" rowspan="1" colspan="1">208</td><td align="left" rowspan="1" colspan="1">4</td><td align="left" rowspan="1" colspan="1">949836</td><td align="left" rowspan="1" colspan="1">MOS6</td><td align="left" rowspan="1" colspan="1">AT4G02150</td><td align="left" rowspan="1" colspan="1">775</td></tr><tr><td align="left" rowspan="1" colspan="1">224</td><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">20910400</td><td align="left" rowspan="1" colspan="1">AHG2</td><td align="left" rowspan="1" colspan="1">AT1G55870</td><td align="left" rowspan="1" colspan="1">8313</td></tr><tr><td align="left" rowspan="1" colspan="1">267</td><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">20737467</td><td align="left" rowspan="1" colspan="1">CPN60B</td><td align="left" rowspan="1" colspan="1">AT1G55490</td><td align="left" rowspan="1" colspan="1">14605</td></tr><tr><td align="left" rowspan="1" colspan="1">363</td><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">25795239</td><td align="left" rowspan="2" colspan="1">AT5G64510</td><td align="left" rowspan="2" colspan="1">AT5G64510</td><td align="left" rowspan="1" colspan="1">6391</td></tr><tr><td align="left" rowspan="1" colspan="1">464</td><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">25795805</td><td align="left" rowspan="1" colspan="1">5825</td></tr><tr><td align="left" rowspan="1" colspan="1">489</td><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">12625100</td><td align="left" rowspan="1" colspan="1">CDR1</td><td align="left" rowspan="1" colspan="1">AT5G33340</td><td align="left" rowspan="1" colspan="1">11918</td></tr></tbody></table><table-wrap-foot><fn><p>In the table, Chrom,Pos and dist(Gene) represent chromosome, position and the distance from the SNP to the specified gene, respectively.</p></fn></table-wrap-foot></table-wrap>
</p><p>In total, 9 of the 217 upscaled SNPs are close to candidate genes. Out of 216 130 genome-wide SNPs, 3959 are in close proximity to candidate genes. Hence, SNPs near candidate genes are significantly enriched among the SNPs upscaled by the ccSVM (<italic>P</italic>=0.020, &#x003b1;=0.05, Binomial <inline-formula><inline-graphic xlink:href="btr204i10.jpg"/></inline-formula>).</p></sec></sec><sec sec-type="discussion" id="SEC4"><title>4 DISCUSSION</title><p>In this article, we have defined the ccSVM, an SVM with correction for confounding side information. In our experiments, it outperforms several state-of-the-art classifiers with confounder correcting schemes for disease diagnosis in humans and for phenotype prediction in <italic>A.thaliana</italic>.</p><p>Our work extends the advantages of SVMs in data integration: while there is lot of work on SVMs for optimally combining several informative sources of data for a joint prediction (<xref ref-type="bibr" rid="B12">Lanckriet <italic>et al.</italic>, 2004</xref>), there was no approach for correcting SVMs for observed confounding factors so far. The ccSVM closes this gap. This is of particular importance for bioinformatics, as side information on confounders is abundant in most classification tasks on biological data.</p><p>It remains to be discovered if SVMs can be corrected for hidden, unobserved confounders as well, as these tend to frequently occur in gene expression phenotypes. Correcting for these hidden confounders may be one way to further improve the accuracy of our predictions.</p><p>On the biological level, our work will focus on applications of the ccSVM to binary phenotype prediction in plant genetics and in personalized medicine. The latter includes improved disease diagnosis, prognosis and therapy outcome prediction for human patients. One challenge we will tackle here is how to optimally account for several confounding factors, that is learning their weights relative to each other to further improve phenotype prediction.</p></sec>